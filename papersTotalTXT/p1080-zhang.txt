VoiceLive: A Phoneme Localization based Liveness
Detection for Voice Authentication on Smartphones

Linghan Zhang‚Ä†, Sheng Tan‚Ä†, Jie Yang‚Ä†, Yingying Chen‚àó

‚Ä†{lzhang, tan, jie.yang}@cs.fsu.edu, ‚àóyingying.chen@stevens.edu

‚Ä†Florida State University, Tallahassee, FL 32306, USA

‚àóStevens Institute of Technology, Hoboken, NJ 07030, USA

ABSTRACT

Voice authentication is drawing increasing attention and
becomes an attractive alternative to passwords for mobile
authentication. Recent advances in mobile technology fur-
ther accelerate the adoption of voice biometrics in an array
of diverse mobile applications. However, recent studies show
that voice authentication is vulnerable to replay attacks,
where an adversary can spoof a voice authentication system
using a pre-recorded voice sample collected from the vic-
tim. In this paper, we propose VoiceLive, a practical liveness
detection system for voice authentication on smartphones.
VoiceLive detects a live user by leveraging the user‚Äôs unique
vocal system and the stereo recording of smartphones. In
particular, with the phone closely placed to a user‚Äôs mouth,
it captures time-diÔ¨Äerence-of-arrival (TDoA) changes in a
sequence of phoneme sounds to the two microphones of the
phone, and uses such unique TDoA dynamic which doesn‚Äôt
exist under replay attacks for liveness detection. VoiceLive
is practical as it doesn‚Äôt require additional hardware but
two-channel stereo recording that is supported by virtually
all smartphones. Our experimental evaluation with 12 par-
ticipants and diÔ¨Äerent types of phones shows that VoiceLive
achieves over 99% detection accuracy at around 1% Equal
Error Rate (EER). Results also show that VoiceLive is ro-
bust to diÔ¨Äerent phone placements and is compatible to dif-
ferent sampling rates and phone models.

Keywords
Voice recognition; Liveness detection; Phoneme localization

1.

INTRODUCTION

As a primary way of communication, our voice is a partic-
ularly attractive biometric for identifying users. It reÔ¨Çects
individual diÔ¨Äerences in both behavioral and physiological
characteristics, such as the inÔ¨Çection and the shape of the
vocal tract [23]. Such distinctive behavioral and physiologi-
cal traits could be captured by voice authentication systems
for diÔ¨Äerentiating each individual [17]. Voice authentication

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS‚Äô16, October 24-28, 2016, Vienna, Austria
¬© 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978296

leveraging built-in microphones on mobile devices is partic-
ularly convenient and low-cost, comparing to the passwords
authentication that is diÔ¨Écult to use while on-the-go and
requires memorization. Recent advances in mobile technol-
ogy further accelerate the adoption of voice biometrics in an
array of diverse mobile applications.

Indeed, voice authentication has been introduced recently
to mobile devices and apps to provide secure access and lo-
gins. For example, Google has integrated it into Android
operating systems (OSs) to allow users to unlock mobile de-
vices [2], and Tencent has updated its WeChat mobile app
to support voice biometric logins [8]. Another appealing
use case of voice authentication is to support mobile Ô¨Ånan-
cial services. For instance, SayPay provides voice biometric
solution for online payment, e-commerce, and online bank-
ing [5]. And an increasing number of Ô¨Ånancial institutions,
HSBC, Citi, and Barclays for example, are deploying voice
authentication for their telephone and online banking sys-
tems [3]. This trend is expected to continue growing at a
rate of 22.15 percent yearly until 2019, and will result in an
estimated $113.2 billion market share by 2017 [4]. Voice au-
thentication thus becomes an attractive alternative to pass-
words in mobile authentication and is increasingly popular.
Voice authentication however has been shown to be vul-
nerable to replay attacks in recent studies [16, 33, 14]. An
adversary can spoof a voice authentication system by us-
ing a pre-recorded voice sample collected from the victim.
The voice sample can be any recording captured inconspic-
uously. Or, an adversary can obtain voice samples from the
victim‚Äôs publicly exposed speeches. The attacker could even
concatenate voice samples from a number of segments in
order to match the victim‚Äôs passphrase. Such attacks are
most accessible to the adversary due to the proliferation of
mobile devices, such as smartphones and digital recorders.
They are also highly eÔ¨Äective in spooÔ¨Ång authentication sys-
tems, as evidenced by recently work [32, 33]. Replay attacks
therefore present signiÔ¨Åcant threats to voice authentication
and are drawing increasing attention. For example, Google
advises users on the vulnerability of their voice logins by
displaying a popup message ‚Äú... a recording of your voice
could unlock your device.‚Äù [1]

Prior work in defending against replay attacks is to uti-
lize liveness detection to distinguish between a passphrase
spoken by a live user and a replayed one pre-recorded by
the adversary. For example, Shang et al. propose to com-
pare an input voice sample with stored instances of past
accesses to detect the voice samples have been seen before
by the authentication system [31]. This method, however,

1080cannot work if the attacker records the voice samples during
a non-authentication time point. Villalba et al. and Wang et
al. suggest that the additional channel noises introduced by
the recording and loudspeaker can be used for attack detec-
tion [32, 33]. These approaches however have limited eÔ¨Äec-
tiveness in practice. For example, the false acceptance rates
of these approaches are as high as 17%. Chetty and Wag-
ner propose to use video camera to extract lip movements
for liveness veriÔ¨Åcation [13], whereas Poss et al. combine
the techniques of a neural tree network and Hidden Markov
Models to improve authentication accuracy [28]. Aley-Raz
et al. develop a liveness detection system based on ‚ÄúIntra -
session voice variation‚Äù [10], which is integrated into Nuance
VocalPassword [6]. In addition to a user-chosen passphrase,
it requires a user to repeat one or more random sentences
prompted by the system for liveness detection.

In this paper, we introduce and evaluate a phoneme sound
localization based liveness detection system on smartphones.
Our system distinguishes a passphrase spoken by a live user
from a replayed one by leveraging (i) the human speech pro-
duction system and (ii) advanced smartphone audio hard-
ware. First, in human speech production, a phoneme is the
smallest distinctive unit sound of a language. Each phoneme
sound can be viewed as air waves produced by the lungs, and
then modulated by the movements of vocal cords and vocal
tract including throat, mouth, nose, tongue, teeth, and lips.
Each phoneme sound thus experiences unique combination
of place and manner of articulation. Consequently, diÔ¨Äer-
ent phoneme sounds could be located at diÔ¨Äerent physical
positions in the human vocal tract system with an acous-
tic localization method. Second, smartphone hardware is
now supporting advanced audio capabilities. Virtually all
smartphones are equipped with two microphones for stereo
recording (one on the top and the other one at the bottom),
and are capable of recording at standard 48kHz and 192kHz
sampling rates. For example, with the latest Android OSs,
Samsung Galaxy S5 and Note3 are capable of stereo record-
ing at 192kHz, which yields 5.21 microseconds‚Äô time reso-
lution or millimeter-level ranging resolution1. We thus can
leverage such stereo recording or dual microphones on smart-
phones to pinpoint the sound origin of each phoneme within
human vocal system for liveness detection.

Ideally, locating a phoneme sound requires at least three
microphones with three individual audio channels. Although
current two-channel stereo recording cannot uniquely lo-
cate the phoneme sound origin, it can capture the time-
diÔ¨Äerence-of-arrival (TDoA) of each phoneme sound to the
two microphones of the phone. With the phone closely
placed to user‚Äôs mouth, the diÔ¨Äerences in TDoA between
most phoneme sounds are distinctive and measurable with
millimeter-level ranging resolution. Very importantly, each
passphrase (usually 5 to 7 words [7, 30]) consists of a se-
quence of diÔ¨Äerent phoneme sounds that will produce a se-
ries of TDoA measurements with various values. We refer
to the changes in TDoA values as TDoA dynamic, which
is determined by the speciÔ¨Åc passphrase, the placement of
the phone, and a user‚Äôs unique vocal system. Such TDoA
dynamic, which doesn‚Äôt exist under replay attacks, is then
utilized for liveness detection.

In particular, when a user Ô¨Årst enrolled in the system,
the TDoA dynamic of the user-chosen or system prompted
1Assuming the speed of sound is 340m/s, each digital sample
represents a distance of 1.77mm.

passphrase is Ô¨Årst captured by the smartphone stereo record-
ing, and then stored in the system. During online authen-
tication phase, the extracted TDoA dynamic of an input
utterance will be compared to the one stored in the sys-
tem. A live user is detected, if that produce a similarity
score higher than a pre-deÔ¨Åned threshold. By relaxing the
problem from locating each phoneme sound to measuring
the TDoA dynamic for a sequence of phonemes, we enable
liveness detection on a single phone without any additional
hardware. Our system does have the limitation of requiring
a user to hold the phone close to her/his mouth with the
same pose in both enrollment and authentication processes.
The contributions of our work are summarized as follows:
‚Ä¢ We show that the origin of each phoneme can be uniquely
located within the human vocal tract system by us-
ing a microphone array. It lays the foundation of our
phoneme localization based liveness detection system.
‚Ä¢ We develop VoiceLive, a practical liveness detection

system that extracts the TDoA dynamic of the passphrase
for live user detection. VoiceLive takes advantages of
the user‚Äôs unique vocal system and high quality stereo
recording of smartphones.
‚Ä¢ We conduct extensive experiments with 12 participants
and three diÔ¨Äerent types of phones under various ex-
perimental settings. Experimental results show that
VoiceLive achieves over 99% detection accuracy at around
1% EER. Results also show that VoiceLive is robust to
diÔ¨Äerent phone placements and is compatible to diÔ¨Äer-
ent sampling rates and phone models.

The remainder of this paper expands on above contribu-
tions. We begin with system and attack model, and a brief
introduction to phoneme sounds localization.

2. PRELIMINARIES
2.1 System and Attack Model

There exists two types of voice authentication systems:
text-dependent and text-independent. We primarily focus
on the text-dependent system as it is currently the most
commercially viable method and produces better authen-
tication accuracy with shorter utterances [31].
In a text-
dependent system, the text to be spoken by a user is the
same one for enrollment and veriÔ¨Åcation. Such text could
be either a user-chosen or system prompted one. Figure 1
shows the processes of a typical voice authentication sys-
tem. Our method can also be extended to text-independent
systems, which will be discussed in Section 5.

For the attack model, we consider replay attacks, which
are the most accessible and eÔ¨Äective attacks aiming at spoof-
ing the system by replaying a pre-recorded voice sample of
the victim [32]. We consider the replay attacks that take
place at two locations, at the microphone point and at the
transmission point, as shown in Figure 1. For the sake of
simplicity, we refer to the former as a playback attack and
the latter as a replace attack. In a playback attack, an adver-
sary uses a speaker to replay the pre-recorded voice sample
in front of the microphones.
In a replace attack, an ad-
versary replaces his/her own speech signal as the victim‚Äôs
before or during transmission. This can be done by lever-
aging the availability of the virtual recorder to bypass the
local microphones, or by intercepting and replacing speech
signal during transmission.

10817UDQVPLVVLRQSRLQW

UHSODFHDWWDFN

([LVWLQJ

6SHDNHU0RGHO

0LFURSKRQH

0LFURSKRQHSRLQW
SOD\EDFNDWWDFN

&ODVVLILHU

'HFLVLRQ

(cid:708)$FFHSWRUQRW(cid:709)

)HDWXUH
([WUDFWLRQ

Figure 1: A typical voice authentication system with
two possible places of replay attacks.

≈Ø«Äƒû≈Ω≈ØƒÇ∆å

ŒÄ∆öŒÅŒÄƒöŒÅŒÄ≈∂ŒÅŒÄ∆êŒÅŒÄ«åŒÅŒÄ≈ØŒÅ

≈ù≈ØƒÇƒè≈ùƒÇ≈Ø
ŒÄ∆âŒÅŒÄƒèŒÅŒÄ≈µŒÅ

>ƒÇƒè≈ù≈Ωƒöƒû≈∂∆öƒÇ≈Ø

ŒÄƒ®ŒÅŒÄ«ÄŒÅ

ƒû≈∂∆öƒÇ≈Ø
ŒÄ…ΩŒÅŒÄƒùŒÅ

WƒÇ≈ØƒÇ∆öƒÇ≈Ø

ŒÄ‡°öŒÅŒÄ‡°©ŒÅŒÄ‡°æŒÅŒÄ‡°ªŒÅŒÄ≈©ŒÅ

sƒû≈ØƒÇ∆å

ŒÄ≈¨ŒÅŒÄ≈êŒÅŒÄ≈ºŒÅŒÄ«ÅŒÅ

Figure 3: Place of articulation and corresponding
consonants.

ŒÄ≈ùŒÅ

ŒÄ≈ΩŒÅ

>≈ù∆â∆ê

&≈Ω∆å«ÅƒÇ∆åƒö

ŒÄ≈ùŒÅ

ƒèƒûƒû∆ö

ƒè≈ù∆öŒÄ‹ºŒÅ

d≈Ω≈∂≈ê∆µƒû

ŒÄ‡†ßŒÅ

ƒèƒû∆öŒÄ‹≠ŒÅ

ŒÄ∆µŒÅ

ƒè≈Ω≈Ω∆ö

ŒÄ‡†Ø‡°°ŒÅ

ƒè≈ΩƒÇ∆ö

h∆â

s≈Ω≈ùƒêƒû
ƒè≈Ω∆ö

ŒÄ‡†™ŒÅ

ŒÄ‡†®ŒÅ

(a) Example

ƒèƒÇ∆öŒÄƒçŒÅ
(b) Vowel chart

Figure 2: Tongue positions of English vowels within
the oral cavity, and the vowel chart.

2.2 Human Speech Production and Phonemes
The human speech production system involves three vi-
tal physiological components: lungs, vocal cords, and vocal
tract [27]. When someone exhales, air is expelled from the
lungs, and then passes over the vocal cords, which dilate or
constrict to allow or impede the air Ô¨Çow to produce unvoiced
or voiced sound. Such sound is then resonated and reshaped
by the vocal tract that consists of multiple organs such as
throat, mouth, nose, tongue, teeth, and lips. The vocal cords
modulation, interaction and movement of these organs can
alter sound waves and produce unique human sounds.

A phoneme is the smallest distinctive unit sound of a lan-
guage [27]. The two major phoneme categories are vow-
els and consonants. In particular, vowels are the phoneme
sounds produced when vocal cords constrict air Ô¨Çow (i.e.,
voiced sound) but with an open vocal tract. The tongue
position is the most important physical feature that distin-
guishes one vowel from another [27]. As diÔ¨Äerent tongue po-
sitions lead to diÔ¨Äerent multipath environments inside the
oral cavity, we can locate the sound origins of diÔ¨Äerent vow-
els at diÔ¨Äerent physical locations inside the human oral cav-
ity. As illustrated in Figure 2 (a), when the tongue moves
to lower right corner, vowel [A] can be pronounced, whereas
when the tongue moves to upper left corner and backward,
vowels [i] and [o] can be produced, respectively. More gen-
erally, Figure 2 (b) shows the vowel chart which involves
two dimensions of tongue movements: up/down movements
(i.e., height) and back/forth movements (i.e., backness). Ex-
tending or retracting the tongue forward or backward to-
wards the teeth produces a more front or back vowel sound,
whereas lowering or raising the tongue towards lower jaw or
towards the roof of mouth produces a more open or close
vowel.

Unlike vowels, consonants are produced when vocal cords
either constrict or dilate air Ô¨Çow and with signiÔ¨Åcant con-
striction of the air Ô¨Çow in the oral cavity. The articulation
place and manner are two major factors that distinguish one

DƒÇ≈∂≈∂ƒû∆å

W≈ØƒÇƒêƒû

EƒÇ∆êƒÇ≈Ø
^∆ö≈Ω∆â

&∆å≈ùƒêƒÇ∆ö≈ù«Äƒû
ƒ®ƒ®∆å≈ùƒêƒÇ∆öƒû

∆â∆â∆å≈Ω«Ü≈ù≈µƒÇ∆öƒû

>ƒÇ∆öƒû∆åƒÇ≈Ø

≈ù≈ØƒÇƒè≈ùƒÇ≈Ø >ƒÇƒè≈ù≈Ωƒöƒû≈∂∆öƒÇ≈Ø ƒû≈∂∆öƒÇ≈Ø ≈Ø«Äƒû≈Ω≈ØƒÇ∆å WƒÇ≈ØƒÇ∆öƒÇ≈Ø sƒû≈ØƒÇ∆å
ŒÄ≈ºŒÅ
ŒÄ≈¨ŒÅŒÄ≈êŒÅ

ŒÄ≈µŒÅ
ŒÄ∆âŒÅŒÄƒèŒÅ

ŒÄ‡¢†ŒÅŒÄ‡°©ŒÅ

ŒÄ≈∂ŒÅ
ŒÄ∆öŒÅŒÄƒöŒÅ
ŒÄ∆êŒÅŒÄ«åŒÅ

ŒÄƒ®ŒÅŒÄ«ÄŒÅ

ŒÄ…ΩŒÅŒÄƒùŒÅ

ŒÄ‡£ÑŒÅŒÄ‡°ªŒÅ

ŒÄ«ÅŒÅ

ŒÄ≈ØŒÅ

Figure 4: Consonants chart based on place and man-
ner of articulation.

consonant from another [27]. The combined eÔ¨Äect of place
and manner of articulation and voiced/unvoiced sound lead
to diÔ¨Äerent consonant sounds emitted from diÔ¨Äerent loca-
tions within the human vocal tract system. In particular,
place of articulation is the location where the constrictions
or obstructions of air stream occur, and can be categorized
into 6 groups: bilabial, labiodental, dental, alveolar, palatal,
and velar. Figure 3 shows each group and the corresponding
consonants. For example, the consonants [p][b][m][w] can be
pronounced when the obstruction of air stream occurs at up-
per and lower lips. The consonants within each group can be
further distinguished by the manner of articulation, which
describes the conÔ¨Åguration and interaction of the speech or-
gans (e.g., the tongue, lips, and palate). There are 6 types
of articulation manners including nasal, stop, fricative, af-
fricate, approximate and lateral. For instance, nasal con-
sonant [m] is produced when the air stream is completely
blocked by mouth and only passes through the nose. Fig-
ure 4 summarizes the categorization of diÔ¨Äerent consonants
based on place of articulation and manner of articulation.
The bolded font in the Ô¨Ågure shows the voiced sounds (e.g.,
[b] and [v]), whereas the rest are unvoiced sounds (e.g., [p]
and [f ] ).
2.3 Phoneme Localization using Microphone

Array

We next conduct experiments to study how the origin of
phoneme sound is located within the human vocal tract sys-
tem by leveraging a microphone array. We utilize six ex-
ternal microphones organized in three pairs A, B, and C.
As shown in Figure 5 (a), the microphones are distributed
in the X-Z plane2 with 5cm and 10cm horizontal distances,
and 5cm and 7.6cm vertical distances. Such a distribution
could cover the size of a human vocal tract. Each pair is
synchronized to measure the TDoA of the sound origin to
the two microphones. These pairs produce three indepen-
dent TDoA values, which could uniquely locate the sound
origin in a 3D space. We measure the TDoA in terms of the

2Note that the sectional view of the human vocal tract in
Figure 5 (b) is on the Y-Z plane.

1082Dœ≠

œ±ƒê≈µ
DœÆ

DœÆ

œ±ƒê≈µ

œ±ƒê≈µ

DœÆ

Dœ≠

œ≥Õòœ≤ƒê≈µ

œ≠œ¨ƒê≈µ

Dœ≠

ŒÄ≈µŒÅ

ŒÄ‡°öÕ¨‡°©ŒÅ

ŒÄ≈©ŒÅ

ŒÄ∆µŒÅ

ŒÄ∆êŒÅ

ŒÄ…ΩŒÅ

ŒÄ«ÅŒÅ




≈Ω

d

ŒÄ«åŒÅŒÄ∆öŒÅ

ŒÄ«ÄŒÅ

ŒÄ‡°ÅŒÅ
ŒÄƒû‡°ÅŒÅ

ŒÄ‡†≥ŒÅ ŒÄ‡°°ŒÅ
ŒÄ‘•ŒÅ

ŒÄ≈ùŒÅ

ŒÄ‹≠ŒÅ
ŒÄ‡†ß‡°ÅŒÅ

ŒÄƒçŒÅ

ŒÄ‡†Ø‡°°ŒÅ

ŒÄ‡†™ŒÅ

ŒÄ‡†®ŒÅ
ŒÄ‡†ß‡•§ŒÅ



z



y

y

D/D\RXWRI0LFURSKRQHV

z

E/RFDOL]HGSKRQHPHVRXQGV

Figure 5: Phonemes localization using microphone
array.

number of delayed samples to the two microphones. As we
use 192kHz for recording, the TDoA ranging resolution is
1.77mm. Before the phoneme localization, we test the local-
ization accuracy by emitting chirp sounds at diÔ¨Äerent Ô¨Åxed
locations in front of the microphones. We observe that it
produces an averaged localization error within 2mm.

We recruit two participants to pronounce each phoneme
sound in front of the microphone array multiple trials. Fig-
ure 5 (b) illustrates the localized phoneme sound origins for
one participant. It shows the sectional view of human vocal
tract on Y-Z plane. The red dots show the localized vowel
sound origins, whereas the green dots show these of conso-
nant ones. We obtain several important observations from
Figure 5. First, the located sound origins of vowels match
the tongue positions very well. For example, the vowels con-
nected by the dotted lines in Figure 5 (b) have similar rel-
ative positions and overall shape as that of the vowel chart
in Figure 2 (b). This is because the tongue position is the
deterministic factor of vowel production. Second, some of
the consonants have the origins close to the place of articu-
lation, while others are signiÔ¨Åcantly aÔ¨Äected by the manner
of articulation. For instance, [s],[z] and [t] have the local-
ized sound origins close to alveolar, which is the place of
articulation of these sounds, whereas [m] is located in the
nasal cavity where the airÔ¨Çows out (i.e., manner of articu-
lation). Moreover, we observe the located phoneme origins
are mainly distributed within the mouth and nasal cavities
with the size of about 4cm by 4cm, and they show little
changes in X axis (i.e., lateral direction of mouth). We also
Ô¨Ånd that diÔ¨Äerent participants produce diÔ¨Äerent localized
sound origins for the same phoneme due to the individual
diversity in the human vocal tract (e.g., shape and size) and
the habitual way of pronouncing phonemes.
3. SYSTEM DESIGN

In this section, we introduce our system design and its

core components and algorithms.
3.1 Approach Overview

The key idea underlying our liveness detection system is
to perform TDoA ranging for a sequence of phoneme sounds
at the two microphones on the phone. As illustrated in
Figure 6, a user Ô¨Årst speaks an utterance, say ‚Äúvoice‚Äù to
the phone that closely placed to the user‚Äôs mouth. Each
phoneme sound (i.e., [v] [A] [I] [s] in the example) is then
emitted from the user‚Äôs vocal system and picked up by the

œ∞Õò«Ü∆ö∆åƒÇƒê∆ö≈ù≈∂≈êd≈Ωƒö«á≈∂ƒÇ≈µ≈ùƒê≈Ωƒ®∆â≈ö≈Ω≈∂ƒû≈µƒû∆ê
ƒ®≈Ω∆å≈Ø≈ù«Äƒû≈∂ƒû∆ê∆êƒöƒû∆öƒûƒê∆ö≈ù≈Ω≈∂Õò

œ≠Õòh∆êƒû∆å∆ê∆âƒûƒÇ≈¨∆êƒÇ≈∂∆µ∆ö∆öƒû∆åƒÇ≈∂ƒêƒûÕïƒûÕò≈êÕòÕïÕû«Ä≈Ω≈ùƒêƒûÕü

«Å≈ù∆ö≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆êÕóŒÄ«ÄŒÅŒÄ‡†™ŒÅŒÄ/ŒÅŒÄ∆êŒÅÕò

ŒÄ«ÄŒÅ

ŒÄ‡†™ŒÅ

ŒÄ‡°ÅŒÅ

ŒÄ∆êŒÅ

d≈ΩŒÄ∆êŒÅ

d≈ΩŒÄ/ŒÅ

ŒÄ∆êŒÅ

ŒÄ‡°ÅŒÅ

ŒÄ‡†™ŒÅ

D≈ùƒêœÆ

d≈ΩŒÄ«ÄŒÅ d≈ΩŒÄ‡†™ŒÅ

D≈ùƒêœ≠

œÆÕòƒÇƒê≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆ê≈Ω∆µ≈∂ƒö∆â∆å≈Ω∆âƒÇ≈êƒÇ∆öƒû∆ê

∆ö≈Ω∆ö≈öƒû∆ö«Å≈Ω≈µ≈ùƒê∆ê ≈Ωƒ®∆ö≈öƒû∆â≈ö≈Ω≈∂ƒûÕò

ŒÄ«ÄŒÅ

œØÕòW≈ö≈Ω≈∂ƒû≈Ω∆åƒÇ∆µ∆ö≈öƒû≈∂∆ö≈ùƒêƒÇ∆ö≈ù≈Ω≈∂∆ê«á∆ê∆öƒû≈µƒöƒûƒö∆µƒêƒû∆êd≈Ω

≈Ωƒ®ƒûƒÇƒê≈ö∆â≈ö≈Ω≈∂ƒû≈µƒû∆ö≈Ω∆ö≈öƒû∆ö«Å≈Ω≈µ≈ùƒê∆å≈Ω∆â≈ö≈Ω≈∂ƒû∆êÕò

Figure 6: Illustration of phoneme localization using
a single phone.

two microphones of the phone with stereo recording. The
phone processes the recorded sound to deduce the TDoA
of each phoneme sound to the two microphones. As most
phoneme sounds have measurable TDoA diÔ¨Äerences to the
two microphones, a sequence of phonemes will produce series
of TDoA with various values, as shown in Figure 6. We refer
to the changes in TDoA measurements as ‚ÄúTDoA dynamic‚Äù,
which is then used for liveness detection.

In particular, the measured TDoA dynamic will be com-
pared with the one extracted when the user enrolled in the
system. A live user is detected if the similarity score exceeds
the pre-deÔ¨Åned threshold. Under playback attacks, the mea-
sured TDoA dynamic will be very diÔ¨Äerent from that of a live
user due to diÔ¨Äerent sound production systems (i.e., loud-
speaker v.s. human vocal system). Under replace attacks,
it is extremely unlikely, if not impossible, for an adversary
to place a stereo recorder (e.g., smartphone) very close, say
5cm, to the victim‚Äôs mouth to collect voice samples. Due
to the origins of the phoneme sounds are crowded in the
mouth and nasal cavities as shown in Figure 5 (b), the TDoA
dynamic diminishes rapidly with the increased distance be-
tween the recorder and the user‚Äôs mouth. For example, if
the phone is placed 30cm away from the user‚Äôs mouth, the
maximum achievable TDoA range among all phonemes is
less than 1cm. With such a small range, most phonemes
have the same TDoA measurement to the two microphones
of the phone. The measured TDoAs under replace attack
thus cannot match the one extracted when the user enrolled
in the system.

Virtually all smartphones are equipped with two micro-
phones and are capable of stereo recording. By leveraging
a sequence of phoneme sounds in an utterance/passphrase,
our approach relaxes the problem of locating each phoneme
sound to tracking TDoA dynamic for live user detection.
We thus enable the phoneme localization based liveness de-
tection on a single phone without requiring any additional
hardware.

Our system does require the user to place the smartphone
close to the mouth with the same pose in both enrollment
and authentication processes. The eÔ¨Äects of diÔ¨Äerent phones
and phone displacement are studied in experiment evalua-
tion. Moreover, data protection mechanisms or secure com-
munication protocols should be in place to prevent an at-
tacker from obtaining the plain-text of TDoA dynamic and
the dual-channel audio samples [18]. For example, TDoA
dynamic could be extracted locally without storing the dual-
channel audio sample, and only the encrypted one-channel

1083d≈Ω
«á≈∂ƒÇ≈µ≈ùƒê
W∆å≈Ωƒ®≈ù≈Øƒû∆ê

5000
5000

4000

AU

E

T

C

TI

N

s≈Ω≈ùƒêƒû
^ƒÇ≈µ∆â≈Øƒû

W≈ö≈Ω≈∂ƒû≈µƒû

^ƒû≈ê≈µƒû≈∂∆öƒÇ∆ö≈ù≈Ω≈∂

d≈Ω

ƒÇ≈Øƒê∆µ≈ØƒÇ∆ö≈ù≈Ω≈∂

^≈ù≈µ≈ù≈ØƒÇ∆å≈ù∆ö«á
≈Ω≈µ∆âƒÇ∆å≈ù∆ê≈Ω≈∂

ƒû∆öƒûƒê∆ö≈ù≈Ω≈∂





>≈ù«Äƒûh∆êƒû∆å

≈Ω∆å

Zƒû∆â≈ØƒÇ«á∆ö∆öƒÇƒê≈¨

Figure 7: The Ô¨Çow of our liveness detection system.







	










	


	



 




	


	









 

Figure 8: Example: spectrogram of phonemes.

audio sample together with the encrypted TDoA dynamic
are transmitted or used for veriÔ¨Åcation and liveness detec-
tion.
3.2 System Flow

Realizing our system requires four major components: Pho-
neme Segmentation, TDOA Calculation, Similarity Compar-
ison, and Detection. As shown in Figure 7, the voice sample
acquired by two microphones Ô¨Årst passes through phoneme
segmentation, which extracts phonemes existing in the voice
sample. In particular, we combine Hidden Markov Model-
ing techniques to perform forced alignment on the words
recognized from the voice sample to identify each phoneme
sound. The words in the voice sample are recognized by
acoustic modeling and language modeling algorithms.

Next, the TDOA calculation component is used to calcu-
late the number of delayed samples of each phoneme sound
to the two microphones. As acoustic signals can be eas-
ily distorted due to multipath propagation, simply corre-
lating phonemes between two channels will result in large
error. To address this challenge, we adopt generalized cross-
correlation and heuristic-based phase transform weighting
approaches for accurate TDoA estimation.

After that, the similarity comparison component measures
the similarity of the calculated TDoA dynamic to the one
stored in the system. It results in a similarity score, which is
then compared with a pre-deÔ¨Åned threshold. If the score is
larger than the threshold, a live user is detected, otherwise
a replay attack is declared. The detection result can be then
combined with the traditional voice authentication system
to verify the claimed identity of a user.
3.3 Phoneme Segmentation

The underlying principle for phoneme segmentation is that
the sound of a phoneme contains a number of diÔ¨Äerent over-
tone pitches simultaneously, known as formants [23]. By
analyzing the sound spectrogram, we are able to discover
these overtone pitches or formats to identify each individual
phoneme sound. Although the most informative formants
are the Ô¨Årst three formants, the two Ô¨Årst formants, F1 and

TH

N

I

A

O

)
z
H

(
 
y
c
n
e
u
q
e
r
F

3000

2000

1000

0
0

0
0

0.3

0.6

Time (s)

0.9

1.2

1.401

Figure 9: Example of segmented phonemes.

F2, are enough to disambiguate the vowel. As illustrated in
Figure 8, it is easy to observe the Ô¨Årst two formants, F1 and
F2, which contribute to the overtone of each vowel most.
It is thus feasible to segment diÔ¨Äerent vowels by looking at
the F1 and F2 in the spectrogram. Unlike vowels, conso-
nants‚Äô spectrograms display as random mixture of diÔ¨Äerent
frequencies, as showed in Figure 8. This static noise-like
sound makes it diÔ¨Écult to accurately identify each conso-
nant by simply utilizing formants. We thus adopt forced
alignment by using HMM (Hidden Markrov Models), which
aligns the input voice spectrogram with existing voice sam-
ples to distinguish diÔ¨Äerent consonants [20].

In particular, we Ô¨Årst recognize the words existing in the
voice sample, which could be done by using automatic speech
recognition (ASR). We use advanced CMUSphinx [29] to au-
tomatically recognize each word in the user‚Äôs voice sample.
More speciÔ¨Åcally, the voice sample is Ô¨Årst parsed into fea-
tures, which are a set of mel-frequency cepstrum coeÔ¨Écients
(MFCC) that model the human auditory system. Then, the
MFCCs are combined together with the dictionary, acoustic
model, and language model to recognize the words in the
voice sample [29].

Given the recognized words, we utilize MAUS as primary
method for phoneme segmentation and labeling [21]. In par-
ticular, the recognized words are Ô¨Årst transferred into ex-
pected pronunciation based on standard pronunciation model
(i.e., SAMPA phonetic alphabet). Then, the generated canon-
ical pronunciation together with the millions of possible ac-
cents of users yield a probabilistic graph including all possi-
ble hypotheses and the corresponding probabilities. At last,
the system searches the graph space for the path of pho-
netic units that have been spoken with highest probability
using a Hidden Markrov Model. Outcomes of the search are
segmented and labeled phonetic units. Figure 9 illustrates
one example of the resulted phoneme segmentation when
one user pronounces the word ‚Äúauthentication‚Äù. We observe
that the segmentation accurately captures both the vowels
and the consonants.
3.4 TDOA Calculation

The basic idea of TDoA calculation is to count the num-
ber of delayed samples to the two microphones by correlating
each segmented phoneme sound between smartphone‚Äôs two
channels. Let‚Äôs denote mic1 and mic2 as the two micro-
phones/channels of the phone, and Œît as the TDoA of one
phoneme sound to the two microphones. Given the phoneme
sound mic1(t) recorded at mic1, we correlate such phoneme
sound to the sound signal mic2(t + d) recorded at the mic2,
with d varying from 0 to N ‚àí 1. Once the best match is
found, the corresponding d value is the number of delayed
samples between mic1 and mic2. In particular, such correla-

1084-30

-35

l

s
e
p
m
a
S

data1
data2
data3
data4
data5
data6
data7
data8
data9
data10

-40

-45

 

 
f
o
s
m
r
e
T
n

 

i
 

A
o
D
T

-50

O s car di dn t

li ke s wee

p day

Phoneme Sound

Figure 10: TDoAs of one passphrase for 10 trials.

tion can be done by using a cross-correlation technique [25],
as shown below:

(cid:2)
(cid:3)(cid:2)

i

CC(d) =

[(mic1(i) ‚àí mic1(i)) ‚àó (mic2(i + d) ‚àí mic2(i + d))]
(mic1(i) ‚àí mic1(i))2

(cid:3)(cid:2)

(mic2(i + d) ‚àí mic2(i + d))2
(1)

i

i

The TDoA Œît can be obtained as:

Œît = argmax

d

CC(d),

(2)

However, simply applying the cross-correlation method
results in an inaccurate estimation of Œît due to the mul-
tipath propagation and reverberation eÔ¨Äect of acoustic sig-
nals. To improve the accuracy, we further utilize general-
ized cross correlation with phase transformation techniques
(PHAT) [22]. By adding a weighting function into cross cor-
relation calculation process, it suppresses the frequency com-
ponents whose power spectra carry intense additive noises.
Meanwhile, PHAT utilizes the cross-power spectral density
of two diÔ¨Äerent acoustic signals to improve the system‚Äôs ro-
bustness to reverberation eÔ¨Äect. Existing work has shown
PHAT can further mitigate the spreading eÔ¨Äect that caused
by uncorrelated noises at two microphones [22].

Figure 10 shows one example of the TDoA values when
one participant performs 10 trials of authentication with the
passphrase ‚ÄúOscar didn‚Äôt like sweep day‚Äù. The X axis shows
each phoneme sound, whereas Y axis shows the TDoA in
terms of number of delayed samples. We observe that TDoA
dynamics of these trials are highly similar and stable, with
only 1 to 2 samples variation under 192kHz sampling rate.
The results show that TDoA calculation is able to catch the
user‚Äôs unique speech production system accurately.
3.5 Similarity Comparison

Once the TDoA dynamic is extracted, we Ô¨Årst normalize
these TDoA values to the same scale as those stored in the
user proÔ¨Åle. Such normalization is used to deal with the is-
sues of device diversity and phone displacement. The phone
a user used to enroll in the system could be diÔ¨Äerent from
the one he/she used for authentication. As diÔ¨Äerent phones
diÔ¨Äer in size or distance between the two microphones, the
absolute TDoA values of the same phoneme could be dif-
ferent. Similarly, if the user places the phone at a location
slightly diÔ¨Äerent from that when he/she enrolled in the sys-
tem, the absolute TDoA values vary slightly. Normalizing
the TDoAs to the same scale could eÔ¨Äectively mitigate these
issues.

To compare the similarity of the TDoA dynamic with
the user proÔ¨Åle, we utilize both the correlation coeÔ¨Écient
and the probability. In particular, the correlation coeÔ¨Écient

,

Figure 11: Two diÔ¨Äerent phone placements diagram.

measures the degree of linear relationship between two se-
quences [35]. Other than calculating the absolute diÔ¨Äerence,
it quantiÔ¨Åes the similarities in the changes of two sequences.
The correlation coeÔ¨Écient ranges from -1 to +1. A value
of near +1 indicates a high degree of similarity, whereas a
value near 0 indicates a lack of similarity.

For the probability based method, we assume the TDoA
ranging error of each phoneme follows an independent stan-
dard Gaussian distribution. Given the TDoA value T DoAi
in the extracted TDoA dynamic, the probability that it
matches the one in the user proÔ¨Åle is represented as:

P (T DoAi) =

‚àö
1
2œÄ

œÉ

e‚àí(T DoAi‚àíT DoAi)2

(3)

whereas œÉ is the standard deviation of the error and T DoAi
is the corresponding TDoA value in the user proÔ¨Åle. Dur-
ing the user enrollment phase, we ask each user to speak a
passphrase three times to extract the averaged TDoA and
the standard deviation of each phoneme for similarity com-
parison. Given the probability value of each phoneme, we
simply average the probability values of all phonemes as the
indicator of the similarity score.

Correlation coeÔ¨Écient and probability are two metrics tar-
geting on diÔ¨Äerent characteristics of the TDoA dynamic. We
refer to the former as Correlation, and latter as Probability.
Moreover, we develop a combined scheme that simply com-
bines the similarity scores of the correlation and probability
based methods. We refer to such a method as Combined
method, which takes advantages of both the correlation co-
eÔ¨Écient and the probability.
4. PERFORMANCE EVALUATION

In this section, we evaluate our liveness detection system
under replay attacks including both playback and replace
attacks3. We also evaluate the robustness of our system
to diÔ¨Äerent types of phones, sampling frequencies, phone
displacements, and lengths of passphrases.
4.1 Experiment Methodology

Phones and Placements. We evaluate our system with
three types of phones with diÔ¨Äerent sizes and audio chipsets.
In particular, we experiment with Samsung Galaxy Note3,
Galaxy Note5 and Galaxy S5. The distance between the
two microphones (i.e., one on the top and one at the bot-
tom) for stereo recording is about 15.1cm for Note3, 15.3cm
for Note5, and 14.1cm for S5. The audio chipset of Note3 is
Qualcomm Snapdragon 800 MSM8974, whereas it is Wolf-
son WM1840 for Note5, and Audience‚Äôs ADNC ES704 for
S5. The operating system of these phones is Android 6.0
Marshmallow, which enables the phones to perform stereo
recording at 48kHz, 96kHz and 192kHz sampling frequen-
cies. These frequencies represent ranging resolutions of 7.08mm,
3This project has obtained IRB approval.

1085P

P

P

P

,QWLPDWH
6SDFH
3HUVRQDO
6SDFH

6RFLDO6SDFH

3XEOLF6SDFH

)

%

(
 
e
t
a
R

 
t
p
e
c
c
A
 
e
u
r
T

100

95

90

85

80
0

0.02

Correlation
Probability
Combined method

0.04

0.06

False Accept Rate (%)

0.08

0.1

Figure 12: Illustration of locations of replace attacks
and diÔ¨Äerent types of social distances.

Figure 13: Playback Attacks: ROC curves under
diÔ¨Äerent methods.

3.54mm, and 1.77mm, respectively. We use 192kHz as our
primary sampling frequency and present the corresponding
results unless otherwise stated. We also experiment with
two types of phone placements, as shown in Figure 11. One
is vertical placement with the phone placed close to user‚Äôs
mouth vertically. We call such placement our primary place-
ment and present the performance of such placement unless
otherwise speciÔ¨Åed. For the vertical placement, the phone
is about 3cm and 1cm away from user‚Äôs mouth on Z and
Y axis, respectively. The other one is horizontal placement
with the phone placed close to the user‚Äôs mouth horizontally.
The phone is about 6cm and 1cm away from user‚Äôs mouth
on Z and Y axis, respectively. We choose these placements
because they have relatively large achievable TDoA ranges,
which is discussed in Section 5.

Data Collection. Our experiments involve 12 partici-
pants including 6 males and 6 females whose ages range be-
tween 25 to 38. These participants are either graduate stu-
dents or university researchers, who are recruited by emails.
The participants are informed of the purpose of our exper-
iments and are required to act as if they were conducting
voice authentication. Each participant chooses 10 diÔ¨Äerent
passphrases of their own and performs 10 times legitimate
authentications for each passphrase after enrollment. To en-
roll in the system, each participant speaks a passphrase three
times to extract the averaged TDoA and the standard devi-
ation of each phoneme for similarity comparison. For online
veriÔ¨Åcation, users only speak the passphrase once. Each par-
ticipant speaks the passphrase with her/his habitual way of
speaking. The lengths of the passphrases are ranging from
2 words to 10 words with proximately half of them are 2-
4 words, one quarter of them are 5-7 or 8-10 words. The
experiments are conducted in both the oÔ¨Éce and home en-
vironments with background and ambient noises, such as
people chatting and HVAC noise.

Attacks. We experiment with two types of replay at-
tacks: playback attacks and replace attacks. For playback
attacks, we replay participants‚Äô voice samples in front of the
smartphone that performs stereo recording for authentica-
tion. We utilize three diÔ¨Äerent types of loudspeaker includ-
ing DELL AC411 wireless speaker system, Samsung Galaxy
note5 and S5 speakers, to replay each pre-recorded voice
sample. In addition, half of the playback attacks are con-
ducted with stationary loudspeakers that are within 10cm
away from the smartphone (i.e., Static Playback Attacks);
while the other half are conducted with mobile loudspeakers
targeting on mimicking TDoA changes of users by moving
the loudspeakers around the smartphone (i.e., Mobile Play-
back Attacks).

For replace attacks, we place a smartphone with stereo

)

%

(
 

y
c
a
r
u
c
c
A

100

95

90

85

80

Correlation

Probability Combined Method

)

%

(
 

R
E
E

4

3

2

1

0

Correlation

Probability Combined Method

(a) Accuracy

(b) EER

Figure 14: Playback Attacks: Accuracy and EER.

recording close to the target user when the user is performing
legitimate voice authentication. In such cases, the adversary
obtains a two-channel voice sample of the target and then
uploads that directly to the voice authentication system.
The only diÔ¨Äerence between the two-channel voice sample
obtained by the adversary and the one in legitimate authen-
tication is the recording distance. We adopt the Edward T.
Hall‚Äôs proxemics theory [15] to emulate how close an adver-
sary could place the phone next to the user‚Äôs mouth. As
shown in Figure 12, the minimum distances between people
are categorized by the relationship and types of interactions
between them. It includes intimate distance, personal dis-
tance, social distance, and public distance. With such a
guideline, we chose the recording distances between the at-
tacker‚Äôs phone to the user‚Äôs mouth as 30cm, 50cm, 100cm,
150cm, 200cm, 300cm, and 450cm, which simulates diÔ¨Äerent
types of relationships. We also consider the circumstances
where the attacker could hide behind or at the side of the
user. The recording distances for such cases are limited by
the size of user‚Äôs head, and are around 40 cm and 25 cm
away to user‚Äôs mouth, as shown in Figure 12.

Metrics. We use the following metrics to evaluate the
performance of our liveness detection system. False Accept
Rate (FAR): the probability that the liveness detection sys-
tem incorrectly declares a replay attack as a live user. False
Reject Rate (FRR): the probability that our system mis-
takenly classiÔ¨Åes a live user as a replay attack. Receiver
Operating Characteristic (ROC): it describes the relation-
ship between the True Accept Rate (i.e., the probability to
identify a live user as a live user) and the FAR when varying
the detection threshold. Equal Error Rate (EER): it shows
a balanced view of the FAR and FRR and is deÔ¨Åned as the
rate at which the FAR equals to the FRR. Accuracy: it mea-
sures the overall probability that the system could detect a
live user and reject a replay attack.
4.2 Overall Performance

We Ô¨Årst evaluate the overall performance of our liveness
detection system under two types of replay attacks: playback
attacks and replace attacks.

1086)

%

(
 
y
c
a
r
u
c
c
A

100

80

60

40

20

0

Correlation
Probability
Combined Method
Mobile

Static

100

99.9

99.8

99.7

99.6

99.5

)

%

(
 
y
c
a
r
u
c
c
A

0.43

99.72

99.72

0.10

0.09

99.47

99.4

30

50

100

100

100

100

100

0.6

0.5

)

%

(
 

R
E
E

0.4

0.3

0.2

0.1

Accuracy
EER

0.00
150

0.00
200

0.00
300

0.00

0
450

Figure 15: Static and Mobile Playback Attacks: Ac-
curacy under diÔ¨Äerent methods.

Figure 17: Replace Attacks: EER and Accuracy of
Combined method under diÔ¨Äerent distances.

)

%

(
 
y
c
a
r
u
c
c
A

100

98

96

94

92

90

Correlation
Probability
Combined Method

Intimate

Personal Space

Social and Beyond

)

%

(
 
y
c
a
r
u
c
c
A

100

99

98

97

96

95

98.78

96.60

96.11

2-4

100.00

99.60

99.65

98.77

98.42

Correlation
Probability
Combined Method

5-7

Passphrase Length

>7

Figure 16: Replace Attacks: Accuracy of diÔ¨Äerent
methods with diÔ¨Äerent social distances.

Figure 18: Accuracy under diÔ¨Äerent lengths of
passphrase.

Playback Attack. Figure 13 shows the ROC curves of
diÔ¨Äerent methods in detecting live users under playback at-
tacks. We observe that our system is highly eÔ¨Äective in
detecting live users and rejecting playback attacks. These
three methods provide more than 94% detection rate with
less than 1% FAR. In particular, the correlation and prob-
ability based methods have comparable performance. The
correlation method provides a detection rate of 95% with 1%
FAR. The combined method has the best performance and
results in over 99% detection rate with less than 1% false
accept rate.

Moreover, Figures 14 depicts the overall accuracy and
EER of diÔ¨Äerent methods under playback attacks. We ob-
serve that the combined method provides the best accuracy
and EER, which are 99.30% and 1.05% respectively. The
correlation method produces an accuracy of 97.95%, which
is slightly better than that of the probability method (i.e.,
97.54%). However, probability method results in a better
EER than that of the correlation method.
In particular,
probability method has an EER of 2.50% and correlation
method has an EER of 3.24%. The above results show that
VoiceLive is highly accurate in detecting live users under
playback attacks, and the combined method provides the
best results since it takes advantages of both the correction
and the probability based methods.

We next take a closer look at how our system performs
under static and mobile playback attacks.
In our experi-
ments, we observe the static playback attacks produce simi-
lar TDoA values for diÔ¨Äerent types of phoneme sounds. Al-
though playback attacks under mobile scenarios could re-
sult in TDoA changes, the resulted changes in TDoA can-
not match with the ones in the user proÔ¨Åle. It is because
the attacker couldn‚Äôt mimic the sound position transition
the same as that of the human vocal system. As shown in
Figure 15, our system is highly eÔ¨Äective in live user detec-
tion under both static and mobile playback attacks. The
combined method achieves 99.2% accuracy under static sce-
narios and 99.65% accuracy under mobile cases.

Replace Attack. We next evaluate the eÔ¨Äectiveness of
our system in defending against the replace attacks. Fig-
ure 16 illustrates the accuracy of diÔ¨Äerent methods with re-

place attacks conducted under diÔ¨Äerent social distances. In
particular, the testing positions of replace attacks fall into
intimate (<45cm), personal space (45cm
three categories:
to 1.2m), and social and beyond (>1.2m). We observe that
our system can eÔ¨Äectively detect the live users and reject the
replace attacks under each category of social distances. For
example, the combined method provides 99.47% detection
accuracy under intimate relationship, 99.82% under personal
relationship, and 100% under social relationship and public
space. And all the methods provide over 98.95% detection
accuracy across diÔ¨Äerent categories.

Figure 17 shows the details on the accuracy and EER
of the combined method under each social distance. We
Ô¨Ånd that both the accuracy and EER are improved with an
increased social distance. In particular, the EER decreases
from 0.43% to 0% and the accuracy is improved from 99.47%
to 100% when the distance is increased from 30cm to 150cm.
When the attacker is further away, our system can detect all
the live user cases and reject all the replace attacks. This
is because when increasing the distance between the phone
and user‚Äôs mouth, the TDoA dynamic diminishes rapidly.

We also investigate the replace attacks launched from 25cm
behind the user and 40cm from the side of user. The EER of
the combined method under these two cases are 0.33% and
0%, respectively. Such results are comparable to the EER
in Figure 17. This shows our system is capable of detecting
replace attack conducted from diÔ¨Äerent directions.
4.3

Impact of Passphrase Length

Generally, a passphrase with longer length provides stronger

security. It also produces more phoneme sounds that gen-
erate more changes in the TDoA measurements. We thus
study the performance of our system with diÔ¨Äerent lengths
of passphrases.
In particular, we sort all the passphrases
into three categories including short passphrases with 2 to 4
words, appropriate passphrasses with 5 to 7 words, and long
passphrases with 8 to 10 words. Note that researchers and
professionals in voice authentication suggest that a passphrase
should contain at least 5 words so as to provide suÔ¨Écient se-
curity level [7].

Figure 18 and Figure 19 illustrate the accuracy and EER

1087)

%

(
 

R
E
E

5

4

3

2

1

0

4.24

3.93

1.78

2-4

Correlation
Probability
Combined Method

2.50
1.62

1.00

5-7

Passphrase Length

0.32

0.00

>7

)

%

(
 

R
E
E

15

10

5

0

Correlation
Probability
Combined Method

48

96

Sampling Frequency (kHz)

192

Figure 19:
passphrase.

EER under diÔ¨Äerent

lengths of

Figure 21: EER under diÔ¨Äerent sampling frequen-
cies.

)

%

(
 
y
c
a
r
u
c
c
A

100

95

90

85

80

Correlation
Probability
Combined Method

48

96

Sampling Frequency (kHz)

192

)

%

(
 
y
c
a
r
u
c
c
A

100

80

60

40

20

0

Note5

Note3/S5

Note3

Note5/S5

Correlation
Probability
Combined method

S5

Note5/Note3

Figure 20: Accuracy under diÔ¨Äerent sampling fre-
quencies.

Figure 22: Accuracy of using one phone as enroll-
ment and the other two as online authentication.

for diÔ¨Äerent lengths of passphrases, respectively. We observe
that both the accuracy and EER are improved for all the
methods when we increase the length of the passphrase. In
particular, the accuracy is improved from 98.78% to 100%
and the EER is reduced from 1.78% to 0% for combined
method, when we increase the length from 2-4 words to
more than 7 words. In addition, with an appropriate length
of passphrase (i.e., 5 to 7 words), the combined method re-
sults in 99.65% accuracy and 1% EER. The results con-
Ô¨Årm our observation that a longer passphrase leads to more
TDoA changes of phoneme sounds, which improves the per-
formance of the live user detection.
4.4 Effect of Sampling Frequency

As not all the smartphones are installed with the latest
OS, older version of OSs can only support the standard sam-
pling frequency at 48kHz or 96kHz. We thus study how ro-
bust our system is to lower sampling frequencies. Figure 20
and Figure 21 show the accuracy and EER under diÔ¨Äer-
ent sampling frequencies respectively. We observe that our
system works eÔ¨Äectively under all of these three sampling
frequencies. Although with higher sampling rates, it does
have better accuracy and EER due to higher ranging reso-
lution. In particular, for combined method the accuracy is
over 95% under 48kHz, and over 97% under 96kHz, whereas
the EER is at around 3% for both 48kHz and 96kHz. These
results show that our liveness detection system could work
with both the state-of-the-art smartphones as well as low-
end smartphones.
4.5

Impact of Different Phones

As one user may use one phone to enroll in the system but
uses another one to perform online authentication, we study
how our system behaves under diÔ¨Äerent phones. SpeciÔ¨Åcally,
we experiment with users to use either Note5, Note3, or S5 to
enroll in the system and then utilize the other two for online
authentication. These three types of phones diÔ¨Äer in size and
audio hardware as described in the experimental setup. Fig-
ure 22 shows the accuracy of diÔ¨Äerent methods when using
one phone as enrollment and the others as online authenti-
cation. We observe that our system still provides accurate

liveness detection. In particular, the combined method re-
sults in accuracy of 97.82%, 96.67%, 96% when using Note5,
Note3, and S5 as the phone for enrollment while the other
two for authentication, respectively. Moreover, we observe
that these three methods have comparable performance no
matter which phone is used for enrollment or authentica-
tion. Although the accuracy is slightly higher when using
the same phone (i.e., about 99%), our system still produces
very accurate detection results with diÔ¨Äerent phones. Such
observations show that our system is robust and compatible
to diÔ¨Äerent phone models.
4.6 Robustness to Phone Displacement

When performing online authentication, our system re-
quires the user to place the phone at a similar position to
that when the user enrolled in the system. We thus study
our system‚Äôs performance if there exists displacement of the
phone between the position for enrollment and online au-
thentication. SpeciÔ¨Åcally, we experiment with diÔ¨Äerent de-
grees of phone displacements when performing authentica-
tion, i.e., 1cm, 2cm, and 3cm away from the position that
user enrolled in the system. Such displacements occur on
each axis: Left (X axis), Down (Z axis), and Forward (Y
axis). Figure 23 and Figure 24 show the accuracy and EER
under diÔ¨Äerent degrees of phone displacements, respectively.
We observe that although a higher degree of displacement
results in lower accuracy and higher EER, our system over-
all still provides accurate detection results: the accuracy
is more than 98% for all the displacements and EER is also
maintained at a very low rate, ranging from 1% to 3%, when
using combined method. Moreover, we Ô¨Ånd that our system
is more sensitive to the displacements on Y axis (i.e., For-
ward) and less sensitive to X axis (i.e., Left). This is be-
cause by moving the phone forward, the maximum achiev-
able TDoA range will be reduced quickly with the increased
distance, as shown in Section 5. The results may also indi-
cate VoiceLive is not sensitive to the small movements of the
phone (e.g., hand movements). In addition, the time dura-
tion of speaking a passphrase is usually about 2-3 seconds.
The movements of the phone within such a short duration
are usually small and may have limited eÔ¨Äect.

1088)

%

(
 

y
c
a
r
u
c
c
A

100

98

96

94

92

90

Correlation
Probability
Combined Method

1cm 2cm 3cm 1cm 2cm 3cm 1cm 2cm 3cm

Left

Down

Forward

Figure 23: Accuracy under diÔ¨Äerent degree of phone
displacement.

)

%

(
 

R
E
E

10

8

6

4

2

0

Correlation
Probability
Combined Method

1cm 2cm 3cm 1cm 2cm 3cm 1cm 2cm 3cm

Left

Down

Forward

Figure 24: EER under diÔ¨Äerent degrees of phone
displacement.
4.7 Effect of Phone‚Äôs Placement

DiÔ¨Äerent users might have diÔ¨Äerent ways to place the
phone close to their mouths during the authentication pro-
cess. We thus compare the performance of our system un-
der two types of placements, vertical and horizontal, as de-
scribed in experimental setup. Figure 25 illustrates the ac-
curacy comparison of these two placements. We observe
that our system achieves very high accuracy for both place-
ments, with the accuracy slightly higher when the phone is
placed vertically. SpeciÔ¨Åcally, the accuracy under horizontal
placement is 97.41%, 97.26%, and 99.13% for correlation,
probability, and combined method respectively. Figure 26
shows the EER under two displacements. We have simi-
lar observation to that of the accuracy. In particular, EER
is 3.3%, 2.69%, and 1.33% for correlation, probability, and
combined method respectively. Results show that our sys-
tem works very well for diÔ¨Äerent phone placements including
both horizontal and vertical placements.

5. DISCUSSION AND FUTURE WORK

Achievable TDoA Range. The achievable TDoA range
is determined by the distance between two microphones and
is aÔ¨Äected by the relative position between the phone and
user‚Äôs mouth. Figure 27 shows the achievable TDoA range
with Samsung Galaxy Note3 by using the sound origin model
we built in Figure 5. The distance between two microphones
is 15.1cm. Figure 27 plots the sectional view on Y-Z plane
and the coordinate (0,0) is the location of the mouth. Each
(y, z) point in vertical placement indicates the center of the
phone when place the phone vertically, whereas it represents
the bottom microphone of the phone when place phone hor-
izontally. The color at each position represents the achiev-
able TDoA range at that position. As we can see from Fig-
ure 28, the maximum achievable TDoA range is around 6cm
for vertical placement, whereas it is about 4cm for horizontal
placement if we place the phone very close to user‚Äôs mouth.
The reason we cannot achieve maximum TDoA range as the
distance between two microphones is that the origin of the

)

%

(
 
y
c
a
r
u
c
c
A

100

80

60

40

20

0

Correlation
Probability
Combined Method

Horizontal

Vertical

Figure 25: Accuracy of horizontal and vertical place-
ments.

)

%

(
 

R
E
E

4

3

2

1

0

Correlation
Probability
Combined Method

Horizontal

Vertical

Figure 26: EER of horizontal and vertical place-
ments.

phoneme sound is crowded in user‚Äôs mouth and nasal cavities
(i.e., all are at similar directions to the two microphones).
Such maximum achievable TDoA ranges (i.e., 6cm and 4cm)
could ideally distinguish 33 and 23 diÔ¨Äerent phoneme sounds
under 192kHz sampling rate, respectively. We also Ô¨Ånd that
the achievable TDoA range decreases rapidly when increas-
ing the distance between the phone and the mouth. For
example, with the phone placed at 30cm away from user‚Äôs
mouth, the achievable TDoA range decreases to less than
1cm, which makes it hard to capture any TDoA dynamic
of a passphrase. This is why our system is robust to the
replace attack, where an adversary attempts to record the
TDoA dynamic under diÔ¨Äerent social distances.

Potential Active Attacks. In our experiments, we only
evaluate our system under the scenarios that an adversary
uses similar recording hardware as the one used by the le-
gitimate users. However, it is possible for an attacker to use
advanced hardware to record the voice samples and further
deduce the TDoA dynamic that can match the victim‚Äôs pro-
Ô¨Åle. In particular, an attacker can leverage a microphone ar-
ray to locate each phoneme within the victim‚Äôs vocal system.
As the maximum achievable TDoA range decreases rapidly
with the increased distance between the recorder and the
user‚Äôs mouth, it requires the microphone array to support
an ultra-high sampling rate so as to have suÔ¨Écient ranging
resolution to uniquely locate each phoneme. For example,
with the microphone array placed 30cm away from the user,
the maximum achievable TDoA range is less than 1cm. To
uniquely locate each phoneme, the ranging resolution should
be at least 0.2mm, which is ten times of that supported by
192kHz. Current professional digital recorders (e.g., Direct-
Stream Digital (DSD) recorders that worth thousands of
dollars and have the sizes similar to a desktop mainframe)
that support 2.8224MHz and 5.6448MHz sampling rates can
be leveraged to locate each phoneme without placing the
recorder very close to victim‚Äôs mouth.

After locating each phoneme, the attacker can deduce the
TDoA dynamic of the victim based on the relative position
between the phone and the victim‚Äôs mouth. This does re-
quire the attacker to observe how the phone is placed to
the victim‚Äôs mouth. Given the obtained TDoA dynamic,

108910

1
10

1
10

1
10

D

C

B

A

1

1

A

10 1

10 1

B

C

10 1

D

10

1

0.8

0.6

0.4

0.2

0

-0.2

-0.4

-0.6

-0.8

Figure 28: Similarity of TDoA dynamics between
diÔ¨Äerent users.

ual way of pronouncing. It also shows that it is promising
to use the TDoA dynamic or the location of phoneme as a
new biometric trait for user authentication. In our future
work, we will study the possibility of verifying or identify-
ing the speaker by making a model using the location of the
phoneme sound.
6. RELATED WORK

In recent years, more and more mobile devices and apps
are embracing voice biometric for mobile authentication.
However, voice authentication is subject to spooÔ¨Ång attacks,
as indicated in recent studies [16, 33, 14, 26]. Voice spoof-
ing attacks can be divided into four categories, which are
described below together with countermeasures.

Replay Attack. An adversary can spoof a voice authen-
tication system by using a pre-recorded voice sample of the
victim [24]. To defend against such attacks, Shang et al.
propose to compare a new access voice sample with stored
instances of past access attempts [31]. If this results in an
extremely high similarity score, a replay attack is identiÔ¨Åed.
As an alternative, Villalba et al. utilize the increased noise
and reverberation of replaying far-Ô¨Åeld recordings for attack
detection [32], whereas Wang et al. use the additional chan-
nel noise of the recording and loudspeaker for attack detec-
tion [33]. However, the eÔ¨Äectiveness of these approaches is
very limited in practice (e.g., the FAR rate could be as high
as 17%.). Chetty and Wagner utilize video camera to detect
lip movements for liveness detection [13], whereas Poss et
al. aim to improve authentication accuracy by combining
the techniques of a neural tree network and Hidden Markov
Models [28]. Aley-Raz et al. develop a liveness detection
system, which requires a user to repeat one or more random
sentences prompted by the system for attack detection [10].
Impersonation Attack. It refers to attacks where an
adversary tries to mimic the victim‚Äôs voice without utilizing
any computer or professional devices. Recent work shows
that impersonation attack could be defended very eÔ¨Éciently
by using advanced speaker models, such as GMM-UBM [11]
and i-vector models [16]. Existing voice authentication sys-
tems with such advanced speaker models thus are resistant
to impersonation attacks.

Speech Synthesize Attack. This type of attack indi-
cates an attacker has the ability to synthesize the victim‚Äôs
voice by utilizing speech synthesize technologies. Earlier
work done by Lindberg and Blomberg [24] shows that the
FAR can be increased to as high as 38.9% with less sophisti-
cated speaker models. Recent work done by De Leon et al.
shows that by adopting both GMM-UBM and SVM tech-

(a) Vertical placement (b) Horizontal place-

ment

Figure 27: The achievable TDoA range under both
vertical and horizontal placements.

the attacker is further required to reproduce the voice sam-
ples that satisfy the TDoA constraints. It could be done by
creating a synthetic two-channel audio stream. With such
an audio stream, the attacker can either conduct a replace
attack or a playback attack to bypass the VoiceLive.

In our future work, we will study the feasibility of con-
ducting such active attacks. Particularly, we will evaluate
whether or not current acoustic localization systems could
achieve the level of localization accuracy required in the ac-
tive attacks. The potential countermeasure is to detect the
synthetic two-channel audio stream. VoiceLive could inte-
grate with existing speaker veriÔ¨Åcation techniques, such as
the higher order Mel-cepstral coeÔ¨Écients [14, 12], which are
able to detect speech synthesize attacks. We will evaluate
the eÔ¨Äectiveness of detecting the synthetic two-channel au-
dio stream with these techniques in our future work.

Extension to Text-Independent System. As a text-
independent system operates on arbitrary utterances, we
cannot rely on the TDoA dynamic of a passphrase for live-
ness detection. However, a text-independent system requires
collecting a large number of utterances from the user to train
its speaker models. We therefore can extract the TDoA
value of each phoneme sound to build a model similar to
that of the Figure 5 by re-using the training data when the
system trains the speaker models. During the online authen-
tication phase, we extract the TDoA value of each phoneme
from the incoming utterances and then could build another
model. Such a model (could be a sub-model of the trained
model) can then be matched with the one trained during
the training phase. It is thus still possible to use the loca-
tion of each phoneme sound for liveness detection in text-
independent systems.

Diversity in Human Vocal System. An individual‚Äôs
vocal system diÔ¨Äers in the shape and size of the larynx,
nasal passages and vocal tract.
In addition, diÔ¨Äerent in-
dividuals have their own habitual ways of pronouncing the
same word, which results in diÔ¨Äerent cadences, accents and
pronunciations. We thus investigate how similar are the ex-
tracted TDoA dynamics for diÔ¨Äerent users with the same
passphrase. Figure 28 depicts the similarity of the extracted
TDoA dynamics for the same passphrase between four users:
A, B, C, and D. Each user speaks the same passphrase 10
times, and we measure the similarity within each user and
between users using Pearson correlation coeÔ¨Écients. We ob-
serve that the correlation coeÔ¨Écients for the same user under
diÔ¨Äerent trials are very high, at around 0.9, whereas they are
generally below 0.6 between diÔ¨Äerent users. This indicates
that the diversity in TDoA dynamic does exist, which is
similar to that of individual vocal system and user‚Äôs habit-

1090nologies, voice authentication systems are able to lower the
FAR of the system to 2.5% [14]. Also, Chen et al. [12] show
that by employing higher order Mel-cepstral coeÔ¨Écients, the
EER can be lowered to 1.58%.

Voice Conversion Attack.

It aims at manipulating
or converting existing voice samples from other users so
that they would resemble the target‚Äôs voice.
In the early
work, researchers demonstrate such attacks can signiÔ¨Åcantly
aÔ¨Äect the authentication system [19]. Recent studies by
Mukhopadhyay et al.
show that current speaker veriÔ¨Åca-
tion systems based on UBM-GMM and ISV speaker models
are vulnerable to voice conversion attacks [26]. To defend
against voice conversion attacks, Wu et al. [34] developed
an authentication system with PLDA component that could
achieve 1.71% FAR, whereas Alegre et al. utilize PLDA and
FA technologies, which result in the FAR rate of 1.6% [9].
7. CONCLUSION

In this work, we developed a liveness detection system
for voice authentication that requires only stereo recording
on smartphones. Our system VoiceLive is practical as no
additional hardware is required during the authentication
process. VoiceLive performs liveness detection by measur-
ing TDoA changes of a sequence of phoneme sounds from
the two microphones of a smartphone. It distinguishes a live
user from a replay attack by comparing the TDoA changes
of the input utterance to the one stored in the system. Our
experimental evaluation demonstrates the viability of dis-
tinguishing between a live user and a replay attack under
various experimental settings. Our experimental results also
show the generality of our system, as we experiment with dif-
ferent phone types, placements and sampling rates. Overall,
VoiceLive can achieve over 99% accuracy, with an EER as
low as 1%.
8. ACKNOWLEDGEMENTS

We thank our shepherd, Dr. Nitesh Saxena, and the
anonymous reviewers for their insightful feedbacks. This
work was partially supported by the National Science Foun-
dation Grants CNS-1514436, SES-1450091, CNS-1505175,
CNS-1652447 and CNS-1514238.

9. REFERENCES
[1] Android voice recognition. http:

//www.popsci.com/new-android-can-recognize-your-voice.

[2] Google smart lock. https://get.google.com/smartlock/.
[3] Hsbc oÔ¨Äers voice biometric.

http://www.bbc.com/news/business-35609833.

[4] Mobile voice biometric security. http://voicevault.com/

hsbc-embraces-mobile-voice-biometric-security-technology/.

[5] Saypay technologies. http://saypaytechnologies.com/.
[6] Vocalpassword.

http://www.nuance.com/ucmprod/groups/enterprise/
@web-enus/documents/collateral/nc 015226.pdf.

[7] Voicekey mobile applications. http://speechpro-usa.com/

product/voice authentication/voicekey#tab2.

[8] Wechat voiceprint. http://blog.wechat.com/2015/05/21/

voiceprint-the-new-wechat-password/.

[9] F. Alegre, A. Amehraye, and N. Evans. A one-class

classiÔ¨Åcation approach to generalised speaker veriÔ¨Åcation
spooÔ¨Ång countermeasures using local binary patterns. In
IEEE BTAS, 2013.

[10] A. Aley-Raz, N. M. Krause, M. I. Salmon, and R. Y. Gazit.

Device, system, and method of liveness detection utilizing
voice biometrics, May 14 2013. US Patent 8,442,824.

[11] T. B. Amin, J. S. German, and P. Marziliano. Detecting
voice disguise from speech variability: Analysis of three
glottal and vocal tract measures. The Journal of the
Acoustical Society of America, 2013.

[12] L.-W. Chen, W. Guo, and L.-R. Dai. Speaker veriÔ¨Åcation

against synthetic speech. In 2010 IEEE Chinese Spoken
Language Processing (ISCSLP), 2010.

[13] G. Chetty and M. Wagner. Automated lip feature

extraction for liveness veriÔ¨Åcation in audio-video
authentication. Proc. Image and Vision Computing, 2004.

[14] P. L. De Leon, M. Pucher, J. Yamagishi, I. Hernaez, and

I. Saratxaga. Evaluation of speaker veriÔ¨Åcation security and
detection of hmm-based synthetic speech. IEEE Processing
of Audio, Speech, and Language, 2012.

[15] E. Hall. Handbook for proxemic research. Anthropology

News, 1995.

[16] R. G. Hautam¬®aki, T. Kinnunen, V. Hautam¬®aki, T. Leino,

and A.-M. Laukkanen. I-vectors meet imitators: on
vulnerability of speaker veriÔ¨Åcation systems against voice
mimicry. In INTERSPEECH, 2013.

[17] A. Jain, R. Bolle, and S. Pankanti. Biometrics: personal

identiÔ¨Åcation in networked society. Springer Science &
Business Media, 2006.

[18] T. Kevenaar. Protection of biometric information. In

Security with Noisy Data. 2007.

[19] T. Kinnunen et al. Vulnerability of speaker veriÔ¨Åcation

systems against voice conversion spooÔ¨Ång attacks: The case
of telephone speech. In IEEE ICASSP, 2012.

[20] A. Kipp, M.-B. Wesenick, and F. Schiel. Automatic

detection and segmentation of pronunciation variants in
german speech corpora. In IEEE ICSLP, 1996.

[21] T. Kisler, F. Schiel, and H. Sloetjes. Signal processing via

web services: the use case webmaus. In Digital Humanities
Conference, 2012.

[22] C. H. Knapp and G. C. Carter. The generalized correlation

method for estimation of time delay. IEEE Processing of
Acoustics, Speech and Signal, 1976.

[23] P. Ladefoged. A course in phonetics. Hardcourt Brace

Jovanovich Inc. NY, 2014.

[24] J. Lindberg, M. Blomberg, et al. Vulnerability in speaker

veriÔ¨Åcation-a study of technical impostor techniques. In
Eurospeech, 1999.

[25] J. Liu, Y. Wang, G. Kar, Y. Chen, J. Yang, and

M. Gruteser. Snooping keystrokes with mm-level audio
ranging on a single phone. In ACM MobiCom, 2015.

[26] D. Mukhopadhyay, M. Shirvanian, and N. Saxena. All your
voices are belong to us: Stealing voices to fool humans and
machines. In European Symposium on Research in
Computer Security, 2015.

[27] J. P. Olive, A. Greenwood, and J. Coleman. Acoustics of
American English speech: a dynamic approach. Springer
Science & Business Media, 1993.

[28] J. C. Poss, D. Boye, and M. W. Mobley. Biometric voice

authentication, June 10 2008. US Patent 7,386,448.
[29] M. K. Ravishankar. EÔ¨Écient algorithms for speech

recognition. Technical report, DTIC Document, 1996.

[30] M. A. Redford. The handbook of speech production. John

Wiley & Sons, 2015.

[31] W. Shang and M. Stevenson. Score normalization in
playback attack detection. In IEEE ICASSP, 2010.

[32] J. Villalba and E. Lleida. Detecting replay attacks from

far-Ô¨Åeld recordings on speaker veriÔ¨Åcation systems. In
Biometrics and ID Management. 2011.

[33] Z.-F. Wang, G. Wei, and Q.-H. He. Channel pattern noise

based playback attack detection algorithm for speaker
recognition. In IEEE ICMLC, 2011.

[34] Z. Wu, T. Kinnunen, E. Chng, and H. Li. A study on

spooÔ¨Ång attack in state-of-the-art speaker veriÔ¨Åcation: the
telephone speech case. In IEEE APSIPA ASC, 2012.

[35] J. Yang, Y. Chen, and W. Trappe. Detecting spooÔ¨Ång

attacks in mobile wireless environments. In SECON, 2009.

1091