MiddlePolice: Toward Enforcing Destination-Deï¬ned

Policies in the Middle of the Internet

Zhuotao Liuâˆ—

Hao Jinâ€ 

Yih-Chun Huâˆ—

Michael Baileyâˆ—

âˆ— University of Illinois at Urbana-Champaign, â€  Nanjing University
âˆ— {zliu48, yihchun, mdbailey}@illinois.edu, â€  jinhaonju@gmail.com

ABSTRACT
Volumetric attacks, which overwhelm the bandwidth of a
destination, are amongst the most common DDoS attacks
today. One practical approach to addressing these attacks is
to redirect all destination traï¬ƒc (e.g., via DNS or BGP) to
a third-party, DDoS-protection-as-a-service provider (e.g.,
CloudFlare) that is well provisioned and equipped with ï¬l-
tering mechanisms to remove attack traï¬ƒc before passing
the remaining benign traï¬ƒc to the destination. An alterna-
tive approach is based on the concept of network capabili-
ties, whereby source sending rates are determined by receiver
consent, in the form of capabilities enforced by the network.
While both third-party scrubbing services and network ca-
pabilities can be eï¬€ective at reducing unwanted traï¬ƒc at
an overwhelmed destination, DDoS-protection-as-a-service
solutions outsource all of the scheduling decisions (e.g., fair-
ness, priority and attack identiï¬cation) to the provider, while
capability-based solutions require extensive modiï¬cations to
existing infrastructure to operate. In this paper we intro-
duce MiddlePolice, which seeks to marry the deployability of
DDoS-protection-as-a-service solutions with the destination-
based control of network capability systems. We show that
by allowing feedback from the destination to the provider,
MiddlePolice can eï¬€ectively enforce destination-chosen poli-
cies, while requiring no deployment from unrelated parties.

1.

INTRODUCTION

Attacks against availability, such as distributed denial of
service attacks (DDoS), continue to plague the Internet. The
most common of these attacks, representing roughly 65% of
all DDoS attacks last year [40], are volumetric attacks. In
these attacks, adversaries seek to deny service by exhaust-
ing a victimâ€™s network resources and causing congestion.
Such attacks are diï¬ƒcult for a victim network to mitigate
as the largest of these attacks can exceed the available up-
stream bandwidth by orders of magnitude. For example,
Internet service providers (ISP) reported attacks in excess
of 500 Gbps in 2015 [40].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proï¬t or commercial advantage and that copies bear this notice and the full citation
on the ï¬rst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciï¬c permission
and/or a fee. Request permissions from permissions@acm.org.
CCSâ€™16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978306

One common solution to this problem is the use of DDoS-
protection-as-a-service providers, such as CloudFlare. These
providers massively over-provision data centers for peak at-
tack traï¬ƒc loads and then share this capacity across many
customers as needed. When under attack, victims use DNS
or BGP to redirect traï¬ƒc to the provider rather than their
own networks. The DDoS-protection-as-a-service provider
applies a variety of techniques to scrub this traï¬ƒc, sepa-
rating malicious from benign, and then re-injects only the
benign traï¬ƒc back into the network to be carried to the vic-
tim. Such methods are appealing, as they require no modi-
ï¬cation to the existing network infrastructure and can scale
to handle very large attacks. However, these cloud-based
systems use proprietary attack detection algorithms and ï¬l-
tering which limit the ability of customers to prioritize traï¬ƒc
kinds or choose preferred scheduling policies. Further, exist-
ing cloud-based systems assume that all traï¬ƒc to the victim
will be routed ï¬rst to their infrastructure, an assumption
that can be violated by a clever attacker [38, 47].

A second approach to solving volumetric DDoS attacks is
network capability-based solutions [8,11,12,34,41,42,50,51].
Such systems require a source to receive explicit permission
before being allowed to contact the destination. Such ca-
pabilities are enforced by the network infrastructure itself
(i.e., routers) and capabilities range from giving the victim
the ability to block traï¬ƒc from arbitrary sources to giving
the victim control over the bandwidth allowed for each ï¬‚ow.
A major advantage, then, of these capability-based systems
is the ability of the victim to control precisely what and how
much traï¬ƒc it wants to receive. However, these capability-
based systems are not without challenges, and most face sig-
niï¬cant deployment hurdles. For instance, approaches such
as TVA [50] and NetFence [34] require secret key manage-
ment and router upgrades across diï¬€erent Autonomous Sys-
tems (ASes). Yet other approaches require clients to modify
their network stack to insert customized packet headers, cre-
ating additional deployment hurdles.

In this paper, we present MiddlePolice, which seeks to
combine the deployability of cloud-based solutions with the
destination-based control of capability-based systems. Mid-
dlePolice is built on a set of traï¬ƒc policing units (referred
as mboxes) which rely on a feedback loop of self-generated
capabilities to guide scheduling and ï¬ltering. MiddlePolice
also includes a mechanism to ï¬lter nearly all traï¬ƒc that
tries to bypass the mboxes, using only the ACL conï¬gura-
tion already present on commodity routers. We implement
MiddlePolice as a Linux Kernel Module, and evaluate it ex-
tensively over the Internet using cloud infrastructures, on

1268our private testbed, and via simulations. Our results show
that MiddlePolice can handle large-scale DDoS attacks, and
eï¬€ectively enforce the destination-chosen policies.
2. PROBLEM FORMULATION
2.1 MiddlePoliceâ€™s Desirable Properties
Readily Deployable and Scalable. MiddlePolice is de-
signed to be readily deployable in the Internet and suï¬ƒ-
ciently scalable to handle large scale attacks. To be readily
deployable, a system should only require deployment at the
destination, and possibly at related parties on commercial
terms. The end-to-end principle of the Internet, combined
with large numbers of end points, is what gives rise to its
tremendous utility. Because of the diversity of administra-
tive domains, including end points, edge-ASes, and small
transit ASes, ASes have varying levels of technological so-
phistication and cooperativeness. However, some ASes can
be expected to help with deployment; many ISPs already
provide some sort of DDoS-protection services [2], so we
can expect that such providers would be willing to deploy
a protocol under commercially reasonable terms. We con-
trast this with prior capability-based work, which requires
deployment at a large number of unrelated ASes in the In-
ternet and client network stack modiï¬cation, that violates
the deployability model.

The goal of being deployable and scalable is the major
reason that MiddlePolice is designed to be built into existing
cloud-based DDoS defense systems.
Destination-driven Policies. MiddlePolice is designed to
provide the destination with ï¬ne-grained control over the
utilization of their network resources. Throughout the pa-
per, we use â€œdestinationâ€ and â€œvictimâ€ interchangeably. Ex-
isting cloud-based systems have not provided such function-
ality. Many previously proposed capability-based systems
are likewise designed to work with a single scheduling pol-
icy. For instance, CRAFT [28] enforces per-ï¬‚ow fairness,
Portcullis [41] and Mirage [37] enforce per-compute fair-
ness, NetFence [34] enforces per-sender fairness, SIBRA [12]
enforces per-steady-bandwidth fairness, and SpeakUp [48]
enforces per-outbound-bandwidth fairness. If any of these
mechanisms is ever deployed, a single policy will be enforced,
forcing the victim to accept the choice made by the defense
approach. However, no single fairness regime can satisfy
all potential victimsâ€™ requirements.
Ideally, MiddlePolice
should be able to support victim-chosen policies. In addi-
tion to these fairness metrics, MiddlePolice can implement
ideas such as ARROWâ€™s [42] special pass for critical traï¬ƒc,
and prioritized services for premium clients.
Fixing the Bypass Vulnerability. Existing cloud-based
systems rely on DNS or BGP to redirect the destinationâ€™s
traï¬ƒc to their infrastructures. However, this model opens up
the attack of infrastructure bypass. For example, a majority
of cloud-protected web servers are subject to IP address ex-
posure [38,47]. Larger victims that SWIP their IP addresses
may be unable to keep their IP addresses secret from a de-
termined adversary. In such cases, the adversary can bypass
the cloud infrastructures by routing traï¬ƒc directly to the
victims. MiddlePolice includes a readily deployable mecha-
nism to address this vulnerability.

MiddlePolice is designed to augment the existing cloud-
based DDoS prevention systems with destination-selectable
policies. The literature is replete with capability-based sys-

tems that provide a single fairness guarantee with extensive
client modiï¬cation and deployment at non-aï¬ƒliated ASes.
The novelty and challenge of MiddlePolice is therefore ar-
chitecting a system to move deployment to the cloud while
enforcing a wide variety of destination-selectable fairness
metrics. Built atop a novel capability feedback mechanism,
MiddlePolice meets the challenge, thereby protecting against
DDoS more ï¬‚exibly and deployably.
2.2 Adversary Model and Assumptions
Adversary Model. We consider a strong adversary owning
large botnets that can launch strategic attacks and amplify
its attack [29]. We assume the adversary is not on-path be-
tween any mbox and the victim, since otherwise it could drop
all packets. Selecting routes without on-path adversaries is
an orthogonal problem and is the subject of active research
in next-generation Internet protocols (e.g., SCION [51]).
Well-connected mboxes. MiddlePolice is built on a dis-
tributed and replicable set of mboxes that are well-connected
to the Internet backbone. We assume the Internet backbone
has suï¬ƒcient capacity and path redundancy to absorb large
volumes of traï¬ƒc, and DDoS attacks against the set of all
mboxes can never be successful. This assumption is a stan-
dard assumption for cloud-based systems.
Victim Cooperation. MiddlePoliceâ€™s defense requires the
victimâ€™s cooperation. If the victim can hide its IP addresses
from attackers, it simply needs to remove a MiddlePolice-
generated capability carried in each packet and return it
back to the mboxes. The victim needs not to modify its
layer-7 applications as the capability feedback mechanism is
transparent to applications. If attackers can directly send
or point traï¬ƒc (e.g., reï¬‚ection) to the victim, the victim
needs to block the bypassing traï¬ƒc. MiddlePolice includes
a packet ï¬ltering mechanism that is immediately deployable
on commodity Internet routers.
Cross-traï¬ƒc Management. We assume that bottlenecks
on the path from an mbox to the victim that is shared
with other destinations are properly managed, such that
cross-traï¬ƒc targeted at another destination cannot cause
unbounded losses of the victimâ€™s traï¬ƒc. Generally, per-
destination-AS traï¬ƒc shaping (e.g., weighted fair share) on
these links will meet this requirement.
3. SYSTEM OVERVIEW

MiddlePoliceâ€™s high-level architecture is illustrated in Fig-
ure 1. A MiddlePolice-protected victim redirects its traï¬ƒc
to the mboxes. Each mbox polices traversing traï¬ƒc to en-
force the bandwidth allocation policy chosen by the victim.
The traï¬ƒc policing relies on a feedback loop of MiddlePolice-
generated capabilities to eliminate the deployment require-
ments on downstream paths. When the victim keeps its
IP addresses secret, a single deploying mbox can secure the
entire path from the mbox to the victim.

For victims whose IP addresses are exposed, attackers can
bypass the mboxes and direct attack traï¬ƒc to the victim.
MiddlePolice designs a packet ï¬ltering mechanism relying
on the ACL on commodity routers or switches to eliminate
the traï¬ƒc that does not traverse any mbox. As long as each
bottleneck link is protected by an upstream ï¬lter, the bypass
attack can be prevented.
4. DETAILED DESIGN OF mboxes

MiddlePoliceâ€™s traï¬ƒc policing algorithm (i) probes the

1269(and thus WR) is underestimated, the mbox can still fur-
ther deliver packets as long as the downstream path is not
congested.
Fairness Regimes. Each mbox allocates its bandwidth
estimate amongst its senders based on the sharing policies
chosen by the victim. For policies enforcing global fairness
among all senders, all mboxes sharing the same bottleneck
share their local observations.
4.1 Information Table

The basis of MiddlePoliceâ€™s traï¬ƒc policing is an informa-
tion table (iTable) maintained by each mbox. Each row of
the iTable corresponds to a single sender. The contents of
the iTable depend on the victim-selected sharing policy; this
section describes iTable elements needed for per-sender fair-
ness, and Â§4.3.5 extends the iTable to other fairness regimes.
In Â§6, we describe a mechanism to ï¬lter source spooï¬ng at
the mbox, so this section ignores source spooï¬ng.

f
64

TA Pid NR ND WR WV
128
32

16

32

32

32

LR
64

Table 1. Fields of an iTable entry and their sizes (bits).
Each sender si has one row in the iTable, identiï¬ed by
a unique identiï¬er f . The table contents are illustrated in
Table 1. Other than f , the remaining ï¬elds are updated in
each detection period. The timestamp TA records the cur-
rent detection period. The capability ID Pid is the maximum
number of distinct capabilities generated for si. NR stores
the number of packets received from si. ND indicates the
number of best-eï¬€ort packets dropped by the mbox. WR de-
termines the maximum number of privileged packets allowed
for si. The veriï¬cation window WV is designed to compute
siâ€™s packet loss rate, whereas LR stores the LLR for si.
4.2 Capability Computation

For si, the mbox generates two types of capabilities: dis-
tinct capabilities and common capabilities. The CHM can
use either capability to authenticate that the packet has tra-
versed the mbox, though only distinct capabilities are used
to infer downstream packet losses.

A distinct capability for si is computed as follows:

C = IPMP || ts || Pid || f || TA ||

MACKs (IPMP || ts || Pid || f || TA),

(1)
where IPMP is the IP address of the mbox issuing C and ts is
the current timestamp (included to mitigate replay attack).
The combination of Pid||f||TA ensures the uniqueness of C.
The MAC is computed based on a secret key Ks shared by
all mboxes. The MAC is 128 bits, so the entire C consumes
âˆ¼300 bits. A common capability is deï¬ned as follows

Cc = IPMP || ts || MACKs (IPMP || ts).

(2)

The design of capability incorporates a MAC to ensure
that attackers without secure keys cannot generate valid ca-
pabilities, preventing capability abuse.
4.3 Trafï¬c Policing Logic
4.3.1 Populating the iTable
We ï¬rst describe how to populate the iTable. At time
ts, the mbox receives the ï¬rst packet from si. It creates an
entry for si, with f computed based on siâ€™s source address,
and initializes the remaining ï¬elds to zero. It then updates

Figure 1. The architecture of MiddlePolice. The mboxes
police traï¬ƒc to enforce victim-chosen policies. The packet
ï¬ltering discards all traï¬ƒc bypassing the mboxes.

available downstream bandwidth from each mbox to the vic-
tim and (ii) allocates the bandwidth to senders according to
the policies chosen by the victim.
Bandwidth Probe. The fundamental challenge of estimat-
ing downstream bandwidth is that MiddlePolice requires no
deployment at downstream links. Such a challenge is two-
sided: an overestimate will cause downstream ï¬‚ooding, ren-
dering traï¬ƒc policing useless, while an underestimate will
waste downstream capacity, reducing performance.

To solve the overestimation problem, MiddlePolice relies
on a capability feedback mechanism to make senders self-
report how many packets they have successfully delivered
to the victim. Speciï¬cally, upon a packet arrival, the mbox
stamps an unforgeable capability in the packet. When the
packet is delivered to the victim, MiddlePoliceâ€™s capability
handling module (CHM) deployed on the victim returns the
carried capability back to the mbox. If the capability is not
returned to the mbox after a suï¬ƒciently long time interval
(compared with the RTT between the mbox and victim), the
mbox will consider the packet lost. Thus, the feedback en-
ables the mbox to infer a packet loss rate (hereinafter, LLR)
for each sender. Then the mbox estimates the downstream
capacity as the diï¬€erence between the number of packets re-
ceived from all senders and packets lost on the downstream
path. As the estimation is based on the traï¬ƒc volume deliv-
ered to the victim, this approach solves the overestimation
problem.

However, the above technique does not overcome the un-
derestimation problem. Speciï¬cally, since the traï¬ƒc de-
mand may be less than downstream capacity, simply using
the volume of delivered traï¬ƒc may cause underestimation.
To prevent underestimation, the mbox categorizes packets
from each sender as privileged packets and best-eï¬€ort pack-
ets. Speciï¬cally, the mbox maintains a rate window WR for
each sender to determine the amount of privileged packets
allowed for the sender in each period (hereinafter, detection
period ). WR is computed based on the above downstream
capacity estimation as well as victim-chosen policies. Pack-
ets sent beyond WR are classiï¬ed as best-eï¬€ort packets. The
mbox forwards all privileged packets to the victim, whereas
the forwarding decisions for best-eï¬€ort packets are subject
to a short-term packet loss rate (hereinafter, SLR). The SLR
reï¬‚ects downstream packet loss rates (congestion) at a RTT
granularity. That is, if the downstream is not congested
upon an arrival of a best-eï¬€ort packet, the mbox will for-
ward the packet. Thus, even when the downstream capacity

The Internet BackboneCloudCloudThe VictimISPISPThe mboxPacket Filtering1270Deï¬nition

Symb.
Dp
Thcap
Thrtt Maximum waiting time for cap. feedback
Thdrop
SLR thres. for dropping best-eï¬€ort pkts

The length of the detection period
The upper bound of capability ID

slr
Î²

Thlpass

Sslr

The weight of historical loss rates
The threshold for calculating LLR

The length limit of the cTable

Table 2. System parameters.

Value

4s
128
1s
0.05
0.8
5

100

TA to ts, increases both NR and Pid by one to reï¬‚ect the
packet arrival and computes a capability using the updated
Pid and TA.
Upon receiving a packet from si with arrival time taâˆ’TA >
Dp (Dp is the length of the detection period), the mbox starts
a new detection period for si by setting TA = ta. The mbox
also updates the remaining ï¬elds based on the traï¬ƒc policing
algorithm (as described in Â§4.3.4). The algorithm depends
on siâ€™s LLR and the mboxâ€™s SLR, the computation of which
is described in the following two sections.
Inferring the LLR for Source si

4.3.2
Capability Generation. For each packet from si, the
mbox generates a distinct capability for the packet if (i) its
arrival time ta âˆ’ TA < Dp âˆ’ Thrtt, and (ii) the capability
ID Pid < Thcap. The ï¬rst constraint ensures that the mbox
allows at least Thrtt for each capability to be returned from
the CHM. By setting Thrtt well above the RTT from the
mbox to the victim, any missing capabilities at the end of
the current detection period correspond to lost packets. Ta-
ble 2 lists the system parameters including Thrtt and their
suggested values. MiddlePoliceâ€™s performance with diï¬€erent
settings are studied in Â§8.3. The second constraint Thcap
bounds the number of distinct capabilities issued for si in
one detection period, and thus bounds the memory require-
ment. We set Thcap = 128 to reduce the LLR sampling error
while keeping memory overhead low.

Packets violating either of the two constraints, if any, will
carry a common capability (Equation (2)), which is not re-
turned by the CHM or used to learn siâ€™s LLR. However, it
can be used for packet authentication.
Capability Feedback Veriï¬cation. Let Kth denote the
number of distinct capabilities the mbox generates for si,
with capability ID ranging from [1, Kth]. Each time the mbox
receives a returned capability, it checks the capability ID to
determine which packet (carrying the received capability)
has been received by the CHM. WV represents a window
with Thcap bits. Initially all the bits are set to zero. When a
capability with capability ID i is received, the mbox sets the
ith bit in WV to one. At the end of the current detection
period, the zero bits in the ï¬rst Kth bits of WV indicate
the losses of the corresponding packets. To avoid feedback
reuse, feedback is processed only for capabilities issued in
the current period.
LLR Computation. LLR in the kth detection period is
computed at the end of the period, i.e., the time when the
mbox starts a new detection period for si. siâ€™s lost pack-
ets may contain downstream losses and best-eï¬€ort packets
dropped by the mbox (ND). The number of packets that si
sent to downstream links is NR âˆ’ ND, and the downstream
packet loss rate is V0Pid
, where V0 is the number of zero bits
in the ï¬rst Pid bits of WV . Thus, the estimated number

of downstream packet losses is N dstream
Then we have LLR = (N dstream

+ ND)/NR.

loss

loss

= (NR âˆ’ ND) V0Pid

.

Our strawman design is subject to statistical bias, and
may have negative eï¬€ects on TCP timeouts. In particular,
assume one legitimate TCP source recovers from a timeout
and sends one packet to probe the network condition.
If
the packet is dropped again, the source will enter a longer
timeout. However, with the strawman design, the source
would incorrectly have a 100% loss rate. Adding a low-pass
ï¬lter can ï¬x this problem: if siâ€™s NR in the current period
is less than a small threshold Thlpass, the mbox sets its LLR
in the current period to zero. Attackers may exploit this
design using on-oï¬€ attacks [30]. In Â§4.3.4, we explain how
to handle such attacks. Formally, we write LLR as follows

(cid:40)0,

if NR < Thlpass

, otherwise

(3)

+ND

LLR =

N dstream
NR
Inferring the SLR

loss

4.3.3
SLR is designed to reï¬‚ect downstream congestion on a per-
RTT basis, and is computed across all ï¬‚ows from the mbox to
the victim. Like LLR, SLR is learned through capabilities
returned by the CHM. Speciï¬cally, the mbox maintains a
hash table (cTable) to record capabilities used to learn its
SLR. The hash key is the capability itself and the value is a
single bit value (initialized to zero) to indicate whether the
corresponding key (capability) has been returned.
As described in Â§4.3.2, when a packet arrives (from any
source), the mbox stamps a capability on the packet. The
capability will be added into cTable if it is not a common ca-
pability and cTableâ€™s length has not reached the predeï¬ned
threshold Sslr. The mbox maintains a timestamp Tslr when
the last capability is added into cTable. Then, it uses the
entire batch of capabilities in cTable to learn the SLR. We
set Sslr = 100 to allow fast capability loading to the cTable,
while minimizing sampling error from Sslr being too small.
The mbox allows at most Thrtt from Tslr to receive feed-
back for all capabilities in cTable. Some capabilities may
be returned before Tslr (i.e., before cTable ï¬lls). Once a
capability in cTable is returned, the mbox marks it as re-
ceived. Upon receiving a new packet with arrival time ta >
, where Z0 is the
number of cTable entries that are not received. The mbox
then resets the current cTable to be empty to start a new
monitoring cycle for SLR.
4.3.4 Trafï¬c Policing Algorithm
We formalize the traï¬ƒc policing logic in Algorithm 1.
Upon receiving a packet P , the mbox retrieves the entry
F in iTable matching P (line 8). If no entry matches, the
mbox initializes an entry for P .
P is categorized as a privileged or best-eï¬€ort packet based
on Fâ€™s WR (line 10). All privileged packets are accepted,
whereas best-eï¬€ort packets are accepted conditionally. If P
is privileged, the mbox performs necessary capability han-
dling (line 11) before appending P to the privileged queue.
The mbox maintains two FIFO queues to serve all accepted
packets: the privileged queue serving privileged packets and
the best-eï¬€ort queue serving best-eï¬€ort packets. The privi-
leged queue has strictly higher priority than the best-eï¬€ort
queue at the output port. CapabilityHandling (line 16) exe-
cutes the capability generation and cTable updates (line 37),
as detailed in Â§4.3.2 and Â§4.3.3.

Tslr + Thrtt, the mbox computes SLR = Z0Sslr

1271Algorithm 1: Traï¬ƒc Policing Algorithm.
1 Input:
2
3 Output:
4
5

iTable updates and possible cTable updates;
The forwarding decision of P ;

Packet P arrived at time ts;

6 Main Procedure:
7 begin
8
9
10

F â† iTableEntryRetrieval(P );
F.NR â† F .NR + 1;
if F.NR < F.WR then

11
12

13

14

15

/* Privileged packets

CapabilityHandling(P , F);
Append P to the privileged queue;

else

/* Best-effort packets

BestEï¬€ortHandling(P , F);

/* Starting a new detection period if necessary

if ts âˆ’ F .TA > Dp then iTableHandling(F);

*/

*/

*/

16 Function: CapabilityHandling(P , F):
17 begin

18
19

20

21

22

23

/* Two constraints for distinct-capability generation */

if F.Pid < Thcap and tsâˆ’F.TA < Dpâˆ’Thrtt then
F.Pid â† F .Pid + 1;
Generate capability C based on Equation (1);
cTableHandling(C);

else

/* Common capability for packet authentication

Generate capability Cc based on Equation (2);

*/

24 Function: BestEï¬€ortHandling(P , F):
25 begin
26

if SLR < Thdrop

slr and F.LR < Thdrop
CapabilityHandling(P , F);
Append P to the best-eï¬€ort queue;
Drop P ; F.ND â† F .ND + 1;

slr

else

then

27
28

29
30

34

35
36

31 Function: iTableHandling(F):
32 begin
33

Compute recentLoss based on Equation (3);
/* Consider the historical loss rate
F.LR â† (1 âˆ’ Î²) Â· recentLoss + Î² Â· F .LR;
WR â† BandwidthAllocationPolicy(F);
Reset WV , Pid, NR and ND to zero;

37 Function: cTableHandling(C):
38 begin

39
40

41

/* One batch of cTable is not ready

if cTable.length < Sslr then
if cTable.length == Sslr then Tslr â† ts ;

Add C into cTable;

*/

*/

slr

If P is a best-eï¬€ort packet, its forwarding decision is sub-
ject to the mboxâ€™s SLR and F(cid:48)s LLR (line 24). If the SLR
exceeds Thdrop
, indicating downstream congestion, the mbox
discards P . Further, if Fâ€™s LLR is already above Thdrop
, the
mbox will not deliver best-eï¬€ort traï¬ƒc for F as well since
F already experiences severe losses. Thdrop
is set to be few
times larger than a TCP ï¬‚owâ€™s loss rate in normal network
condition [46] to absorb burst losses. If the mbox decides to
accept P , it performs capability handling (line 27).

slr

slr

Finally, if P â€™s arrival triggers a new detection period for
F (line 15), the mbox performs corresponding updates for
F (line 31). To determine Fâ€™s LLR, the mbox incorporates
both the recent LLR (recentLoss) obtained in the current
detection period based on Equation (3) and Fâ€™s historical
loss rate LR. This design prevents attackers from hiding
their previous packet losses via on-oï¬€ attacks. Fâ€™s WR is
updated based on the victim-selected policy (line 35), as
described below.
4.3.5 Bandwidth Allocation Policies
We list the following representative policies that may be
NaturalShare: for each sender, the mbox sets its WR for
the next period to the number of delivered packets from the
sender in the current period. The design rationale is that the
mbox allows a rate that the sender can sustainably transmit
without experiencing a large LLR.

chosen to implement in BandwidthAllocationPolicy.

PerSenderFairshare allows the victim to enforce per-sender
fair share at bottlenecks. Each mbox fairly allocates its es-
timated total downstream bandwidth to the senders that
reach the victim through the mbox. To this end, the mbox
maintains the total downstream bandwidth estimate N total
size ,
which it allocates equally among all senders.

To ensure global fairness among all senders, two mboxes
sharing the same bottleneck (i.e., the two paths connect-
ing the two mboxes with the victim both traverse the bot-
tleneck link) share their local observations. We design a
co-bottleneck detection mechanism using SLR correlation:
if two mboxesâ€™ observed SLRs are correlated, they share a
bottleneck with high probability. In Â§8.3, we evaluate the
eï¬€ectiveness of this mechanism.
PerASFairshare is similar to PerSenderFairshare except that
the mbox fairly allocates N total
size on a per-AS basis. This pol-
icy mimics SIBRA [12], preventing bot-infested ASes from
taking bandwidth away from legitimate ASes.
PerASPerSenderFairshare is a hierarchical fairness regime:
the mbox ï¬rst allocates N total
size on a per-AS basis, and then
fairly assigns the share obtained by each AS among the
senders of the AS.

PremiumClientSupport provides premium service to pre-
mium clients, such as bandwidth reservation for upgraded
ASes. The victim pre-identiï¬es its premium clients to Mid-
dlePolice. PremiumClientSupport can be implemented to-
gether with the aforementioned allocation policies.
5. PACKET FILTERING

When the victimâ€™s IP addresses are kept secret, attack-
ers cannot bypass MiddlePoliceâ€™s upstream mboxes to route
attack traï¬ƒc directly to the victim. In this case, the down-
stream packet ï¬ltering is unnecessary since MiddlePolice can
throttle attacks at the upstream mboxes. However, in case
of IP address exposure [38, 47], the victim needs to deploy
a packet ï¬lter to discard bypassing traï¬ƒc. MiddlePolice
designs a ï¬ltering mechanism that extends to commodity
routers the port-based ï¬ltering of previous work [20, 21].
Unlike prior work, the ï¬ltering can be deployed upstream
of the victim as a commercial service.
5.1 Filtering Primitives

Although the MAC-incorporated capability can prove that
a packet indeed traverses an mbox, it requires upgrades from
deployed commodity routers to perform MAC computation
to ï¬lter bypassing packets. Thus, we invent a mechanism

1272based on the existing ACL conï¬gurations on commodity
routers. Speciï¬cally, each mbox encapsulates its traversing
packets into UDP packets (similar techniques have been ap-
plied in VXLAN and [24]), and uses the UDP source and
destination ports (a total of 32 bits) to carry an authentica-
tor, which is a shared secret between the mbox and the ï¬lter-
ing point. As a result, a 500Gbps attack (the largest attack
viewed by Arbor Networks [40]) that uses random port num-
bers will be reduced to âˆ¼100bps since the chance of a correct
guess is 2âˆ’32. The shared secret can be negotiated period-
ically based on a cryptographically secure pseudo-random
number generator. We do not rely on UDP source address
for ï¬ltering to avoid source spooï¬ng.
5.2 Packet Filtering Points

Deployed ï¬ltering points should have suï¬ƒcient bandwidth
so that the bypassing attack traï¬ƒc cannot cause packet
losses prior to the ï¬ltering. The ï¬ltering mechanism should
be deployed at, or upstream of, each bottleneck link caused
by the DDoS attacks. For instance, for a victim with high-
bandwidth connectivity, if the bottleneck link is an internal
link inside the victimâ€™s network, the victim can deploy the
ï¬lter at the inbound points of its network.
If the bottle-
neck link is the link connecting the victim with its ISP, the
victim can can work with its ISP, on commercially reason-
able terms, to deploy the ï¬lter deeper in the ISPâ€™s network
such that the bypassing traï¬ƒc cannot reach the victimâ€™s net-
work. Working with the ISPs does not violate the deploy-
ment model in Â§2 as MiddlePolice never requires deployment
at unrelated ASes.
6. SOURCE VALIDATION

MiddlePolice punishes senders even after an oï¬€ending ï¬‚ow
ends. Such persistence can be built on source authentica-
tion or any mechanism that maintains sender accountability
across ï¬‚ows. As a proof-of-concept, we design a source val-
idation mechanism that is speciï¬c to HTTP/HTTPS traf-
ï¬c. The mechanism ensures that a sender is on-path to its
claimed source IP address. This source veriï¬er is completely
transparent to clients.

Our key insight is that the HTTP Host header is in the
ï¬rst few packets of each connection. As a result, the mbox
monitors a TCP connection and reads the Host header. If
the Host header reï¬‚ects a generic (not sender-speciï¬c) host-
name (e.g., victim.com), the mbox intercepts this ï¬‚ow, and
redirects the connection (HTTP 302) to a Host containing a
token cryptographically generated from the senderâ€™s claimed
source address, e.g., T .victim.com, where T is the token. If
the sender is on-path, it will receive the redirection, and its
further connection will use the sender-speciï¬c hostname in
the Host header. When an mbox receives a request with a
sender-speciï¬c Host, it veriï¬es that the Host is proper for the
claimed IP source address (if not, the mbox initiates a new
redirection), and forwards the request to the victim. Thus,
by performing source veriï¬cation entirely at the mbox, pack-
ets from spoofed sources cannot consume any downstream
bandwidth from the mbox to the victim.

If the cloud provider hosting the mbox is trusted (for in-
stance, large CDNs have CAs trusted by all major browsers),
the victim can share its key such that HTTPS traï¬ƒc can be
handled in the same way as HTTP traï¬ƒc. For untrusted
providers [21, 31], the mbox relays the encrypted connec-
tion to the victim, which performs Host-header-related op-
erations. The victim terminates unveriï¬ed senders without

Figure 2. The software stack of the mbox and CHM.

returning capabilities to the mbox, so the additional traï¬ƒc
from the unveriï¬ed senders is best-eï¬€ort under Algorithm 1.
In this case, packets from spoofed sources consume limited
downstream bandwidth but do not rely on the trustworthi-
ness of the cloud provider. We acknowledge that performing
source validation at the victim is subject to the DoC at-
tack [10] in which attackers ï¬‚ood the victim with new con-
nections to slow down the connection-setup for legitimate
clients. This attack is mitigated if the source validation is
completely handled by the mbox.
IMPLEMENTATION
7.

We have a full implementation of MiddlePolice.

7.1 The Implementation of CHM and mboxes
The mboxes and the CHM at the victim are implemented
based on the NetFilter Linux Kernel Module, which com-
bined have âˆ¼1500 lines of C code (excluding the capability
generation code). The software stack of our implementation
is illustrated in Figure 2.

All inbound traï¬ƒc from clients to an mbox is subject to the
traï¬ƒc policing whereas only accepted packets go through the
capability-related processing. Packet dropping due to traï¬ƒc
policing triggers iTable updates. For each accepted packet,
the mbox rewrites its destination address as the victimâ€™s ad-
dress to point the packet to the victim. To carry capabil-
ities, rather than deï¬ning a new packet header, the mbox
appends the capabilities to the end of the original data pay-
load, which avoids compatibility problems at intermediate
routers and switches. The CHM is responsible for trimming
these capabilities to deliver the original payload to the vic-
timâ€™s applications. If the packet ï¬lter is deployed, the mbox
performs the IP-in-UDP encapsulation, and uses the UDP
source and destination port number to carry an authenti-
cator. All checksums need to be recomputed after packet
manipulation to ensure correctness. ECN and encapsula-
tion interactions are addressed in [13].

To avoid packet fragmentation due to the additional 68
bytes added by the mbox (20 bytes for outer IP header, 8
bytes for the outer UDP header, and 40 bytes reserved for a
capability), the mbox needs to be a priori aware of the MTU
Md on its path to the victim. Then the mbox sets its MSS
to no more than Mdâˆ’68âˆ’40 (the MSS is 40 less than the
MTU). We do not directly set the MTU of the mboxâ€™s NIC
to rely on the path MTU discovery to ï¬nd the right packet
size because some ISPs may block ICMP packets. On our
testbed, Md = 1500, so we set the mboxâ€™s MSS to 1360.

Upon receiving packets from upstream mboxes, the CHM
strips their outer IP/UDP headers and trims the capabili-
ties. To return these capabilities, the CHM piggybacks ca-
pabilities to the payload of ACK packets. To ensure that

ClientsThe	VictimRewrite	daddras	the	victimâ€™s	addressAppend	capability	to	the	payloadPerform	IP-in-UDP	encapsulationStrip	the	outer	IP	headers	Trim	capability	feedbackRewrite	saddras	the	mboxâ€™saddressStrip	the	outer	IP	and	UDP	headers	Trim	capabilitiesatpacketfooterAppend	capabilities	in	ACK	payloadPerform	IP	tunneling	to	mboxesTraffic	PolicingUpdate	cTable&	iTableThe	InternetmboxCHMPacket	Filtering1273a capability is returned to the mbox issuing the capability
even if the Internet path is asymmetric, the CHM performs
IP-in-IP encapsulation to tunnel the ACK packets to the
right mbox. We allow one ACK packet to carry multiple
capabilities since the victim may generate cumulative ACKs
rather than per-packet ACKs. Further, the CHM tries to
pack more capabilities in one ACK packet to reduce the
capability feedback latency at the CHM. The number of ca-
pabilities carried in one ACK packet is stored in the TCP
option (the 4-bit res1 option). Thus, the CHM can append
up to 15 capabilities in one ACK packet if the packet has
enough space and the CHM has buï¬€ered enough capabilities.
Upon receiving an ACK packet from the CHM, the mbox
strips the outer IP header and trims the capability feedback
(if any) at the packet footer. Further, the mbox needs to
rewrite the ACK packetâ€™s source address back to its own
address, since the clientâ€™s TCP connection is expecting to
communicate with the mbox. Based on the received capa-
bility feedback, the mbox updates the iTable and cTable
accordingly to support the traï¬ƒc policing algorithm.
7.2 Capability Generation

We use the AES-128 based CBC-MAC, based on the In-
tel AES-NI library, to compute MACs, due to its fast speed
and availability in modern CPUs [6, 23]. We port the capa-
bility implementation (âˆ¼400 lines of C code) into the mbox
and CHM kernel module. The mbox needs to perform both
capability generation and veriï¬cation whereas the CHM per-
forms only veriï¬cation.
8. EVALUATION
8.1 The Internet Experiments

This section studies the path length and latency inï¬‚ation

for rerouting clientsâ€™ traï¬ƒc to mboxes hosted in the cloud.

8.1.1 Path Inï¬‚ation
We construct the AS level Internet topology based on the
CAIDA AS relationships dataset [4], including 52680 ASes
and their business relationships [16]. To construct the com-
munication route, two constraints are applied based on the
routes export policies in [19, 22]. First, an AS prefers cus-
tomer links over peer links and peer links over provider links.
Second, a path is valid only if each AS providing transit is
paid. Among all valid paths, an AS prefers the path with
least AS hops (a random tie breaker is applied if necessary).
As an example, we use Amazon EC2 as the cloud provider
to host the mboxes, and obtain its AS number based on the
report [5]. Amazon claims 11 ASes in the report. We ï¬rst
exclude the ASes not appearing in the global routing table,
and ï¬nd that AS 16509 is the provider for the remaining
Amazon ASes, so we use AS 16509 to represent Amazon.

We randomly pick 2000 ASes as victims, and for each vic-
tim we randomly pick 1500 access ASes. Among all victims,
1000 victims are stub ASes without direct customers and the
remaining victims are non-stub ASes. For each AS-victim
pair, we obtain the direct route from the access AS to the
victim, and the rerouted path through an mbox. Table 3
summarizes the route comparison. N hop
inï¬‚a is the average AS-
hop inï¬‚ation of the rerouted path compared with the direct
route. P short
is the percentage of access ASes that can reach
the victim with fewer hops after rerouting and P no
inï¬‚a is per-
centage of ASes without hop inï¬‚ation.

cut

Victims

Non-stub ASes

Stub ASes

Overall

N hop
inï¬‚a
1.1
1.5
1.3

cut

P short
P no
inï¬‚a
10.6% 22.2%
8.4% 18.0%
9.5% 20.1%

Table 3. Rerouting traï¬ƒc to mboxes causes small AS-hop
inï¬‚ation, and âˆ¼10% access ASes can even reach the victim
with fewer hops through mboxes.

Figure 3.
paths under various Internet conditions.

[Internet] FCTs for direct paths and rerouted

Overall, it takes an access AS 1.3 more AS-hops to reach
the victim after rerouting. Even for stub victims, which are
closer the Internet edge, the average hop inï¬‚ation is only
1.5. We also notice that âˆ¼10% ASes have shorter paths due
to the rerouting.

Besides EC2, we also perform path inï¬‚ation analysis when
mboxes are hosted by CloudFlare. The results show that
the average path inï¬‚ation is about 2.3 AS-hops. For any
cloud provider, MiddlePolice has the same path inï¬‚ation as
the cloud-based DDoS solutions hosted by the same cloud
provider, since capability feedback is carried in ACK pack-
ets. As such, deploying MiddlePolice into existing cloud-
based systems does not increase path inï¬‚ation.
8.1.2 Latency Inï¬‚ation
In this section, we study the latency inï¬‚ation caused by
the rerouting. In our prototype running on the Internet, we
deploy 3 mboxes on Amazon EC2 (located in North Amer-
ica, Asia and Europe), one victim server in a US university
and about one hundred senders (located in North America,
Asia and Europe) on PlanetLab [1] nodes. We also deploy
few clients on personal computers to test MiddlePolice in
home network. The wide distribution of clients allows us to
evaluate MiddlePolice on various Internet links. We did not
launch DDoS attacks over the Internet, which raises ethical
and legal concerns.
Instead, we evaluate how MiddlePo-
lice may aï¬€ect the clients in the normal Internet without
attacks, and perform the experiments involving large scale
DDoS attacks on our private testbed and in simulation.

In the experiment, each client posts a 100KB ï¬le to the
server, and its traï¬ƒc is rerouted to the nearest mbox before
reaching the server. We repeat the posting on each client
10,000 times to reduce sampling error. We also run the
experiment during both peak hours and midnight (based on
the serverâ€™s timezone) to test various network conditions. As
a control, clients post the ï¬les to server via direct paths.
Figure 3 shows the CDF of the ï¬‚ow completion times
(FCTs) for the ï¬le posting. Overall, we notice âˆ¼9% av-
erage FCT inï¬‚ation, and less than 5% latency inï¬‚ation in
home network. Therefore, traï¬ƒc rerouting introduces small
extra latency to the clients.

MiddlePoliceâ€™s latency inï¬‚ation includes both rerouting-
induced networking latency and capability-induced compu-

 0 20 40 60 80 100 0 500 1000 1500 2000 2500CDF (%)Flow Completion Time (FCT) (ms)Home (direct)Home (reroute)Institute (direct)Institute (reroute)1274[Testbed] Throughput and goodput when Mid-

Figure 4.
dlePolice policies diï¬€erent numbers of senders.
tational overhead. As evaluated in Â§8.2.1, the per-packet
processing latency overhead caused by capability computa-
tion is âˆ¼1.4 Âµs, which is negligible compared with typical
Internet RTTs. Thus, MiddlePolice has latency almost iden-
tical to the existing cloud-based DDoS mitigation.
8.2 Testbed Experiments
8.2.1 Trafï¬c Policing Overhead
In this section, we evaluate the traï¬ƒc policing overhead
on our testbed. We organize three servers as one sender, one
mbox and one receiver. All servers, shipped with a quad-core
Intel 2.8GHz CPU, run the 3.13.0 Linux kernel. The mbox
is installed with multiple Gigabit NICs to connect both the
sender and receiver. A long TCP ï¬‚ow is established be-
tween the sender and receiver, via the mbox, to measure
the throughput. To emulate a large number of sources, the
mbox creates an iTable with N entries. Each packet from the
sender triggers a table look up for a random entry. We im-
plement a two-level hash table in the kernel space to reduce
the look up latency. Then the mbox generates a capability
based on the obtained entry.

Figure 4 shows the measured throughput and goodput un-
der various N . The goodput is computed by subtracting the
additional header and capability size from the total packet
size. The baseline throughput is obtained without Middle-
Police. Overall, the policing overhead in high speed network
is small. When a single mbox deals with 100,000 sources
sending simultaneously, throughput drops by âˆ¼10%. Equiv-
alently, MiddlePolice adds around 1.4 microseconds latency
to each packet processed. By replicating mboxes, the vic-
tim can distribute the workload across many mboxes when
facing large scale attacks.
8.2.2 Enforce Destination-Deï¬ned Policies
We now evaluate MiddlePoliceâ€™s performance for enforc-
ing victim-deï¬ned policies, along with the eï¬€ectiveness of
ï¬ltering bypassing traï¬ƒc. This section evaluates pure Mid-
dlePolice.
In reality, once built into cloud-based systems,
MiddlePolice needs only to process traï¬ƒc that passes their
pre-deployed defense.
Testbed Topology. Figure 5 illustrates the network topol-
ogy, including a single-homed victim AS purchasing 1Gbps
bandwidth from its ISP, an mbox and 10 access ASes. The
ISP is emulated by a Pronto-3297 48-port Gigabit switch to
support packet ï¬ltering. The mbox is deployed on a server
with multiple Gigabit NICs, and each access AS is deployed
on a server with a single NIC. We add 100ms latency at the
victim via Linux traï¬ƒc control to emulate the typical Inter-
net RTT. To emulate large scale attacks, 9 ASes are com-
promised. Attackers adopt a hybrid attack proï¬le: 6 attack

Figure 5. Testbed network topology.

Figure 6. [Testbed] Packet ï¬ltering via ACL.

ASes directly send large volumes of traï¬ƒc to the victim,
emulating ampliï¬cation-based attacks, and the remaining
attack ASes route traï¬ƒc through the mbox. Thus, the total
volume of attack traï¬ƒc is 9 times as much as the victimâ€™s
bottleneck link capacity. Both the inbound and outbound
points of the mbox are provisioned with 4Gbps bandwidth
to ensure the mbox is not the bottleneck, emulating that the
mbox is hosted in the cloud.
Packet Filtering. We ï¬rst show the eï¬€ectiveness of the
packet ï¬lter. Six attack ASes spoof the mboxâ€™s source ad-
dress and send 6Gbps UDP traï¬ƒc to the victim. The attack
ASes scan all possible UDP port numbers to guess the shared
secret. Figure 6 shows the volume of attack traï¬ƒc bypass-
ing the mbox and its volume received by the victim. As the
chance of a correct guess is very small, the ï¬lter can eï¬€ec-
tively stop the bypassing traï¬ƒc from reaching the victim.
Further, even if the shared secret were stolen by attackers
at time ts, the CHM would suddenly receive large numbers
of packets without valid capabilities. Since packets travers-
ing the mbox carry capabilities, the CHM realizes that the
upstream ï¬ltering has been compromised. The victim then
re-conï¬gures the ACL using a new secret to recover from key
compromise. The ACL is eï¬€ective within few milliseconds
after reconï¬guration. Thus, the packet ï¬ltering mechanism
can promptly react to a compromised secret.
NaturalShare and PerASFairshare Policies.
In this sec-
tion, we ï¬rst show that the mbox can enforce the Natural-
Share and PerASFairshare policies. We use the default pa-
rameter setting in Table 2, and defer detailed parameter
study in Â§8.3. Since MiddlePolice conditionally allows an AS
to send faster than its WR, we use the window size, deï¬ned
as the larger value between an ASâ€™s WR and its delivered
packets to the victim, as the performance metric. For clear
presentation, we normalize the window size to the maximum
number of 1.5KB packets deliverable through a 1Gbps link
in one detection period. We do not translate window sizes
to throughput because packet sizes vary.

Attackers adopt two representative strategies:

(i) they
send ï¬‚at rates regardless of packet losses, and (ii) they dy-
namically adjust their rates based on packet losses (reac-
tive attacks). To launch ï¬‚at-rate attacks, the attackers keep
sending UDP traï¬ƒc to the victim. The CHM uses a dedi-

 0 200 400 600 800 10 20 30 40 50 60 70 80 90 100Bandwidth (Mbps)The number of senders (K)GoodputThroughputBaselinemboxISPVictimLegitimate ASAttackersâ€¦Bottleneck	01234567Traffic	Volume	(Gbps)Attack	Traffic	bypassing	the	mboxBypassingtraffic	received	by	the	victimKey	stolenğ’•ğ’”ACLreconfig1275(a) Window sizes in ï¬‚at-attacks.

(b) LLRs in ï¬‚at-attacks.

(c) Window sizes in reactive att. (d) LLRs in reactive attacks.

Figure 7. [Testbed] Enforcing the NaturalShare policy. The legitimate AS gradually obtains a certain amount of bandwidth
under ï¬‚at-rate attacks since attackersâ€™ window sizes drop consistently over time (Figure 7(a)) due to their high LLRs (Figure
7(b)). However, the attack ASes can consume over 95% of the bottleneck bandwidth via reactive attacks (Figure 7(c)) while
maintaining low LLRs similar to the legitimate ASâ€™s LLR (Figure 7(d)).

(a) Window sizes in ï¬‚at-attacks.

(b) LLRs in ï¬‚at-attacks.

(c) Window sizes in reactive att. (d) LLRs in reactive attacks.

[Testbed] Enforcing the PerASFairshare policy. The legitimate AS can obtain at least the per-AS fair rate at
Figure 8.
the bottleneck regardless of the attack strategies (Figures 8(a) and 8(c)). Further, the legitimate AS gains slightly more
bandwidth than the attackers under ï¬‚at-rate attacks as the attack ASes have large LLRs (Figure 8(b)).

cated ï¬‚ow to return received capabilities to the mbox since
no ACK packets are generated for UDP traï¬ƒc. One way
of launching reactive attacks is that the attackers simul-
taneously maintain many more TCP ï¬‚ows than the legiti-
mate AS. Such a many-to-one communication pattern allows
the attackers to occupy almost the entire bottleneck, even
through each single ï¬‚ow seems completely â€œlegitimateâ€.

The legitimate AS always communicates with the victim

via a long-lived TCP connection.

Figure 7 shows the results for the NaturalShare policy. As
the bottleneck is ï¬‚ooded by attack traï¬ƒc, the legitimate AS
is forced to enter timeout at the beginning, as illustrated in
Figure 7(a). The attackersâ€™ window sizes are decreasing over
time, which can be explained via Figure 7(b). As the volume
of attack traï¬ƒc is well above the bottleneckâ€™s capacity, all
attack ASesâ€™ LLRs are well above Thdrop
. Thus, the mbox
drops all their best-eï¬€ort packets. As a result, when one
attack ASâ€™s window size is W (t) in detection period t, then
W (t+1) â‰¤ W (t) since in period t+1 any packet sent beyond
W (t) is dropped. Further, any new packet losses from the
attack AS, caused by an overï¬‚ow at the bottleneck buï¬€er,
will further reduce W (t + 1). Therefore, all attack ASesâ€™
window sizes are consistently decreasing over time, creating
spare bandwidth at the bottleneck for the legitimate AS. As
showed in Figure 7(a), the legitimate AS gradually recovers
from timeouts.

slr

The NaturalShare policy, however, cannot well protect the
legitimate AS if the attackers adopt the reactive attack strat-
egy. By adjusting the sending rates based on packet losses,
the attack ASes can keep their LLRs low enough to regain
the advantage of delivering best-eï¬€ort packets. Meanwhile,

Figure 9. [Testbed] MiddlePolice ensures that the premium
client (AS A) receives consistent bandwidth.

they can gain much more bandwidth by initiating more TCP
ï¬‚ows. Figure 7(c) shows the window sizes when each at-
tack AS starts 200 TCP ï¬‚ows whereas the legitimate AS
has only one. The attackers consume over 95% of the bot-
tleneck bandwidth, while keeping low LLRs similar to that
of the legitimate AS (Figure 7(d)).

Figure 8 shows the results for the PerASFairshare policy.
Figures 8(a) and 8(c) demonstrate that the legitimate AS
receives at least per-AS fair rate at the bottleneck regard-
less of the attack strategies, overcoming the shortcomings
of the NaturalShare policy. Further, under ï¬‚at-rate attacks,
the legitimate AS has slightly larger window sizes than the
attackers since, again, the mbox does not accept any best-
eï¬€ort packets from the attackers due to their high LLRs (as
showed in Figure 8(b)).
PremiumClientSupport Policy. This section evaluates the
PremiumClientSupport policy. We consider a legitimate AS
(AS A) that is a premium client which reserves half of the
bottleneck bandwidth. Figure 9 plots AS Aâ€™s bandwidth

 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0.4 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 20 40 60 80 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 0.1 0.2 0.3 0 20 40 60 80 100Normalized window sizeTime (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS 0 1 2 3 4 5 0 20 40 60 80 100LLR (%)Time (s)The legitimate ASThe 1st attack ASThe 2nd attack ASThe 3rd attack AS	010020030040050060070011020100100020003000AS	A's	Bandwidth	(Mbps)The	number	of	senders	from	the	attack	ASesPremium	SupportNo	Protection1276(a) NaturalShare.

(b) PerSenderFairshare.

(c) Jainâ€™s fairness index (FI).

(d) FI for various mbox counts.

Figure 10. [Simulation] Evaluating NaturalShare & PerSenderFairshare in large scale. Figures 10(a) and 10(b) show that the
clientsâ€™ average window size is larger than that of the attackers under both ï¬‚at-rate and shrew attacks. Figure 10(c) proves
that the clientsâ€™ window sizes converge to fairness in the PerSenderFairshare policy. Figure 10(d) shows that MiddlePolice can
enforce strong fairness among all senders even without coordination among the mboxes.

when the number of senders from the attack ASes increases.
With the PremiumClientSupport policy, MiddlePolice ensures
AS A receives consistent bandwidth regardless of the number
of senders from the attack ASes. However, without such a
policy, the attack ASes can selï¬shly take away the majority
of bottleneck bandwidth by involving more senders.
8.3 Large Scale Evaluation

In this section, we further evaluate MiddlePolice via large
scale simulations on ns-3 [3]. We desire to emulate real-world
DDoS attacks in which up to millions of bots ï¬‚ood a victim.
To circumvent the scalability problem of ns-3 at such a scale,
we adopt the same approach in NetFence [34], i.e., by ï¬x-
ing the number of nodes (âˆ¼5000) and scaling down the link
capacity proportionally, we can simulate attack scenarios
where 1 million to 10 million attackers ï¬‚ood a 40Gbps link.
The simulation topology is similar to the testbed topology,
except that all attackers are connected to the mbox.

Besides the ï¬‚at-rate attacks and reactive attacks, we also
consider the on-oï¬€ shrew attacks [30] in the simulations.
Both the on-period and oï¬€-period in shrew attacks are 1s.
The number of attackers is 10 times larger than that of le-
gitimate clients. In ï¬‚at-rate attacks and shrew attacks, the
attack traï¬ƒc volume is 3 times larger than the capacity of
the bottleneck. In reactive attacks, each attacker opens 10
connections, whereas a client has one. The bottleneck router
buï¬€er size is determined based on [9], and the RTT is 100ms.
NaturalShare & PerSenderFairshare in Scale. Figure 10
shows the results for enforcing NaturalShare and PerSender-
Fairshare policies with default parameter settings. We plot
the ratio of clientsâ€™ average window size to attackersâ€™ aver-
age window size for the NaturalShare policy in Figure 10(a).
For ï¬‚at-rate attacks and shrew attacks, it may be surprising
that the clientsâ€™ average window size is larger than that of
the attackers. Detailed trace analysis shows that it is be-
cause that the window sizes of a large portion of attackers
keep decreasing, as we explained in our testbed experiment.
As the number of attackers is much larger than the client
count, the attackersâ€™ average window size turns out to be
smaller than that of the clients, although the absolute vol-
ume of attack traï¬ƒc may be still higher. Under reactive
attacks, the clientsâ€™ average window size (almost zero) is
too small to be plotted in Figure 10(a).

Figure 10(b) shows that the clients enjoy even larger win-
dow ratio gains under the PerSenderFairshare policy in ï¬‚at-
rate and shrew attacks because even more attackers enter
the window dropping mode. Further, the PerSenderFairshare

Figure 11.
reï¬‚ects whether two mboxes share a bottleneck.

[Simulation] The SLR correlation coeï¬ƒcient

ensures that the clientsâ€™ average window size is close to the
per-sender fair rate in reactive attacks. Figure 10(c) demon-
strates that each clientâ€™s window size converges to per-client
fairness as Jainâ€™s fairness index [15] (FI) is close to 1.
mbox Coordination. To enforce global per-sender fairness,
the mboxes sharing the same bottleneck link share their lo-
cal observations (Â§4.3.5). We ï¬rst investigate how bad the
FI can be without such inter-mbox coordination. We recon-
struct the topology to create multiple mboxes, and map each
client to a random mbox. The attackers launch reactive at-
tacks. The results, plotted in Figure 10(d), show that the
FI drops slightly, by âˆ¼8%, even if 20 mboxes make local rate
allocations without any coordination among them.

To complete our design, we further propose the following
co-bottleneck detection mechanism. The design rationale
is that if two mboxesâ€™ SLR observations are correlated, they
share a bottleneck with high probability. To validate this, we
rebuild the network topology to create the scenarios where
two mboxes share and do not share a bottleneck, and study
the correlation coeï¬ƒcient of their SLRs. We compute one co-
eï¬ƒcient for every 100 SLR measurements from each mbox.
Figure 11 shows the CDF of the coeï¬ƒcient. Clearly, the
coeï¬ƒcient reï¬‚ects whether the two mboxes share a bottle-
neck. Thus, by continuously observing such correlation be-
tween two mboxesâ€™ SLRs, MiddlePolice can determine with
increasing certainty whether or not they share a bottleneck,
and can conï¬gure their coordination accordingly.
Parameter Study. We evaluate MiddlePolice using dif-
ferent parameters than the default values in Table 2. We
mainly focus on Dp, Thdrop
and Î². For each parameter, we
vary its value to obtain the clientsâ€™ average window size un-
der the 10-million bot attack. The results showed in Table
5 are normalized to the window sizes obtained using the
default parameters in Table 2.
Under the NaturalShare policy, the shorter Dp produces a

slr

 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksWindow gain over flatWindow gain over shrew 0 1 2 3 4 5 2 4 6 8 10Average window ratio (log2)The number of attackers (million)Flat-rate attacksShrew attacksReactive attacksWindow gain over flatWindow gain over shrew 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jainâ€™s fairness indexThe number of attackers (million)Flat-rate attacksShrew attacksReactive attacks 0 0.2 0.4 0.6 0.8 1 2 4 6 8 10Jainâ€™s fairness indexThe number of attackers (million)Single mbox10 mboxes20 mboxes 0 20 40 60 80 100-0.4-0.2 0 0.2 0.4 0.6 0.8 1CDF (%)SLR correlation coefficientCo-bottleneckDiff. bottlenecks1277Pushback [36]

TVA [50]

Netfence [34]

Phalanx [17] Mirage [37]

SIBRA [12]

MiddlePolice

Source upgrades
Dest. upgrades
AS deployment

No
No

Unrelated

Router support

O(N ) states

Fairness regimes

Other

requirements

None

None

Yes
Yes

Unrelated

O(N ) states;
Cryptography

None

New header

Yes
Yes

Yes
Yes

Unrelated

Unrelated

O(N ) states

O(N ) states;
Cryptography

Per-sender
New header;
Passport [32]

Yes
Yes

Related

Larger
memory

Puzzle;

Yes
Yes

No
Yes

Unrelated

Related

None

Per-AS
Redesign

None

Victim-chosen

None

None

Per-compute

New header

IPv6 upgrade

the Internet

Table 4. Property comparison with other research proposals. â€œO(N ) statesâ€ means that the number of states maintained by a
router increases with the number of attackers. â€œCryptographyâ€ means that a router needs to support cryptography operation,
e.g., MAC computation. â€œPuzzleâ€ means that the mechanism requires computational puzzle distribution.

(a) The NaturalShare Policy

Dp

Thdrop

slr

2s
Flat
1.1
Shrew 1.3

8s
0.17
0.65

0.03
0.78
0.77

0.1
0.39
1.0

Î²

0.5
1.1
1.2

0.9
0.78
0.80

(b) The PerSenderFairshare Policy

Dp

Thdrop

slr

Flat
Shrew

Reactive

2s
1.0
1.1
1.0

8s
1.1
0.98
0.99

0.03
1.0
0.72
1.0

0.1
0.69
0.83
0.94

Î²

0.5
0.85
1.0
1.0

0.9
0.81
0.98
1.0

Table 5.
diï¬€erent parameter settings.

[Simulation] Clientsâ€™ average window size under

larger window size for legitimate clients since each senderâ€™s
WR is updated per-period so that a smaller Dp causes faster
cut in attackersâ€™ window sizes. For Thdrop
, a smaller value
slows down the clientsâ€™ recovery whereas a larger value al-
lows larger window sizes for attackers. Both will reduce the
clientsâ€™ share. A larger Î² has negative eï¬€ects as it takes
more time for the clients to recover to a low LLR.

slr

With the PerSenderFairshare policy, MiddlePoliceâ€™s perfor-
mance is more consistent under diï¬€erent parameter settings.
The most sensitive parameter is Thdrop
slr because it determines
whether one source can send best-eï¬€ort traï¬ƒc.
9. RELATED WORK

In this section, we brieï¬‚y discuss previous academic work.
Previous research approaches can be generally categorized
into capability-based approaches (SIFF [49], TVA [50], Net-
Fence [34]), ï¬ltering-based approaches (Traceback [43, 45],
AITF [11], Pushback [25, 36], StopIt [33]), overlay-based
approaches (Phalanx [17], SOS [27]), deployment-friendly
approaches (Mirage [37], CRAFT [28]), approaches based
on new Internet architectures (SCION [51], SIBRA [12],
XIA [39], AIP [7]), and others (SpeakUp [48], SDN-based [18,
44], CDN-based [21]). We summarize the properties of one
or two approaches from each category in Table 4. The com-
parison shows that MiddlePolice requires the least deploy-
ment (no source upgrades, no additional router support and
no deployment from unrelated ASes) while providing the
strongest property (enforcing destination-chosen policies).
10. DISCUSSION

We brieï¬‚y cover some aspects not previously discussed.

mboxes Mapping. MiddlePolice can leverage the end-user
mapping [14] to achieve better mbox assignment, such as

redirecting clients to the nearest mbox, mapping clients ac-
cording to their ASes, and load balancing.
Incorporating Endhost Defense. MiddlePolice can co-
operate with the DDoS defense mechanism deployed, if any,
on the victim. For instance, via botnet identiï¬cation [26,35],
the victim can instruct the mboxes to block botnet traï¬ƒc
early at upstream so as to save more downstream bandwidth
for clients. Such beneï¬ts are possible because the policies
enforced by MiddlePolice are completely destination-driven.
Additional Monetary Cost. As discussed in 8.2.1, Mid-
dlePolice introduces small computational overhead. Com-
pared with basic DDoS-as-a-service solutions, MiddlePolice
oï¬€ers additional functionalities such as enabling destination-
chosen policies and ï¬ltering bypassing traï¬ƒc. In a competi-
tive marketplace, serviceâ€™s price (the monetary cost) should
scale with the cost of providing that service, which, in the
case of MiddlePolice, is low.
11. CONCLUSION

This paper presents MiddlePolice, a DDoS defense system
that is as deployable as cloud-based systems and has the
same destination-based control of capability-based systems.
In its design, MiddlePolice explicitly addresses three chal-
lenges. First, MiddlePolice designs a capability mechanism
that requires only limited deployment from the cloud, rather
than widespread Internet upgrades. Second, MiddlePolice is
fully destination-driven, addressing the shortcomings of the
existing capability-based systems that can work only with a
single fairness regime. Finally, MiddlePolice addresses the
traï¬ƒc-bypass vulnerability of the existing cloud-based solu-
tions. Extensive evaluations on the Internet, testbed and
large scale simulations validate MiddlePoliceâ€™s deployability
and eï¬€ectiveness in enforcing detestation-chosen policies.
12. ACKNOWLEDGMENTS

We thank the anonymous CCS reviewers for their valu-
able feedback. This material is based upon work partially
supported by NSF under Contract Nos. CNS-0953600, CNS-
1505790 and CNS-1518741. The views and conclusions con-
tained here are those of the authors and should not be in-
terpreted as necessarily representing the oï¬ƒcial policies or
endorsements, either express or implied, of NSF, the Univer-
sity of Illinois, or the U.S. Government or any of its agencies.
13. REFERENCES
[1] PlanetLab. https://www.planet-lab.org/. WebCite archive.
[2] AT&T Denial of Service Protection.

http://soc.att.com/1IIlUec, Accessed in 2016. WebCite
archive.

[3] NS-3: a Discrete-Event Network Simulator.
http://www.nsnam.org/, Accessed in 2016.

1278[4] AS Relationships â€“ CIDR Report.

http://www.caida.org/data/as-relationships/, Accessed in
Dec 2015. WebCite archive.
[5] AS Names - CIDR Report.

http://www.cidr-report.org/as2.0/autnums.html, Dec 2015.

[6] Akdemir, K., Dixon, M., Feghali, W., Fay, P., Gopal,
V., Guilford, J., Ozturk, E., Wolrich, G., and Zohar,
R. Breakthrough AES Performance with Intel R(cid:13) AES New
Instructions. Intel white paper (2010).

[7] Andersen, D. G., Balakrishnan, H., Feamster, N.,

Koponen, T., Moon, D., and Shenker, S. Accountable
Internet Protocol (AIP). In ACM SIGCOMM (2008).

[8] Anderson, T., Roscoe, T., and Wetherall, D.

Preventing Internet Denial-of-Service with Capabilities.
ACM SIGCOMM (2004).

[29] KÂ¨uhrer, M., Hupperich, T., Rossow, C., and Holz, T.

Exit from Hell? Reducing the Impact of Ampliï¬cation
DDoS Attacks. In USENIX Security Symposium (2014).

[30] Kuzmanovic, A., and Knightly, E. W. Low-Rate

TCP-Targeted Denial of Service Attacks: the Shrew vs. the
Mice and Elephants. In ACM SIGCOMM (2003).

[31] Liang, J., Jiang, J., Duan, H., Li, K., Wan, T., and Wu,
J. When HTTPS Meets CDN: A Case of Authentication in
Delegated Service. In IEEE S&P (2014).

[32] Liu, X., Li, A., Yang, X., and Wetherall, D. Passport:
Secure and Adoptable Source Authentication. In USENIX
NSDI (2008).

[33] Liu, X., Yang, X., and Lu, Y. To Filter or to Authorize:

Network-Layer DoS Defense Against Multimillion-node
Botnets. In ACM SIGCOMM (2008).

[9] Appenzeller, G., Keslassy, I., and McKeown, N. Sizing

[34] Liu, X., Yang, X., and Xia, Y. NetFence: Preventing

Router Buï¬€ers. ACM SIGCOMM, 2004.

[10] Argyraki, K., and Cheriton, D. Network Capabilities:

The good, the Bad and the Ugly. ACM HotNets-IV (2005).

[11] Argyraki, K. J., and Cheriton, D. R. Active Internet

Traï¬ƒc Filtering: Real-Time Response to Denial-of-Service
Attacks. In USENIX ATC (2005).

[12] Basescu, C., Reischuk, R. M., Szalachowski, P.,

Perrig, A., Zhang, Y., Hsiao, H.-C., Kubota, A., and
Urakawa, J. SIBRA: Scalable Internet Bandwidth
Reservation Architecture. NDSS (2016).

[13] Briscoe, B. Tunnelling of Explicit Congestion Notiï¬cation.

RFC 6040, 2010.

[14] Chen, F., Sitaraman, R. K., and Torres, M. End-User
Mapping: Next Generation Request Routing for Content
Delivery. In ACM SIGCOMM (2015).

[15] Chiu, D.-M., and Jain, R. Analysis of the Increase and

Decrease Algorithms for Congestion Avoidance in
Computer Networks. Computer Networks and ISDN
systems (1989).

[16] Dimitropoulos, X., Krioukov, D., Fomenkov, M.,

Huffaker, B., Hyun, Y., Riley, G., et al. AS
Relationships: Inference and Validation. ACM SIGCOMM
CCR (2007).

[17] Dixon, C., Anderson, T. E., and Krishnamurthy, A.

Phalanx: Withstanding Multimillion-Node Botnets. In
NSDI (2008).

[18] Fayaz, S. K., Tobioka, Y., Sekar, V., and Bailey, M.
Bohatei: Flexible and Elastic DDoS Defense. In USENIX
Security Symposium (2015).

[19] Gao, L., Griffin, T. G., and Rexford, J. Inherently Safe

Backup Routing with BGP. In IEEE INFOCOM (2001).
[20] Gilad, Y., and Herzberg, A. LOT: a Defense against IP

Spooï¬ng and Flooding Attacks. ACM Transactions on
Information and System Security (TISSEC) (2012).

[21] Gilad, Y., Herzberg, A., Sudkovitch, M., and

Goberman, M. CDN-on-Demand: An Aï¬€ordable DDoS
Defense via Untrusted Clouds. NDSS (2016).

[22] Goldberg, S., Schapira, M., Hummon, P., and

Rexford, J. How Secure are Secure Interdomain Routing
Protocols. ACM SIGCOMM CCR (2011).

[23] Gueron, S. Intel Advanced Encryption Standard (AES)

Instructions Set. White Paper, Intel (2010).

[24] Herbert, T. UDP Encapsulation in Linux. In The
Technical Conference on Linux Networking (2015).
[25] Ioannidis, J., and Bellovin, S. M. Implementing

Pushback: Router-Based Defense Against DDoS Attacks.
USENIX NSDI (2002).

[26] Karasaridis, A., Rexroad, B., and Hoeflin, D.

Wide-scale Botnet Detection and Characterization. In
USENIX HotBots (2007).

[27] Keromytis, A. D., Misra, V., and Rubenstein, D. SOS:

Secure Overlay Services. ACM SIGCOMM (2002).

[28] Kim, D., Chiang, J. T., Hu, Y.-C., Perrig, A., and

Kumar, P. CRAFT: A New Secure Congestion Control
Architecture. In ACM CCS (2010).

Internet Denial of Service from Inside Out. ACM
SIGCOMM (2011).

[35] Livadas, C., Walsh, R., Lapsley, D., and Strayer,
W. T. Using Machine Learning Technliques to Identify
Botnet Traï¬ƒc. In IEEE LCN (2006).

[36] Mahajan, R., Bellovin, S. M., Floyd, S., Ioannidis, J.,
Paxson, V., and Shenker, S. Controlling High Bandwidth
Aggregates in the Network. ACM SIGCOMM (2002).

[37] Mittal, P., Kim, D., Hu, Y.-C., and Caesar, M. Mirage:

Towards Deployable DDoS Defense for Web Applications.
arXiv preprint arXiv:1110.1060 (2011).

[38] Miu, T. T., Hui, A. K., Lee, W., Luo, D. X., Chung,

A. K., and Wong, J. W. Universal DDoS Mitigation
Bypass. Black Hat USA (2013).

[39] Naylor, D., et al. XIA: Architecting a More Trustworthy

and Evolvable Internet. ACM SIGCOMM (2014).

[40] Networks, A. Worldwide Infrastructure Security Report,

Volume IX. https://www.arbornetworks.com/images/
documents/WISR2016 EN Web.pdf, 2016.

[41] Parno, B., Wendlandt, D., Shi, E., Perrig, A., Maggs,
B., and Hu, Y.-C. Portcullis: Protecting Connection Setup
from Denial-of-Capability Attacks. ACM SIGCOMM
(2007).

[42] Peter, S., Javed, U., Zhang, Q., Woos, D., Anderson,

T., and Krishnamurthy, A. One Tunnel is (often)
Enough. In ACM SIGCOMM (2014).

[43] Savage, S., Wetherall, D., Karlin, A., and Anderson,

T. Practical Network Support for IP Traceback. ACM
SIGCOMM (2000).

[44] Shin, S., Porras, P., Yegneswaran, V., Fong, M., Gu,

G., and Tyson, M. FRESCO: Modular Composable
Security Services for Software-Deï¬ned Detworks. In NDSS
(2013).

[45] Song, D. X., and Perrig, A. Advanced and Authenticated
Marking Schemes for IP Traceback. In INFOCOM (2001).

[46] Sundaresan, S., De Donato, W., Feamster, N.,

Teixeira, R., Crawford, S., and Pescap`e, A. Broadband
Internet Performance: a View from the Gateway. In ACM
SIGCOMM (2011).

[47] Vissers, T., Van Goethem, T., Joosen, W., and

Nikiforakis, N. Maneuvering Around Clouds: Bypassing
Cloud-based Security Providers. In ACM CCS (2015).

[48] Walfish, M., Vutukuru, M., Balakrishnan, H.,

Karger, D., and Shenker, S. DDoS Defense by Oï¬€ense.
In ACM SIGCOMM (2006).

[49] Yaar, A., Perrig, A., and Song, D. SIFF: A Stateless

Internet Flow Filter to Mitigate DDoS Flooding Attacks. In
IEEE S&P (2004).

[50] Yang, X., Wetherall, D., and Anderson, T. A

DoS-limiting Network Architecture. In ACM SIGCOMM
(2005).

[51] Zhang, X., Hsiao, H.-C., Hasker, G., Chan, H., Perrig,
A., and Andersen, D. G. SCION: Scalability, Control, and
Isolation on Next-Generation Networks. In IEEE S&P
(2011).

1279