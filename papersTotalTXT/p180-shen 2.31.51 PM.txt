EpicRec: Towards Practical Differentially Private
Framework for Personalized Recommendation

Yilin Shen

Samsung Research America

665 Clyde Ave, Mountain View, CA 94043

yilin.shen@samsung.com

Hongxia Jin

Samsung Research America

665 Clyde Ave, Mountain View, CA 94043

hongxia.jin@samsung.com

ABSTRACT
Recommender systems typically require users‚Äô history data
to provide a list of recommendations and such recommen-
dations usually reside on the cloud/server. However, the
release of such private data to the cloud has been shown to
put users at risk. It is highly desirable to provide users high-
quality personalized services while respecting their privacy.
In this paper, we develop the Ô¨Årst Enhanced Privacy-built-
In Client for Personalized Recommendation (EpicRec) sys-
tem that performs the data perturbation on the client side
to protect users‚Äô privacy. Our system needs no assumption
of trusted server and no change on the recommendation al-
gorithms on the server side; and needs minimum user inter-
action in their preferred manner, which makes our solution
Ô¨Åt very well into real world practical use.

The design of EpicRec system incorporates three main
modules: (1) usable privacy control interface that enables
two user preferred privacy controls, overall and category-
based controls, in the way they understand; (2) user privacy
level quantiÔ¨Åcation that automatically quantiÔ¨Åes user pri-
vacy concern level from these user understandable inputs;
(3) lightweight data perturbation algorithm that perturbs
user private data with provable guarantees on both diÔ¨Äeren-
tial privacy and data utility.

Using large-scale real world datasets, we show that, for
both overall and category-based privacy controls, EpicRec
performs best with respect to both perturbation quality and
personalized recommendation, with negligible computational
overhead. Therefore, EpicRec enables two contradictory
goals, privacy preservation and recommendation accuracy.
We also implement a proof-of-concept EpicRec system to
demonstrate a privacy-preserving personal computer for movie
recommendation with web-based privacy controls. We be-
lieve EpicRec is an important step towards designing a prac-
tical system that enables companies to monetize on user data
using high quality personalized services with strong provable
privacy protection to gain user acceptance and adoption of
their services.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full cita-
tion on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS‚Äô16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978316

Keywords
Privacy-Preserving Recommendation; DiÔ¨Äerential Privacy;
Privacy Paradox

1.

INTRODUCTION

The last few decades have witnessed wide applications
of recommender systems to provide personalized service to
users, such as intelligent personal assistant and smart TV
or other content recommendations. In fact, such personal-
ized services become key business drivers for many compa-
nies. As one can understand, personalized service is based
on user‚Äôs data and oftentimes requires substantial user data
in order to provide high-quality recommendation services.

However, many user concerns about recommender systems
have been raised from privacy perspectives due to the release
of users‚Äô private data. Consumer fears over privacy continue
to escalate. Based on Pew Research, 68% consumers think
that current laws are insuÔ¨Écient to protect their privacy
and demand tighter privacy laws; and 86% of Internet users
have taken proactive steps to remove or mask their digital
footprints. Responding to increasing user privacy concerns,
governments in US/EU are increasing and enforcing existing
regulations. LG TV, as another example, was caught in
lawsuit on illegally obtaining users‚Äô private data.

To resolve the tensions between business intelligence and
user privacy, it is critically desirable to develop technologies
that can preserve and control user data privacy and in the
meanwhile still allow intelligence and personalization busi-
ness. Without such technology enabler, future users will
stop using services and companies will not be able to deploy
services due to privacy law constraints and user concerns.

A majority of existing methods [5, 6, 9, 10, 11, 19] are
developed based on the scenarios that recommender servers
are trusted such as the NetÔ¨Çix movie recommendation sys-
tem. One main reason for such assumption is that classic
recommender system algorithms, such as Collaborative Fil-
tering [25], require multiple users‚Äô data in order to perform
personalized recommendation. It is easy to understand that
a trusted server collects all users‚Äô data and can therefore per-
form such personalized recommendation. The most relevant
privacy preserving approach is proposed by McSherry et al.
[19], in which the server does the anonymization on user pri-
vate data in which random noises are added into each step
of aggregates in recommendation algorithm. All these meth-
ods attempted to protect user privacy when server releasing
user data to third party applications and business partners.
Unfortunately, in such device-cloud based recommender
systems, there are many other privacy attacks (as shown in

180Figure 1: Attacking Model

Figure 2: EpicRec System under Untrusted Server

Figure 1) that cannot be modeled and addressed in such a
trusted server setting and require other types of protections.
For example, when user data travels from device to cloud, at-
tackers can eavesdrop the transmission channel and launch
a ‚Äúman-in-the-middle attack‚Äù, therefore data may need to
be encrypted during transmission. Malicious attackers can
break into the cloud/server and steal user data. This de-
mands many security measures to take to protect data on
the cloud, such as encrypting data in storage. Moreover,
server insiders may also leak user data to other parties.

As such, in this paper, we design a novel and practical
privacy-built-in client under untrusted server settings,
in
which user data is perturbed and anonymized on their pri-
vate devices before leaving their devices and users are given
more peace of mind. As one can understand, data pertur-
bation on device side under untrusted server settings poses
extra challenges than that under trusted server settings be-
cause data perturbation has to be done without knowing
other users‚Äô data.

There are some existing approaches developed under such
untrusted server settings, including cryptography techniques
[3, 21], diÔ¨Äerential privacy-based techniques [24], and ran-
domization techniques [23]. Unfortunately, these approaches
cannot be applied in practice due to various reasons such as
computation cost, the need of an impractical trusted third
party, lack of usability and so on.

In this paper, we propose the Ô¨Årst practical Enhanced

Privacy-built-In Client for Personalized Recommendation (Epi-
cRec) system. As one can see in Figure 2, EpicRec, residing
on the user‚Äôs hub device (e.g., personal laptop, smartphone,
etc.), collects user private data from a variety of user‚Äôs de-
vices and perturbs the private data based on user‚Äôs privacy
concerns. More importantly, the existence of EpicRec on
user‚Äôs device not only satisÔ¨Åes users‚Äô privacy needs but also
requires no assumption of trusted server and no changes of
recommendation algorithms, rendering EpicRec very prac-
tical. Our contributions are summarized as follows:

‚Ä¢ We design the Ô¨Årst privacy-preserving EpicRec frame-
work on user client for personalized recommendation.
EpicRec collects user private data from various de-
vices, provides usable privacy control interfaces, quan-
tiÔ¨Åes user privacy control input and uses it to perturb
user data. EpicRec enables user preferred overall pri-
vacy control (S-EpicRec) and category-based privacy
control (M-EpicRec) to satisfy users‚Äô diÔ¨Äerent needs;
and in the meanwhile maintains low user cognitive load
by minimizing the needs of user interactions.
‚Ä¢ We design S-EpicRec and M-EpicRec systems respec-
tively, both based on the state-of-the-art diÔ¨Äerential
privacy and utility notions. We quantify the user pri-

vacy level by optimizing the utility based on the under-
lying data properties; and develop a light-weight and
data perturbation algorithm to preserve the category
aggregates with both privacy and utility theoretical
guarantees, which signiÔ¨Åcantly improves the existing
approach [24] from both privacy and utility aspects.
‚Ä¢ We conduct extensive experiments to evaluate the per-
formance of EpicRec on large-scale real-world datasets.
The results show that, from both privacy and utility
perspectives, our proposed S-EpicRec and M-EpicRec
systems consistently outperform other (pseudo) com-
petitors that apply existing methods into some com-
ponents of our EpicRec system. In addition, our ap-
proach takes less than 1.5 seconds on personal com-
puters.
‚Ä¢ We implement a proof-of-concept EpicRec system for
personalized movie recommendation with web-based
overall and category-based privacy concern controls.

The rest of paper is organized as follows. Section 2 dis-
cusses the related work. The background and architecture
design of EpicRec system are presented in Section 3. Sec-
tion 4 and 5 propose detailed design of S-EpicRec and M-
EpicRec systems supporting diÔ¨Äerent granularities of pri-
vacy controls. The experimental results and implementa-
tion of proof-of-concept system are presented in Section 6
and Section 7 respectively. Section 8 concludes the whole
paper and discusses some future work.

2. RELATED WORK

Privacy-preserving Recommendation. Table 1 shows
the comparison between our EpicRec system and existing
approaches for personalized recommendation under untrusted
server settings. The earliest work by Polat et al. [23] devel-
oped randomized mechanisms to perturb user private data
before releasing to recommender systems. However, their
method does not have provable privacy guarantees and was
later identiÔ¨Åed that using clustering method on their per-
turbed data can still accurately infer users‚Äô original raw data
with accuracy up to more than 70% [30] and make the pri-
vacy protection useless.
In the meanwhile, cryptography-
based approaches [21, 3] are proposed with privacy guar-
antees. However, these approaches require a trusted third-
party (Cryptographic Service Provider (CSP)) and expen-
sive private computation. Another class of orthogonal ap-
proaches [24] are based on the state-of-the-art diÔ¨Äerential
privacy notion, with both privacy and utility guarantees.
Unfortunately, in addition to the above limitations, all ex-
isting approaches largely ignore the usable privacy control

Cloud¬†Intrusion&¬†Data¬†LeakageUser¬†Data¬†EavesdropCloudAttackerUntrusted¬†Recommender¬†SystemRecommendationPerturbed¬†User¬†DataInsiderEpicRecServer: Existing analytics and recommendation algorithms with NO ChangesProvide Recommendation to UserPerturbed DataClientPrivacy Concerns181Table 1: Comparison between Privacy-Preserving Recommendation under Untrusted Server Settings

Approaches

Polat et al. [23]

Nikolaenko et al. [21]

Canny et al. [3]
Xin et al. [27]
Shen et al. [24]

EpicRec

No Change
of Service
Provider

(cid:88)



(cid:88)
(cid:88)

No Need
of Trusted
Third Party

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

User

Privacy
Control
Single





Single

Single &

Category-based

User-friendly

Privacy Control

Privacy

Utility

Privacy

Interface

QuantiÔ¨Åcation Guarantee

Guarantee






(cid:88)






(cid:88)


(cid:88)
(cid:88)

(cid:88)
(cid:88)



Cryptography
Cryptography

2nd Order Privacy
DiÔ¨Äerential Privacy

DiÔ¨Äerential Privacy

such that users cannot provide their privacy concerns in a
way they understand.

In addition, there are some other privacy-preserving rec-
ommendation approaches under trusted server settings [19]
or some particular recommendation services [17]. Moreover,
system developers, and policy makers recently have been
coming up with solutions at diÔ¨Äerent levels [13, 29].

Our proposed EpicRec system provides a comprehensive
solution at all levels, towards a practical and usable privacy-
preserving client for untrusted recommender system, with
strong privacy and utility guarantees.

DiÔ¨Äerential Privacy. DiÔ¨Äerential privacy [7, 8] has be-
come the de facto standard for privacy preserving data an-
alytics. Dwork et al. [8] established the guideline to guar-
antee diÔ¨Äerential privacy for individual aggregate queries by
calibrating the Laplace noise to each query based on the
global sensitivity. Various works have adopted this deÔ¨Ånition
for publishing histograms [28], search logs [15], mining data
streams [4], and record linkage [1]. Later on, a probabilis-
tic relaxation was proposed by Machanavajjhala et al. [18],
called probabilistic diÔ¨Äerential privacy. This novel diÔ¨Äeren-
tial privacy notion allows the privacy preservation with high
probability, thereby improve the Ô¨Çexibility of global sensi-
tivity analysis. An alternative approach for noise mitigation
was instance-based noise injection approaches by Nissim et
al. [22]. This paper Ô¨Årst introduced local sensitivity and its
upper bound smooth sensitivity, which allows the injection
of Admissible noise to ensure diÔ¨Äerential privacy. Unfortu-
nately, all these approaches require the strict satisfaction of
perturbed aggregates in the sanitized data and restrict their
applications to only statistical data publishing. A recent
work [24] was proposed to address the above constraints, by
perturbing data to guarantee both diÔ¨Äerential privacy and
recommendation quality. Our data perturbation module in
EpicRec framework provides better privacy and utility than
[24] from both theoretical and empirical perspectives.

3. EpicRec SYSTEM DESIGN

In this section, we present the framework design of our
proposed Enhanced Privacy-built-In Client for Recommen-
dation (EpicRec) system. The goal of EpicRec system is
three-fold: (1) enable user-friendly privacy concern control
on their private data in a way they understand; (2) quantify
user‚Äôs privacy level input from layman and user-understandable
language to quantiÔ¨Åed private budget for data perturbation;
(3) conduct light-weight perturbation of user private data on
their device such that the perturbed data can be released to
existing recommender systems to provide user high-quality
personalized recommendations. Next, we Ô¨Årst brieÔ¨Çy dis-
cuss about background to motivate and guide system design.

We then describe the overall architecture of EpicRec system
and the details of each component.

3.1 Background & Motivation

We Ô¨Årst give a succinct overview of the key results from
our two user studies (details are in separate papers to be
submitted with preliminary results in [29, 26]), in order to
motivate and guide the design of EpicRec system.

The Ô¨Årst user study focuses on the research question: Which

information is considered as private by users? Using a popu-
lar video recommender system as an example, we conducted
an online user study by recruiting 161 participants through
Amazon Mechanical Turk (MTurk) and studied 11 types of
data collected by smart TV. The majority of the participants
were male (66.5%) and White (75.8%). There are 77.0% of
participants aged between 20 and 40. We created 11 types
of data collected by smart TV including content, channel,
time(watch, change, service), status(on/oÔ¨Ä, on duration, if
using DVR), TV settings, clicked buttons and interacted ser-
vices. The results show that most participants raised their
most privacy concerns about their watching content history
for either personalized program recommendation or targeted
advertising purposes.

The second user study focuses on the research question:
How does level of control in a smart TV inÔ¨Çuence user‚Äôs
perceptions and behaviors? We recruited 505 participants
through MTurk and studied 15 diÔ¨Äerent privacy control mech-
anisms with diÔ¨Äerent levels and types of control. The major-
ity of the participants were male (57.0%) and White (75.8%).
The average age was 34.15 (range 19-72). We created 15
privacy control conditions by combining the following as-
pects: non-hierarchical and hierarchical controls on diÔ¨Äer-
ent content types (overall, category, maturity rating, watch-
ing time). The results show that the overall privacy control
and Ô¨Åner-grained category-based privacy control are the best
two control interfaces the participants selected among the 15
diÔ¨Äerent designs. The participants rated them as the most
useful in helping to make data disclosure decisions; the least
privacy concerns; the most valuable in disclosing their in-
formation for personalized recommendations; and the most
likely to use in the future.

3.2 EpicRec Architecture

Figure 3 shows the overall architecture of EpicRec system.
Our focus is on user‚Äôs device side where EpicRec system sits
while the service provider remains unchanged. The goal of
device-side EpicRec system is to perform data perturbation
on user private data with user-speciÔ¨Åed privacy concern lev-
els, such that the format of perturbed data remains the same
and recommendation results remain accurate.

182releasing protected data, and ‚ÄúAll Release‚Äù as releasing all
private data (Note that ‚ÄùAll Release‚Äù will release as much
information as possible if conÔ¨Çict happens in category-based
privacy control). The designs of EpicRec system with these
two privacy controls are later presented in Section 4 and Sec-
tion 5 respectively, referred to as S-EpicRec and M-EpicRec
systems.

C-4. Data Perturbation: perturb private user data
from C-2, using public information from C-1 and the pri-
vacy parameters from C-5. The perturbed data maintains
the same format as user private data, such that it can be
used by existing recommender algorithms. In addition, the
perturbed data needs to meet two conÔ¨Çict goals, privacy
preservation and recommendation accuracy. As such, this
data perturbation module is associated with two correspond-
ing notions: privacy notion and utility notion. Examples of
privacy notions can be diÔ¨Äerential privacy, k-anonymity, in-
formation gain, etc. while examples of utility notions can be
mean absolute error, root mean square error, TopK, etc.

C-5. Privacy QuantiÔ¨Åcation: quantify user speciÔ¨Åed
privacy concern levels to mathematical privacy parameters
to be used in data perturbation (C-4) component. The ex-
amples of these quantiÔ¨Åed parameters based on diÔ¨Äerent pri-
vacy notions can be as follows: (1) the privacy budget  in
diÔ¨Äerential privacy; (2) the value of k in k-anonymity pri-
vacy; (3) the threshold of information gain; etc.

C-6. Recommendation Output: output recommenda-
tion results (e.g., overall recommendation, per-category rec-
ommendation) to users obtained from service provider using
perturbed user data.

4. DESIGN OF S-EpicRec:

SINGLE-LEVEL PRIVACY CONTROL

In this section, we focus on the design of Single-level Epi-
cRec (S-EpicRec) to enable overall privacy control. In the
rest of this section, we Ô¨Årst introduce our focus of privacy
and utility notions, and some general notations. Then we
present the detailed design of the main components (data
perturbation (C-4) and privacy quantiÔ¨Åcation (C-5) compo-
nents) in S-EpicRec.
4.1 Privacy & Utility Notions
4.1.1 Privacy Notion
We consider using the state-of-the-art privacy notion, Dif-
ferential Privacy [8], which not only provides strong privacy
guarantee but also allows attackers to have unlimited back-
ground knowledge. Informally, an algorithm A is diÔ¨Äeren-
tially private if the output is insensitive to any particular
record in the dataset.

Definition 1

(-Differential Privacy). Let  > 0

be a small constant. A randomized algorithm A is -diÔ¨Äerentially
private if for all data sets D1 and D2 diÔ¨Äering on at most
one element, i.e., d(D1, D2) = 1, and all S ‚äÜ Range(A),

Pr[A(D1) ‚àà S] ‚â§ exp()Pr[A(D2) ‚àà S]
The probability is taken over the coin tosses of A.

(4.1)

The parameter  > 0 is called privacy budget, which allows
user to control the level of privacy. A smaller  suggests more
limit posed on the inÔ¨Çuence of an individual item, leading

Figure 3: Architecture of EpicRec System

Motivated by the user study results in Section 3.1, our
proposed EpicRec system provides users their most preferred
overall and category-based privacy controls. The goal of data
perturbation is to protect user concerned private history data.
SpeciÔ¨Åcally, EpicRec enables the protection of both individ-
ual records and categories of history data. Last but not least,
EpicRec focuses on perturbing user‚Äôs history data rather than
rating data since users are more concerned about history data
than rating data (based on our Ô¨Årst user study) and history
data can be easily obtained implicitly from user‚Äôs client while
it is infeasible to continuously request user interaction to rate
each item in large amount of his history data.

As one can see in Figure 3, the dashed lines indicate the
interactions between user and EpicRec. A user inputs his
privacy levels using user privacy control input (C-3) and pro-
vides his private data on device to user private input (C-2).
On the other hand, the solid lines indicate the interactions
between diÔ¨Äerent components. We next discuss the details
of each component as well as the interactions between them:

C-1. Public Data Input: obtain public knowledge as-
sociated with user private data from either inside or outside
knowledge resources. SpeciÔ¨Åcally, C-1 collects two types of
data from public resources: public data universe items and
their associated public categories. The goal of this compo-
nent is to preserve the quality of perturbed data without
sacriÔ¨Åcing any privacy breach that could be derived using
public information.

C-2. User Private Data Input: obtain user history
data without extra user interaction, such as location history
data on iPhone; user‚Äôs web browsing history based on the
websites users clicked, or history of watched TV programs,
movies on smart TV, etc.

C-3. User Privacy Control Input: provide user inter-
face to obtain user‚Äôs privacy concern levels. Motivated by
our user study results discussed in Section 3.1, C-3 provides
the following two granularities of privacy control interfaces:

Overall (Single-level) Privacy Control:

Provide users a single input of privacy concern level;

Category-based (Multiple-level) Privacy Control:

Provide users inputs of privacy concern levels for each
category;

In each control, we use three privacy concern levels: ‚ÄúNo
Release‚Äù as releasing no information, ‚ÄúPerturbed Release‚Äù as

C‚Äê4.¬†Data¬†PerturbationC‚Äê4.¬†Data¬†PerturbationC‚Äê5.¬†Privacy¬†QuantificationC‚Äê5.¬†Privacy¬†QuantificationC‚Äê3.¬†User¬†Privacy¬†Control¬†InputC‚Äê3.¬†User¬†Privacy¬†Control¬†InputEpicRecon¬†DeviceRecommendation¬†using¬†PerturbedUser¬†DataRecommendation¬†using¬†PerturbedUser¬†DataC‚Äê2.¬†User¬†Private¬†Data¬†InputC‚Äê2.¬†User¬†Private¬†Data¬†InputC‚Äê1.¬†Public¬†Data¬†InputC‚Äê1.¬†Public¬†Data¬†InputExisting¬†Recommender¬†SystemC‚Äê6.¬†Recommendation¬†OutputC‚Äê6.¬†Recommendation¬†OutputPerturbedUser¬†DataPerturbedUser¬†Data183Table 2: Notations

Symbol

Description

I
C
C
dr
dp
PT


public item set of size n
public category set of size c
public item-category correlation matrix of size n √ó c
user‚Äôs private item vector of size n
user‚Äôs perturbed item vector of size n
privacy concern level
quantiÔ¨Åed privacy budget

to stronger privacy protection. More importantly, the ap-
plication of diÔ¨Äerential privacy ensures perturbed data in-
dependent of any auxiliary knowledge [7] the adversary may
obtain from public data in C-2.
4.1.2 Utility Notion
Recommender systems typically require many users‚Äô his-
tory data for providing each user a list of his/her personal-
ized recommendations. However, when perturbing data on
each user‚Äôs device side under untrusted server settings, the
device does not even know other users‚Äô data (no matter pri-
vate or perturbed). Therefore, it is impractical to directly
use the quality of recommendation results as a utility notion.
As such, we alternatively consider using similar utility in
[24] to measure data category aggregates on each user‚Äôs per-
turbed data as our utility notion. More speciÔ¨Åcally, we use
expected Mean Absolute Error (MAE) between user‚Äôs raw
and perturbed category aggregates, which is shown later in
experiment to be suÔ¨Écient to guarantee recommendation
accuracy even without knowing other users‚Äô data.
4.2 Notations

We deÔ¨Åne notations based on in each component:
C-1: Let I be public universe/set of items of size |I| =
n. Public category set is deÔ¨Åned as C of size |C| = c, in
which each item is associated with a subset of categories
represented by a public item-category correlation matrix C
of size n √ó c. An item i is associated with category j if and
only if the entry cij in C is equal to 1.

C-2: User‚Äôs raw private history is denoted as a vector dr
of size n. The ith entry in dr is either 1 or 0, meaning
that item i does or does not belong to user‚Äôs private history.
Note that those items in private history but not in collected
public set will be simply be considered no release.
C-3: User‚Äôs privacy concern level, denoted as PT, belongs
to one of the following three levels, {‚ÄúNo Release‚Äù, ‚ÄúPer-
turbed Release‚Äù, ‚ÄúAll Release‚Äù }. When PT is selected as
‚ÄúNo Release‚Äù or ‚ÄúAll Release‚Äù, the device simply releases no
private or all private data to recommender system respec-
tively.

C-4: User‚Äôs perturbed data is denoted as a vector dp of
size n. The ith entry in dp is either 1 or 0, meaning that
item i does or does not belong to user‚Äôs perturbed data.

C-5:  is the quantiÔ¨Åed privacy parameter (privacy bud-
get in diÔ¨Äerential privacy) when PT is selected as Perturbed
Release.

For simplicity and consistency, we denote the ith entry in
a vector v as v(i) in the rest of paper. For reference, we list
all notations in Table 2.
4.3 Design of Data Perturbation (C-4)

As described in Section 3.2, data perturbation compo-
nent (C-4) generates perturbed user data to meet both pri-
vacy preservation and recommendation quality, speciÔ¨Åcally

via the privacy and utility notions in Section 4.1. The rest
of this subsection consists of problem deÔ¨Ånition, challenges,
proposal algorithm and theoretical analysis.
4.3.1 Problem DeÔ¨Ånition

Problem 1

(S-Perturbation Problem). Given a user‚Äôs

private item vector dr associated with public item set I, a
public item-category correlation matrix C, privacy budget
 > 0. The objective is to generate the user‚Äôs perturbed item
vector dp such that (1) (privacy goal) the category aggre-
gates (number of items belonging to each category) of per-
turbed data satisfy -diÔ¨Äerential privacy with the presence or
absence of an individual item to defend against privacy leak-
age via public category information; (2) (utility goal) the
quality of category aggregates on perturbed data is well main-
tained using metrics in Section 4.1.2. SpeciÔ¨Åcally, the for-
mal mathematical deÔ¨Ånition of Expected Mean Absolute Er-
ror (MAE) is deÔ¨Åned as E
, given
user raw and perturbed category aggregates CR and CP.

j=1 |CR(j) ‚àí CP (j)|(cid:105)
(cid:80)c

(cid:104) 1

c

Remarks: Our deÔ¨Åned S-Perturbation problem targets on
a stronger privacy guarantee (-diÔ¨Äerential privacy rather
than (, Œ¥)-diÔ¨Äerential privacy in [24]) and relaxes the objec-
tive (discard the maximization of diÔ¨Äerence between private
and perturbed data in [24]), which is shown in later experi-
ments that does not hurt the quality of data perturbation.
4.3.2 Challenges
Large Magnitude of Noises for Achieving  Dif-
ferential Privacy. One of the most widely used mecha-
nisms to achieve -diÔ¨Äerential privacy is Laplace mechanism
[8] (Theorem 1), which adds random noises to the numeric
output of a query, in which the magnitude of noises follows
Laplace distribution with variance ‚àÜf
 where ‚àÜf represents
the global sensitivity of query f (DeÔ¨Ånition 2).

Definition 2

(Global Sensitivity [8]). For a query

f : D ‚Üí Rk, the global sensitivity ‚àÜf of f is

‚àÜf = max

d(D1,D2)=1

(cid:107)f (D1) ‚àí f (D2)(cid:107)1

(4.2)

for all neighboring datasets D1, D2, i.e., d(D1, D2) = 1.

Theorem 1

(Laplace Mechanism [8]). For f : D ‚Üí
Rk, a randomized algorithm Af = f (D) + Lapk( ‚àÜf
 ) is -
diÔ¨Äerentially private.
(The Laplace distribution with pa-
rameter Œ≤, denoted Lap(Œ≤), has probability density function
(z) = 1
Œ≤ ) and cumulative distribution function
2 (1 + sgn(z)(1 ‚àí exp(‚àí |z|

2Œ≤ exp(‚àí |z|

Œ≤ ))).)

1

Unfortunately, as an item usually belongs to many cate-
gories, the naive application of Laplace mechanism results
in the signiÔ¨Åcantly large noise magnitude and uselessness of
perturbed data because of large global sensitivity.

Intractability of Generating Useful Perturbed Data.

Even after the noise magnitude is determined, the data per-
turbation still remains intractable (NP-hard) when we need
to guarantee the usefulness of perturbed data.
4.3.3 Proposed S-DPDP Approach
In this subsection, we propose a novel Single-Level DiÔ¨Äer-
entially Private Data Perturbation (S-DPDP) Algorithm to

184solve Problem 1. In general, S-DPDP algorithm consists of
two phases to overcome the aforementioned two challenges:
(1) Phase 1, noise calibration, focuses on selecting the mag-
nitude (denoted as z(j)) for each category using public do-
main knowledge that determines injected Lap(z(j)) noises
for each category; (2) Phase 2, data sanitization, aims to
generate the useful perturbed data based on the noisy cat-
egory aggregates. Note that this phase will not lead to any
privacy loss without the access to user private data. We
next present the details of these two phases (Algorithm 1).
Phase 1: Noise calibration. The selection of noise magni-
tude is determined by optimizing the expected MAE deÔ¨Åned
in 4.3.1. Here we denote z = (z(1), . . . , z(c)) in which z(j) is
the magnitude of Laplace noise for category j. For simplic-
1
ity, we also denote zI = ( 1
z(c) ). Therefore, the noise
magnitude of Laplace noises on each category aggregate can
be determined via the following mathematical programming
(4.3):

z(1) , . . . ,

(cid:107)z(cid:107)1

minimize
subject to CzI ‚â§ 1, z, zI ‚â• 0

(4.3)

The objective in (4.3) is to minimize expected MAE of all
injected Laplace noises onto category aggregates since the
noise on each category aggregate has E[|Lap(z(j))|] = z(j)
and the injected noises are independent. The Ô¨Årst constraint
serves two purposes: First, it imposes the -diÔ¨Äerential pri-
vacy guarantee as later shown in privacy analysis (Theorem
2); Second, it captures the correlation between categories
from the public information that what categories each item
belongs to. The last two constraints ensure the non-negative
noise magnitude of z and zI.

As the formulation (4.3) is non-convex, we then transform
it into the convex programming (4.4) to obtain a global op-
timal solution. SpeciÔ¨Åcally, we regard both z and zI as free
variables. For the sake of clariÔ¨Åcation, we introduce two
more variables z1 = z, z2 = zI. Then, we add additional
constraints z1(j)z2(j) = 1 for each category j to ensure their
reciprocal relationship. Moreover, we further relax this con-
straint to z1z2

T ‚â• I.

(cid:107)z1(cid:107)1

minimize
subject to Cz2 ‚â§ 1, z1z2

T ‚â• I, z1, z2 ‚â• 0

(4.4)

In this phase, our data perturbation algorithm Ô¨Årst solves
the convex programming (4.4). Then, we set zI = z2 such
that the Ô¨Årst constraint in (4.4) is not violated; and set z by
letting each entry z(j) be the reciprocal of the jth entry in
zI. Thanks to the convexity property of (4.4), our optimized
noise calibration algorithm is guaranteed to outperform the
traditional Laplace mechanism.

Example 1. Figure 4 shows a running example that ex-
plains why our novel noise calibration approach (phase 1)
can always outperform the existing Laplace mechanism in
Theorem 1. In this tiny example, public set of items con-
tains Ô¨Åve items and their associated Ô¨Åve categories. A check
represents that an item belongs to this category (e.g., item
1 belongs to category 1,2,3). The row in gray shows our
novel category based sensitivity obtained by solving (4.4) with
 = 1. For a category associated with more items, the sen-
sitivity of this category intends to be larger since there is
higher probability that the aggregate of this category will be
aÔ¨Äected by adding or removing a single item. Therefore, the
last column of last two rows shows a better MAE error using

Input : private user data dr, item-category matrix C,

privacy budget 

Output: perturbed user‚Äôs data dp
// Phase 1: Noise calibration
1 Solve mathematical programming (4.4);
2 z ‚Üê reciprocal of each entry in zI;
3 Sample noises from Lap(z(j)) for each category j;
4 Set NA with each entry

N A(j) =(cid:80)

i‚ààI cij dr(i) + Lap(z(j));

// Phase 2: Data sanitization

5 Relax integral constraints in (4.5);
6 Solve the relaxed (4.5) by replacing dp with dr
7 foreach each element i in dr
8

Randomly select a number Œæ between 0 and 1;
dp(i) ‚Üê 1 if Œæ ‚â§ dr
p(i) and dp(i) ‚Üê 0 otherwise;

p do

9

p ‚àà [0, 1]n;

10 end
11 return dp;

Algorithm 1: S-DPDP Algorithm

our category based sensitivity via (4.4) than using traditional
global sensitivity.

Figure 4: Running Example of Noise Calibration

Phase 2: Data sanitization. This phase takes the above
noise magnitude vector output z as input and generates the
useful perturbed user data. We Ô¨Årst quantify the useful-
ness of perturbed data by minimizing the error between the
category aggregates on perturbed data and the noisy cat-
egory aggregates. SpeciÔ¨Åcally, we introduce two error vec-
tors l, r and consider the root mean square error (RMSE)
as 1
2. Then, we formulate the following optimization
formulation (4.5):

2(cid:107)l + r(cid:107)2

1

2(cid:107)l + r(cid:107)2

2

minimize
subject to NA ‚àí l ‚â§ CT dp ‚â§ NA + r, dp ‚àà {0, 1}n (4.5)
where NA is the noisy category aggregate vector in which

each entry N A(j) =(cid:80)

i‚ààI cijdr(i) + Lap(z(j)).

It is not hard to see that solving (4.5) is NP-hard by re-
ducing it from Exact Cover problem (The proof is similar
to that in [24] and omitted due to space limit). Therefore,
our data perturbation algorithm solves the relaxed formula-
p ‚àà [0, 1]n.
tion of (4.5) by replacing dp with dr
Then, we obtain dp by rounding each entry dr
p(i) to 1 with
probability dr
4.3.4 Theoretical Analysis
We theoretically analyze privacy and utility, as well as

p in which dr

p(i).

time complexity.

Privacy Analysis. We show the diÔ¨Äerential privacy guar-

antee of S-DPDP algorithm:

Theorem 2

(S-DPDP Privacy Analysis). S-DPDP al-

gorithm enjoys -diÔ¨Äerential privacy.

item/categorycategory 1category 2category 3category 4category 5MAEerror when privacy budget ùùê=ùüèitem 1ÔÉºÔÉºÔÉºitem2ÔÉºÔÉºitem3ÔÉºÔÉºÔÉºitem4ÔÉºÔÉºitem5ÔÉºÔÉºSensitivity3.612.363.342.361.382.61globalsensitivity equals to 3 for all categories3185(cid:89)

Proof. We observe that there is no privacy loss in phase
2 of S-DPDP algorithm as it is considered as post-processing
on diÔ¨Äerentially private category aggregates without the ac-
cess of user private data. Since any post-processing of the
answers cannot diminish this rigorous privacy guarantee ac-
cording to Hey et al. [12], we only need to focus on analyzing
the privacy guarantee in phase 1 of S-DPDP algorithm.

Let D1, D2 be neighboring datasets (i.e., d(D1, D2) = 1)
and f (Di) be the category aggregates of user‚Äôs private data
i). For any r = (r1, . . . , rc) ‚àà Range(S-DPDP),
Di (w.r.t. dr
we have the following analysis:

Pr[S-DPDP(D1)(j) = r(j)]
Pr[S-DPDP(D2)(j) = r(j)]

Pr[S-DPDP(D1) = r]
Pr[S-DPDP(D2) = r]

(cid:16) ‚àí(cid:88)
(cid:16) ‚àí max

j‚ààC

1

z(j)

=

j‚ààC

|fj(D1) ‚àí fj(D2)|(cid:17)
(cid:88)

1

‚â• exp

‚â• exp

d(D1,D2)=1

z(j)

j‚ààC

|fj(D1) ‚àí fj(D2)|(cid:17) ‚â• e

‚àí

The Ô¨Årst step holds due to the independently injected noises
on each category aggregate; the second step is derived from
the injected Laplace noises and triangle inequality; and the
last step holds from the Ô¨Årst constraint in (4.4), that is,

max

d(D1,D2)=1

z(j)

j‚ààC

1

|fj(D1) ‚àí fj(D2)| = max
i‚ààI

1

cij ‚â§ 

z(j)

j‚ààC

(cid:88)

(cid:88)

The proof is complete.

Utility Analysis. We show the utility bound of MAE

on category aggregates between raw and perturbed data:

Theorem 3

(S-DPDP Utility Analysis). The expected

MAE between raw and perturbed category aggregates (via S-
DPDP algorithm) is upper bounded by 2(cid:107)z(cid:107)1/c, where z is
the optimal solution of (4.4).

Proof. Let z(j), lj, rj be the jth entry in vectors z, l, r.

E[c ¬∑ MAE] = E

j=1

(cid:104) c(cid:88)
|Lap(z(j))|(cid:105)
(cid:105)
(cid:104)(cid:107)l + r(cid:107)1

‚â§ E

(cid:104) c(cid:88)
‚â§ c(cid:88)

j=1

j=1

= (cid:107)z(cid:107)1 + E

E[|Lap(z(j))|] + E

|CR(j) ‚àí CP (j)|(cid:105)
(cid:104) c(cid:88)
| max{lj, rj}|(cid:105)
(cid:104) c(cid:88)

(cid:105)

j=1

(lj + rj)

+ E

j=1

Then, let xj be the random variable representing noisy
aggregate on category j with probability density function
œÜ(xj). We analyze the bound of E

as follows:

E

¬∑¬∑¬∑

(cid:105)
(cid:104)(cid:107)l + r(cid:107)1
(cid:90)
(cid:104)(cid:107)l + r(cid:107)1|x1, . . . , xc
(cid:90) (cid:16) c(cid:88)
(cid:104)(cid:90)
(cid:104)

(cid:90)
(cid:90)
c(cid:88)

¬∑¬∑¬∑

(cid:104)

(cid:105)

j=1

E

E

lj + rj|xj

œÜ(xj)dxj

E

=

‚â§

=

(cid:105)

(cid:104)(cid:107)l + r(cid:107)1
(cid:105) c(cid:89)
(cid:105)(cid:17) c(cid:89)
(cid:105) ‚â§ c(cid:88)

j=1

j=1

j=1

j=1

lj + rj|x1, . . . , xc

œÜ(xj)dx1 ¬∑¬∑¬∑ dxc

œÜ(xj)dx1 ¬∑¬∑¬∑ dxc

|z(j)| = (cid:107)z(cid:107)1

(cid:104)
n(cid:88)

E

where the Ô¨Årst step derives from the law of total expectation;
the last step holds because for each category j ‚àà C, we have
the following inequality:

(cid:105)

(cid:104)

(cid:105)

lj + rj|x1, . . . , xc

= E

lj + rj|xj

=

|cijdr

p(i) ‚àí N A(j)|dr

p(i) ‚â§ |xj| = |zj|

i=1

where the last step holds from the fact that dr
the optimal solution to relaxed (4.4).

p(i) ‚â§ 1 and

Time Complexity Analysis. We analyze the time com-
plexity to phase 1 and 2 respectively. The time complex-
ity of phase 1 is determined by solving (4.4).
It is not
hard to see that both vector variables z1 and z2 have di-
mension c and there are n + 3c constraints. In practice, c
is equal to the number of categories from public informa-
tion, which is actually a constant. Therefore, according to
Megiddo [20], the time complexity of solving (4.4) is O(n).
In phase 2, we Ô¨Årst solve the relaxed (4.5), which is equiv-
alent to solving the non-negative least square programming
p ‚â• 0 that has been well studied in
1
literature [2] and will be shown in Section 6.2 with fast run-
ning time in practice. The last rounding phase takes another
O(n) time.
4.4 Design of Privacy QuantiÔ¨Åcation (C-5)

p ‚àí NA(cid:107)2

2(cid:107)CT dr

2, s.t. dr

In this subsection, we design the privacy quantiÔ¨Åcation
(C-5) component to quantify ‚ÄúPerturbed Release‚Äù level to a
privacy budget , which is used to feed into S-DPDP al-
gorithm in data perturbation component (C-4) discussed
above. SpeciÔ¨Åcally, we devise a novel Single Privacy Budget
QuantiÔ¨Åcation (S-PBQ) algorithm to select a privacy budget
 to optimize the utility of perturbed data.

The idea of S-PBQ algorithm is based on our observation
that the utility loss of perturbed data will not signiÔ¨Åcantly de-
crease any more when privacy budget  is larger than some
threshold. In detail, S-PBQ algorithm Ô¨Årst understands the
distribution of noise magnitude to each category aggregate
and then search the optimal privacy budget after which util-
ity loss can be negligible. Next, we discuss about the details
of these two phases in S-PBQ algorithm.

Phase 1: Noise magnitude determination. We follow the
idea of phase 1 in S-DPDP algorithm. The key tweak is
based on the following observation: when we replace  in
phase 1 of Algorithm 1 with an arbitrary constant Œ±, it is
easy to see that each entry in new solution z(cid:48) is propor-
tional to that in z obtained in phase 1 of Algorithm 1. The
proportionality constant is equal to 
plicity and then obtain z(cid:48)
ical programming similar as (4.4):

Therefore, in this phase, we consider using Œ± = 1 for sim-
2 using the following mathemat-

Œ± , i.e., z(cid:48) = 

1, z(cid:48)

Œ± z.

(cid:107)z(cid:48)

1(cid:107)1

minimize
subject to Cz2 ‚â§ 1, z(cid:48)

1z(cid:48)

2

T ‚â• I, z(cid:48)

1, z(cid:48)

2 ‚â• 0

(4.6)

Our S-PBQ algorithm Ô¨Årst solves (4.6) and then sets z(cid:48)
I =
z(cid:48)
2 and each entry in z(cid:48) as the reciprocal of each entry in z(cid:48)
I.
Phase 2: Privacy budget optimization. This phase deter-
mines the privacy budget  using the above z(cid:48)
I, z(cid:48). Based on
the idea that the utility loss of perturbed data will not sig-
niÔ¨Åcantly decrease after  is larger than some threshold, we
search for the optimal  from 0 using a small learning step

186Input : item-category matrix C, learning step rate Œ¥
Output: quantiÔ¨Åed optimal privacy budget 
// Phase 1: Noise magnitude determination

1 Solve mathematical programming (4.6);
2 z(cid:48) ‚Üê reciprocal of each entry in zI;

// Phase 2: Privacy budget calculation
3 Formulate (4.7) with sampled S and RandAs;
4 foreach i = 1, 2, . . . do
5

Solve relaxed (4.7) with I = 1
as line 5-10 in Algorithm 1);
err(i) ‚Üê 1
if i ‚â• 2 & err(i) ‚àí err(i ‚àí 1) ‚â• err(i ‚àí 1) ‚àí err(i ‚àí 2)
then

m(cid:107)l + r(cid:107)1 + I with rounded dp;

iŒ¥ and round dp (similar

6

7

8

9

 ‚Üê (i ‚àí 1)Œ¥;
break;

end

10
11 end

// Repeat Phase 2

12 Repeat [Phase 2] N times and return averaged ;

Algorithm 2: S-PBQ Algorithm

rate Œ¥ (we choose Œ¥ = 0.02 as an experience value in prac-
tice). That is, we solve the following formulation (4.7) by
substituting I with 1
3Œ¥ , . . . The algorithm terminates
at ith iteration when the improvement of optimal utility loss
obtained by (4.7) using I = 1
(i‚àí1)Œ¥ is no smaller
than that using I = 1
(i‚àí2)Œ¥ . At last, we set
 = (i ‚àí 1)Œ¥.

(i‚àí1)Œ¥ over I = 1

iŒ¥ over I = 1

2Œ¥ , 1

Œ¥ , 1

minimize
subject to RandAs + I Sz(cid:48) ‚àí l ‚â§ CT dp

2

1

2(cid:107)l + r(cid:107)2
‚â§ RandAs + I Sz(cid:48) + r
dp ‚àà {0, 1}n

(4.7)

where S = diag(s1, . . . , sm), in which each diagonal entry
sj is sampled from Laplace distribution Lap(1) such that
z(cid:48)(j)s(j) is a sample from Laplace distribution Lap(z(cid:48)(j))
due to the property of Laplace distribution kX ‚àº Lap(k¬µ, kŒ≤)
for a random variable X ‚àº Lap(¬µ, Œ≤) for a positive constant
k [16]. In addition, in order to avoid potential privacy leak-
age caused by  quantiÔ¨Åcation, we do not use user‚Äôs private
history dr. Instead, we consider using RandAs, the cate-
gory aggregates on randomly selected items from all public
items. For simplicity, we use uniform sampling with sam-
pling rate as the averaged number of items for each user.

Our S-PBQ algorithm repeatedly samples diÔ¨Äerent S and
RandAs to obtain I by solving (4.7) in this phase. In the
experiment, we select the number of repeat times N to be 10
by considering the eÔ¨Éciency of S-PBQ algorithm. At last,
the averaged  is chosen as the quantiÔ¨Åed privacy budget.

Time Complexity Analysis. S-PBQ algorithm is sim-
ilar as S-DPDP algorithm with O(n) (phase 1 and rounding
steps in phase 2) and the running time to solve relaxed (4.7).
The second phase has its time complexity O(T ) similar as
that of solving relaxed (4.5). To provide strong privacy guar-
antee, we consider  ‚â§ 1 and therefore the repeat of phase
2 will take at most O(T + n). Our experimental results in
Figure 7 shows the eÔ¨Écient running time in practice.
4.5 Overall Analysis of S-EpicRec System

We summarize the performance of S-EpicRec, that is, the
output of S-DPDP algorithm with the quantiÔ¨Åed privacy
budget using S-PBQ algorithm.

Privacy Guarantee. The perturbed data using S-EpicRec

satisÔ¨Åes -diÔ¨Äerential privacy where  is determined by S-
PBQ algorithm (Algorithm 2).

(cid:80)c

j=1 cij.

c

(cid:107)C(cid:107)1

), where (cid:107)C(cid:107)1 = max1‚â§i‚â§n

Utility Guarantee. The expected MAE between raw
and perturbed category aggregates (output of S-EpicRec) is
upper bounded by O(
It is not hard to see that the optimal solution of (4.6) is
upper bounded by (cid:107)C(cid:107)1. Therefore, the optimal solution of
(4.4) is upper bounded by O(
) based on Theorem 3, in
which the constant factor is dependent on the quantiÔ¨Åed .
Time Complexity. The overall time complexity of S-
EpicRec is O(n + T ) where T is the time complexity of solv-
ing relaxed (4.5) as discussed in [2].

(cid:107)C(cid:107)1

c

5. DESIGN OF M-EpicRec:

MULTI-LEVEL PRIVACY CONTROL

In this section, we further design a M-EpicRec framework
to enable the category-based privacy concern controls. The
idea of M-EpicRec is extended from S-EpicRec proposed in
Section 4. Basically, we consider using the same privacy
and utility notions for designing C-4 and C-5 components in
M-EpicRec system. The rest of this section focuses on the
diÔ¨Äerent parts between S-EpicRec and M-EpicRec, mainly
in terms of notations and the design of C-4 and C-5.
5.1 Notations

In M-EpicRec, the only diÔ¨Äerent notation from those in S-
EpicRec is in user privacy control component (C-3). Specif-
ically, we deÔ¨Åne the vector of privacy concern levels PT =
{PT(1), . . . , PT(c)} to replace a single PT. Each entry PT(j)
is the user-speciÔ¨Åed privacy concern level for category j,
which still belongs to one of the same three levels {‚ÄúNo
Release‚Äù, ‚ÄúPerturbed Release‚Äù, ‚ÄúAll Release‚Äù }. Correspond-
ingly, we deÔ¨Åne the vector privacy budget  = {(1), . . . , (c)},
where (j) =  when PT(j) is selected as ‚ÄúPerturbed Release‚Äù
and (j) = 0 when PT(j) is selected as either ‚ÄúNo Release‚Äù
or ‚ÄúAll Release‚Äù indicating that no randomness is considered
for these categories.
Moreover, we deÔ¨Åne three vectors Ln, Lp, La with respect
to three privacy concern levels {‚ÄúNo Release‚Äù, ‚ÄúPerturbed
Release‚Äù, ‚ÄúAll Release‚Äù }, based on PT. For example, when
PT=(no, perturbed, no, all, all, perturbed) w.r.t. 6 cate-
gories, we have Ln = (1, 0, 1, 0, 0, 0), Lp = (0, 1, 0, 0, 0, 1), La =
(0, 0, 0, 1, 1, 0). Note that each category has one privacy tol-
erance level and Ln + Lp + Ln = 1.
5.2 Design of Data Perturbation (C-4)

5.2.1 Problem DeÔ¨Ånition
We consider M-Perturbation Problem which diÔ¨Äers
from S-Perturbation Problem deÔ¨Åned in Section 4.3.1
from the following aspects:
(1) M-Perturbation problem
takes two diÔ¨Äerent inputs: the category-based privacy con-
cern levels Ln, Lp, La (derived from PT) and a privacy bud-
get  for categories with ‚Äúperturbed release‚Äù privacy concern
level; (2) M-Perturbation problem targets on maintaining
the quality of category aggregates on perturbed data only
associated with ‚Äúperturbed release‚Äù categories while releas-
ing no data in ‚Äúno release‚Äù categories and all raw data only
associated with ‚Äúall release‚Äù categories. (Note that we choose
to prioritize privacy protection when it conÔ¨Çicts with util-
ity. That is, we do not release an item as long as one of its
categories is set ‚Äúno release‚Äù; we perturb an item if one of
its categories is set ‚Äúperturbed release‚Äù privacy concern level

187and none of its categories is set ‚Äúno release‚Äù; we release an
item only if all of its categories are set ‚Äúall release‚Äù.)
5.2.2 Challenges
In addition to the challenges of S-Perturbation problem
described in Section 4.3.2, M-Perturbation problem poses
some additional challenges mainly from the constraints from
‚Äúno release‚Äù and ‚Äúall release‚Äù categories. Especially when an
item belongs to multiple categories, the preservation of util-
ity on category aggregates on perturbed data in the ‚Äúper-
turbed release‚Äù categories becomes more diÔ¨Écult.
5.2.3 Proposed M-DPDP Approach
In this subsection, we focus on discussing about the novel
part of M-DPDP approach beyond S-DPDP approach (in
Section 4.3.3), which is developed to support multi-level pri-
vacy control. SpeciÔ¨Åcally, as the key idea of these two ap-
proaches are consistent, we will mainly present the diÔ¨Äerence
in phase 1 and 2 respectively.

Phase 1: Noise calibration. The major diÔ¨Äerence between
phase 1 in M-DPDP and that in S-DPDP is that the noises
are injected only on categories with ‚Äúperturbed release‚Äù pri-
vacy level while the noise magnitude on other categories are
enforced to be 0. Thus, (4.3) can be rewritten as follows:

minimize
subject to CzILpI ‚â§ 1, (Ln + La)T z = 0

(cid:107)z(cid:107)1
(Ln + La)T zI = 0, z, zI ‚â• 0

(5.1)

where the Ô¨Årst constraint imposes the satisfaction of diÔ¨Äer-
ential privacy on these categories with ‚Äúperturbed release‚Äù
privacy level; the next two constraints ensure no noise cal-
ibration into other categories. Accordingly, the quadratic
programming (4.4) can be rewritten as follows:

(cid:107)z1(cid:107)1

minimize
subject to Cz2LpI ‚â§ 1

(Ln + La)T z1 = 0, (Ln + La)T z2 = 0
z1z2

T ‚â• IT LpI, z1, z2 ‚â• 0

(5.2)

In this phase, M-DPDP algorithm solves (5.2) and sets
z2(j) if category j has ‚Äúperturbed release‚Äù privacy

z(j) = 1
level and z(j) = 0 otherwise.

Phase 2: Data sanitization. The major diÔ¨Äerence in phase

2 is from the following two aspects:

First, in order to address the constraints of categories with
‚Äúno release‚Äù and ‚Äúall release‚Äù privacy levels, we select ‚Äúall
release‚Äù user data (denoted as da
r ) from user raw data dr,
where the ith entry in da
r is 1 if and only if this data belongs
to user raw data (ith entry in dr is 1) and all of its associated
categories have ‚Äúall release‚Äù privacy levels.

Second, we inject Lap(z(j)) into the jth category for those
categories with ‚Äúperturbed release‚Äù privacy level. Then, we
reformulate (4.7) as follows:

1

2

minimize
subject to NA ‚àí l ‚â§ CT dp ‚â§ NA + r

2(cid:107)l + r(cid:107)2
dp ‚â• da
(Ln + La)T l = 0, (Ln + La)T r = 0

r , dp ‚àà {0, 1}n

where N A(j) = (cid:80)
lease‚Äù privacy level; and N A(j) =(cid:80)

i‚ààI cijdr(i) for categories with ‚Äúall re-
lease‚Äù privacy level; N A(j) = 0 for categories with ‚Äúno re-
i‚ààI cijdr(i) + Lap(z(j))
for categories with ‚Äúperturbed release‚Äù privacy level. The
second constraint imposes that the raw data only in cate-
gories with ‚Äúall release‚Äù privacy level will be released; the

(5.3)

last two constraints guarantee the equivalence for categories
with ‚Äúall release‚Äù and ‚Äúno release‚Äù privacy levels.
5.2.4 Theoretical Analysis.

Theorem 4

(M-DPDP Privacy Analysis). M-DPDP

algorithm enjoys -diÔ¨Äerential privacy.

Theorem 5

(M-DPDP Utility Analysis). The expected

MAE between raw and perturbed aggregates (via M-DPDP
algorithm) is upper bounded by 2(cid:107)z(cid:107)1/c, where z is the op-
timal solution of (5.2).

The proofs are similar as Theorem 2, 3 and omitted due to
space limit. The time complexity of M-DPDP is also similar
as S-DPDP and omitted.
5.3 Design of Privacy QuantiÔ¨Åcation (C-5)

We propose M-PBQ approach in this section, extending
from S-PBQ algorithm in S-EpicRec system (Section 4.4).
Similarly, in phase 1, M-PBQ replace unknown  in (5.2) by
1 and solve it to obtain z(cid:48).

The diÔ¨Äerence between them mainly lies in the second
phase. In phase 2, we substitute (4.7) in Algorithm 2 with
the following formulation:

2

1

minimize
subject to RandAm + I Sz(cid:48) ‚àí l ‚â§ CT dp

2(cid:107)l + r(cid:107)2
‚â§ RandAm + I Sz(cid:48) + r
dp ‚â• da
(Ln + La)T l = 0, (Ln + La)T r = 0

r , dp ‚àà {0, 1}n

(5.4)

where S = diag(s1, . . . , sm) , in which the jth diagonal entry
s(j) is sampled from Laplace distribution Lap(1) if this cat-
egory has ‚Äúperturbed release‚Äù privacy level and 0 otherwise;
RandAm is the category aggregates on randomly selected
items from all public items, in which Randm(j) = 0 for cat-
egories with ‚Äúno release‚Äù privacy level. The time complexity
of M-PBQ is similar as S-PBQ and omitted.
5.4 Performance Analysis of M-EpicRec

We summarize the performance of M-EpicRec as follows:
Privacy Guarantee. The category aggregates of per-
turbed data from M-EpicRec satisfy -diÔ¨Äerential privacy
where  is determined by M-PBQ algorithm.

Utility Guarantee. The expectation of MAE between

(cid:80)m

raw and perturbed category aggregates (output of M-EpicRec)
is upper bounded by O(

), where (cid:107)C(cid:107)1 = max1‚â§i‚â§n

(cid:107)C(cid:107)1

j=1 cij.

c

Time Complexity. The overall time complexity of S-
EpicRec is O(n + T ) where T is the time complexity of solv-
ing relaxed (5.3) as discussed in [2].

6. EXPERIMENTAL EVALUATION
6.1 Datasets, Metrics, Competitors & Settings
Datasets. We test EpicRec on two real-world datasets:
MovieLens1: a movie rating dataset collected by the Grou-

pLens Research Project at the University of Minnesota through
the website movielens.umn.edu during the 7-month period
from September 19th, 1997 through April 22nd, 1998. The
number of movie categories is 18. We use the MovieLens-
1M, with 1000,209 ratings from 6,040 users on 3,883 movies.

1

http://grouplens.org/datasets/movielens

188Yelp2: a business rating data provided by RecSys Chal-
lenge 2013, in which Yelp reviews, businesses and users are
collected at Phoenix, AZ metropolitan area. The number
of business categories is 21. We use all reviews in training
dataset, with 229,907 reviews from 43,873 users on 11,537
businesses.

and M-EpicRec systems from the following aspects:

Metrics. We evaluate perturbation quality of S-EpicRec
‚Ä¢ Perturbed Category Aggregates Quality: we use the ex-
pected MAE metric discussed in Section 4.1.2 with its math-
ematical deÔ¨Ånition in Section 4.3.4;
‚Ä¢ Recommendation Accuracy: we consider the MAE Loss
between the recommendation results using raw and per-
p ‚àíGT ui|
r ‚àíGT ui| ‚àí 1, where
turbed data, deÔ¨Åned as
n is the number of items, U is the number of all users and
Recui
p are the elements in ith column and uth row
(item i for user u) in predicted recommendation matrices
Recr, Recp using user raw data dr and perturbed data dp.
In addition, we also show the scalability of our EpicRec

(cid:80)U
(cid:80)U
u=1 |Recui
u=1 |Recui

r , Recui

(cid:80)n
(cid:80)n

i=1

i=1

system is further validated via running time.

(Pseudo) Competitors. As this paper is the Ô¨Årst at-
tempt for designing a privacy preserving system to enable
user-understandable privacy concern control, there is no ex-
isting work to fairly compare with our approach. Therefore,
we consider embedding existing approaches into our system,
called pseudo-competitors. SpeciÔ¨Åcally, we plug them into
S-DPDP/M-DPDP algorithms to replace phase 1 for noise
calibration only, which Ô¨Årst uses our quantiÔ¨Åed privacy bud-
get  from S-PBQ/M-PBQ algorithms and then sanitizes
data by phase 2 in S-DPDP/M-DPDP algorithms.

rithms into EpicRec system for comparison:

We plug the following two existing noise calibration algo-
‚Ä¢ Pseudo-LPA (Pseudo Laplace Mechanism): the base-
line method that injects Laplace perturbation noise to each
count using domain-speciÔ¨Åc global sensitivity ‚àÜf ;
‚Ä¢ Pseudo-GS (Pseudo Grouping&Smoothing Mechanism):
the best method for aggregate release with grouping and
smoothing proposed in [14].

Note that we do not compare with the approach in [24]
since it only supports larger  ( > 1) which our proposed
EpicRec focuses on stronger privacy protection with  ‚â§ 1.
Settings. We conduct the classic recommender system
algorithm, collaborative Ô¨Åltering [25], using GraphLab3. The
only parameter Œ¥ is set to 0.02 according to diÔ¨Äerential pri-
vacy literature. We test the experiments on personal com-
puter which is equipped with 1.9GHz CPU and 8GB RAM.
We run each experiment 10 times and report the average re-
sult. To evaluate recommendation accuracy, we use 10-fold
cross-validation and stochastic gradient descent algorithm
for collaborative Ô¨Åltering. In M-EpicRec case, we randomly
select the privacy levels for each category for each user.
6.2 Evaluation Results

Perturbation Quality. Figure 5 reports the results of
S-EpicRec system. As shown in Figure 5(a), the perturbed
category aggregates quality of S-DPDP algorithm in S-EpicRec
system continuously outperforms other competitors up to
10% in both MovieLens and Yelp datasets. The reason is
that our S-EpicRec determines the calibrated noises based
on the underlying data property via the correlation between

2

3

https://www.kaggle.com/c/yelp-recsys-2013/data
http://select.cs.cmu.edu/code/graphlab/pmf.html

(a) Perturbed Category Aggregate Quality

(b) Recommendation Accuracy

Figure 5: S-EpicRec Results (L:MovieLens; R:Yelp)

categories as described in (4.3), thereby capturing the mini-
mum noise magnitude that is suÔ¨Écient to ensure the privacy
guarantees. In the meanwhile, data sanitization (phase 2 in
Algorithm 1) can also take advantage of the category cor-
relations to minimize the error when sanitizing data. On
the other hand, the grouping and averaging of category ag-
gregates in Pseudo-GS loses a lot of correlation informa-
tion between categories and lead to a larger error. Likewise,
Pseudo-LPA applies the global sensitivity to determine the
noise magnitude, which only captures the maximum number
of categories an item can belong to and largely ignores the
correlation between categories. Moreover, the recommenda-
tion loss in Figure 5(b) is up to 5% and 3% in MovieLens
and Yelp datasets with strong privacy guarantees ( = 0.2
in MovieLens and  = 0.3 in Yelp).

Figure 6 shows the similar performance results in M-EpicRec

system that our M-DPDP algorithm outperforms other com-
petitors from both aspects. The recommendation quality of
M-DPDP algorithm (in Figure 6(b)) has slightly worse than
S-DPDP algorithm in S-EpicRec due to constraints from
items in ‚ÄúNo Release‚Äù categories.

One may question that what if a user has privacy con-
cern on some category rather than particular items in this
category. The bottom two Ô¨Ågures in Figure 6(b) show that
our proposed M-EpicRec framework can indeed provide this
function by allowing user to select ‚Äúno release‚Äù for those cat-
egories. Thanks to the correlation between categories (an
item usually belongs to many categories), we did some addi-
tional experiments showing that the recommendation MAE
loss on movies in ‚Äúno release‚Äù categories is less than 5% worse
than that in ‚Äúperturbed release‚Äù categories.

Privacy Budget QuantiÔ¨Åcation. As we can see in Fig-
ure 5, the blue shadows show the range of quantiÔ¨Åed optimal
privacy budget  spanning in our 10 testings. It is interesting
to see that our quantiÔ¨Åed  values fall in the range of around
 = 0.23 in MovieLens dataset and  = 0.25 in Yelp dataset
respectively, after which the recommendation loss does not
reduce dramatically and maintains relatively stable. Sim-
ilar observations are shown in Figure 6 for category-based
privacy control, with  = 0.29 in MovieLens dataset and
 = 0.33 in Yelp dataset.

Scalability. Figure 7 shows that the running time of
both S-EpicRec and M-EpicRec systems is no longer than
0.7 and 1.5 seconds respectively. It takes slightly longer on
Yelp dataset since the number of public items is relatively

 0 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget ŒµS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 0 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget ŒµS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 0 10 20 30 40 50 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 0 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ189(a) Perturbed Category Aggregate Quality

(b) Recommendation Accuracy (Top: ‚ÄúPerturbed Release‚Äù
Categories; Bottom: ‚ÄúNo Release‚Äù Categories)

Figure 6: M-EpicRec Results(L:MovieLens; R:Yelp)

Figure 7: Running Time (L:MovieLens; R:Yelp)

larger than that on MovieLens dataset. M-EpicRec takes
longer than S-EpicRec due to its more complex optimiza-
tion process (with more constraints). As the perturbation
process is usually conducted oÔ¨Ñine, the running time of our
proposed framework is considered good enough.

7.

IMPLEMENTATION

In this section, we present the implementation of a proof-
of-concept EpicRec system. As shown in Figure 8, we im-
plement a web-based EpicRec client for movie recommen-
dation, incorporated with a standard recommender server
using classic recommendation approaches. The rest of this
section Ô¨Årst brieÔ¨Çy discusses about server side implementa-
tion and then focuses on the implementation of each com-
ponent in EpicRec client. We implement EpicRec client on
a laptop with Ubuntu Desktop OS and the recommender
server on a workstation with Ubuntu Server OS.
7.1 Movie Recommendation Service Provider
In our PoC system, we use a workstation with Ubuntu
Server OS as the recommender system. We conduct the
personalized recommendation using collaborative Ô¨Åltering
(stochastic gradient descent algorithm) via GraphLab3. Rrec-
ommendation results are ranked overall and in each category.
All transmissions between client and server are via SSL/TLS
security protocol.

7.2 EpicRec for Movie Recommendation

On the device side, we maintain a local database to store
the input from public data input (C-1) component and user
private data input (C-2) component. Then, we design and
implement user interfaces for user privacy control (C-3) com-
ponent and read the data from user input.
Public Data Input (C-1): We crawl ‚àº6,000 recent movies‚Äô
meta-data from the public ‚ÄúMy API Films‚Äù website4, includ-
ing movie title, genre, plot description and poster image; we
then store them in the table ‚ÄúallMovies‚Äù in local database.
Moreover, the physical Ô¨Åles of poster images are stored lo-
cally with the corresponding names as in the allMovies table.
Each movie/record in this table is associated with an addi-
tional boolean attribute ‚Äúwatched‚Äù, which is initialized as 0
(indicating that no movies have been watched).

User Private Data Input (C-2): In order to obtain user
private history of watched movies, we implement in a sim-
pliÔ¨Åed manner by scanning the history Ô¨Åles of each web
browser. We mainly focus on two popular web browsers,
Google Chrome and Mozilla Firefox, where we download
the history Ô¨Åles ‚Äú‚àº/.conÔ¨Åg/google-chrome/Default/History‚Äù
and ‚Äú‚àº/.mozilla/Ô¨Årefox/*.default /places.sqlite*‚Äù respectively.
We next search for each movie title in all these history Ô¨Åles
and update the ‚Äúwatched‚Äù attribute to 1 if a movie‚Äôs title
exists in the history Ô¨Åle.

User Privacy Control Input (C-3): We designed the user
interface for user privacy control input (see C-3 in Figure 8),
in which a user is allowed Ô¨Årst to select his overall privacy
concern level from ‚Äúno release‚Äù, ‚Äúperturbed release‚Äù or ‚Äúall
release‚Äù. If ‚Äúperturbed release‚Äù is selected, user can further
select if he wants to select diÔ¨Äerent privacy concern levels
for diÔ¨Äerent categories of movies. If so, a list of categories
is popped up with privacy concern level drop-down boxes.

Privacy QuantiÔ¨Åcation (C-5) & Data Perturbation (C-4):
If a user selects ‚Äúperturbed release‚Äù and does not check the
box to set category-based privacy concern levels, we call C-4
and C-5 components in S-EpicRec system. (When ‚Äúno re-
lease‚Äù or ‚Äúall release‚Äù is selected, we simply release no data or
all raw data.) Otherwise, we call C-4 and C-5 components in
M-EpicRec system to support user speciÔ¨Åed category-based
multiple privacy concern levels.

Recommendation Output (C-6): We simply use a netÔ¨Çix-
style output to provide users overall and per-category top
movie recommendation. Moreover, the categories are ranked
on the client side by the number of movies the user actually
watched to capture user‚Äôs preference on diÔ¨Äerent categories.

8. CONCLUSION AND FUTURE WORK

Conclusion. In this paper, we designed a novel practical
privacy-preserving system on client, EpicRec, for person-
alized recommendation via state-of-the-art diÔ¨Äerential pri-
vacy. EpicRec provides users a privacy control interface
such that users can control their privacy concerns in a way
they understand and of their preferred granularities, either
overall or category-based concerns. EpicRec further quanti-
Ô¨Åes these layman privacy concern levels to privacy budget,
which is next used as input to conduct data perturbation
algorithm via diÔ¨Äerential privacy. With these key compo-
nents, EpicRec can also work with other data collection and
output components. We believe this is an important step

4

http://www.myapiÔ¨Ålms.com/index.jsp

 0 1 2 3 4 5 6 7 8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 0 1 2 3 4 5 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 8 8.5 9 9.5 10 10.5 11 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 4 6 8 10 12 14 16 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 8 9 10 11 12 13 14 15 16 17 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 6 8 10 12 14 16 18 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget ŒµM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal Œµ 300 400 500 600 700 800 900 1000 1100 1200 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Running Time (ms)Privacy Budget ŒµS-EpicRecM-EpicRec 500 1000 1500 2000 2500 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Running Time (ms)Privacy Budget ŒµS-EpicRecM-EpicRec190Figure 8: Proof-of-Concept Implementation of EpicRec for Movie/TV Recommendation

towards designing a practical privacy-preserving system for
personalized recommendation.

Future work. We will extend EpicRec into more com-
prehensive and practical cases from diÔ¨Äerent aspects: (1)
we will improve the implementation of C-1 and C-2 compo-
nents in our PoC system by potentially developing browser
extensions; (2) we will conduct a large-scale Ô¨Åeld study to
observe and understand users‚Äô natural behaviors, in-situ at-
titude and perceptions when using our browser extensions
to interact with EpicRec system; (3) we will continue to de-
velop data perturbation techniques to support user‚Äôs stream-
ing private data; diÔ¨Äerent types of user private data; and al-
low users to iteratively adjust their privacy levels for trading
oÔ¨Ä privacy and recommendation.

9. REFERENCES

[1] L. Bonomi, L. Xiong, and J. J. Lu. Linkit: privacy preserving

record linkage and integration via transformations. In
SIGMOD, pages 1029‚Äì1032, 2013.

[2] S. Boyd and L. Vandenberghe. Convex Optimization.

Cambridge University Press, 2004.

[3] J. Canny. Collaborative Ô¨Åltering with privacy. In IEEE

Symposium on S&P, pages 45‚Äì57, 2002.

[4] T.-H. H. Chan, M. Li, E. Shi, and W. Xu. DiÔ¨Äerentially private
continual monitoring of heavy hitters from distributed streams.
In PETS, pages 140‚Äì159, 2012.

[5] K. Chaudhuri, A. Sarwate, and K. Sinha. Near-optimal

diÔ¨Äerentially private principal components. In NIPS, pages
989‚Äì997. 2012.

[6] K. Chaudhuri and S. A. Vinterbo. A stability-based validation
procedure for diÔ¨Äerentially private machine learning. In NIPS,
pages 2652‚Äì2660. 2013.

[7] C. Dwork. DiÔ¨Äerential privacy: A survey of results. In TAMC,

pages 1‚Äì19, 2008.

[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating

noise to sensitivity in private data analysis. In TCC, pages
265‚Äì284, 2006.

[9] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu.

Privacy-preserving data publishing: A survey of recent
developments. ACM Comput. Surv., 42(4):14:1‚Äì14:53, 2010.

[10] A. Guha Thakurta and A. Smith. (nearly) optimal algorithms

for private online learning in full-information and bandit
settings. In NIPS, pages 2733‚Äì2741. 2013.

[11] M. Hardt, K. Ligett, and F. Mcsherry. A simple and practical

algorithm for diÔ¨Äerentially private data release. In NIPS, pages
2339‚Äì2347. 2012.

[12] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the

accuracy of diÔ¨Äerentially private histograms through
consistency. VLDB, 3(1-2):1021‚Äì1032, 2010.

[13] B. Heitmann, J. G. Kim, A. Passant, C. Hayes, and H.-G. Kim.

An architecture for privacy-enabled user proÔ¨Åle portability on
the web of data. In HetRec, pages 16‚Äì23, 2010.

[14] G. Kellaris and S. Papadopoulos. Practical diÔ¨Äerential privacy
via grouping and smoothing. VLDB, 6(5):301‚Äì312, Mar. 2013.

[15] A. Korolova, K. Kenthapadi, N. Mishra, and A. Ntoulas.

Releasing search queries and clicks privately. In WWW, pages
171‚Äì180, 2009.

[16] S. Kotz, T. J. Kozubowski, and K. Podgorski. The Laplace

distribution and generalizations: a revisit with applications to
communications, economics, engineering, and Ô¨Ånance. 2001.

[17] B. Liu and U. Hengartner. ptwitterrec: A privacy-preserving

personalized tweet recommendation framework. In Proceedings
of ASIA CCS, pages 365‚Äì376, 2014.

[18] A. Machanavajjhala, D. Kifer, J. Abowd, J. Gehrke, and

L. Vilhuber. Privacy: Theory meets practice on the map. In
ICDE, pages 277‚Äì286, 2008.

[19] F. McSherry and I. Mironov. DiÔ¨Äerentially private

recommender systems: Building privacy into the net. In KDD,
pages 627‚Äì636, 2009.

[20] N. Megiddo. Linear programming in linear time when the

dimension is Ô¨Åxed. J. ACM, 31(1):114‚Äì127, Jan. 1984.

[21] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Taft,

and D. Boneh. Privacy-preserving matrix factorization. In CCS,
pages 801‚Äì812, 2013.

[22] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity

and sampling in private data analysis. In STOC, pages 75‚Äì84,
2007.

[23] H. Polat and W. Du. Privacy-preserving collaborative Ô¨Åltering

using randomized perturbation techniques. In ICDM, pages
625‚Äì628, 2003.

[24] Y. Shen and H. Jin. Privacy-preserving personalized

recommendation: An instance-based approach via diÔ¨Äerential
privacy. In ICDM, pages 540‚Äì549, 2014.

[25] P. Symeonidis, A. Nanopoulos, A. N. Papadopoulos, and

Y. Manolopoulos. Collaborative recommender systems:
Combining eÔ¨Äectiveness and eÔ¨Éciency. Expert Syst. Appl.,
34(4):2995‚Äì3013, 2008.

[26] J. Wang, N. Wang, and H. Jin. Context matters?: How adding

the obfuscation option aÔ¨Äects end users‚Äô data disclosure
decisions. In IUI, pages 299‚Äì304, 2016.

[27] Y. Xin and T. Jaakkola. Controlling privacy in recommender

systems. In NIPS, pages 2618‚Äì2626. 2014.

[28] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. DiÔ¨Äerentially

private histogram publication. In ICDE, pages 32‚Äì43, 2012.
[29] B. Zhang, N. Wang, and H. Jin. Privacy concerns in online
recommender systems: InÔ¨Çuences of control and user data
input. In SOUPS, pages 159‚Äì173, 2014.

[30] S. Zhang, J. Ford, and F. Makedon. Deriving private

information from randomly perturbed ratings. In SDM, pages
59‚Äì69, 2006.

Local DatabaseC-1. Public Data InputC-2. User Private Data InputHistory on user‚Äôs device from various resourcesC-3. User Privacy Control InputImplementation of Data Perturbation Component C-4Recommendation Service Provider using collaborative filtering algorithm in GraphLabNetflix-like Recommendation Output to Client C-6EpicRecon Device for Movie RecommendationImplementation of Privacy Quantification Component C-5SSL/TLSSSL/TLSPrivacyBudget191