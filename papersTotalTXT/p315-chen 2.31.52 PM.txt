Network Performance of Smart Mobile Handhelds in a

University Campus WiFi Network

Xian Chen, Ruofan Jin
University of Connecticut

Kyoungwon Suh

Illinois State University

Bing Wang, Wei Wei
University of Connecticut

ABSTRACT
Smart mobile handheld devices (MHDs) such as smartphones
have been used for a wide range of applications. Despite the
recent Ô¨Çurry of research on various aspects of smart MHDs,
little is known about their network performance in WiFi net-
works. In this paper, we measure the network performance
of smart MHDs inside a university campus WiFi network,
and identify the dominant factors that aÔ¨Äect the network
performance. SpeciÔ¨Åcally, we analyze 2.9TB of data col-
lected over three days by a monitor that is located at a gate-
way router of the network, and make the following Ô¨Åndings:
(1) Compared to non-handheld devices (NHDs), MHDs use
well provisioned Akamai and Google servers more heavily,
which boosts the overall network performance of MHDs.
Furthermore, MHD Ô¨Çows, particularly short Ô¨Çows, bene-
Ô¨Åt from the large initial congestion window that has been
adopted by Akamai and Google servers.
(2) MHDs tend
to have larger local delays inside the WiFi network and are
more adversely aÔ¨Äected by the number of concurrent Ô¨Çows.
(3) Earlier versions of Android OS (before 4.X) cannot take
advantage of the large initial congestion window adopted by
many servers. On the other hand, the large receive window
adopted by iOS is not fully utilized by most Ô¨Çows, poten-
tially leading to waste of resources. (4) Some application-
level protocols cause ineÔ¨Écient use of network and operat-
ing system resources of MHDs in WiFi networks. Our ob-
servations provide valuable insights on content distribution,
server provisioning, MHD system design, and application-
level protocol design.

Categories and Subject Descriptors
D.4.8 [Performance]: Measurements

General Terms
Performance, Measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
IMC‚Äô12, November 14‚Äì16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords
Smart Mobile Handhelds, WiFi Networks, Performance Mea-
surement

1.

INTRODUCTION

Smart mobile handheld devices (MHDs) such as smart-
phones have been used for a wide range of applications in-
cluding surÔ¨Ång web, checking email, watching video, ac-
cessing social networking services, and online games. Most
MHDs are equipped with both cellular and WiFi interface
cards. Whenever available, WiFi is still a preferred way for
Internet connection due to its higher bandwidth, lower delay
and lower energy consumption [9]. Although recently there
is a Ô¨Çurry of studies on various aspects of smart MHDs (see
Section 2), little is known about the network performance
of smart MHDs in WiFi networks. SpeciÔ¨Åcally, how is their
network performance? What are the major factors that af-
fect the network performance?

In this paper, we answer the above questions by measuring
the performance of smart MHDs inside a university campus
WiFi network. SpeciÔ¨Åcally, our study is over a data set
that is passively captured by a monitor placed at a gateway
router in the University of Connecticut (UConn). The data
set is collected over three days (2.9TB of data), containing
traÔ¨Éc from various wireless devices, including MHDs such
as iPhones, iPod touches, iPads, Android phones, Windows
phones, and Blackberry phones, and wireless non-handheld
devices (NHDs) such as Windows laptops and MacBooks.
To understand the performance of MHDs, we Ô¨Årst separate
the traÔ¨Éc of MHDs from that of NHDs. Our focus is on
MHDs; we describe the results on NHDs when necessary for
comparison.

Analyzing the data set, we Ô¨Ånd HTTP is the dominant
traÔ¨Éc type, accounting for over 92% of the TCP Ô¨Çows. We
therefore focus on the performance of HTTP Ô¨Çows in this pa-
per. The behavior of HTTP is complicated: we Ô¨Ånd many
HTTP Ô¨Çows contain multiple HTTP requests, and a sig-
niÔ¨Åcant portion of an HTTP Ô¨Çow is idling (with no data
packets). Hence, the traditional throughput metric (i.e., the
amount of data downloaded in a Ô¨Çow divided by the total
duration of the Ô¨Çow) may introduce bias in measuring the
performance of HTTP Ô¨Çows. We therefore deÔ¨Åne a metric,
per-Ô¨Çow servicing rate, i.e., the amount of data downloaded
corresponding to HTTP requests in a Ô¨Çow divided by the
downloading duration of the Ô¨Çow, to quantify the perfor-
mance of an HTTP Ô¨Çow (see Section 4). This metric is
interesting in its own right:
it represents the network per-
formance of an HTTP Ô¨Çow, while excluding the eÔ¨Äect of

315various delays (e.g., client processing delays and user pause
times) that are irrelevant to network performance. Our main
Ô¨Åndings are as follows.

‚Ä¢ We observe that the best performance is achieved for
the MHD Ô¨Çows served by an Akamai server cluster
that is located close to UConn. Furthermore, 16% of
the MHD Ô¨Çows are served by this server cluster, ac-
counting for 35% of the bytes. Overall, a large per-
centage (38.4%) of MHD Ô¨Çows are served by well pro-
visioned Akamai and Google servers, account for 62%
of the bytes. Interestingly, these fractions are signiÔ¨Å-
cantly larger than the corresponding values for NHDs,
indicating that MHDs use Akamai and Google servers
more heavily, which boosts their overall network per-
formance. We also observe that Akamai and Google
servers have adopted large initial congestion window,
which contributes to the network performance of MHDs
(particularly for short Ô¨Çows). On the other hand, we
observe that most Ô¨Çows have low loss rates, indicating
that loss rate is not a limiting factor in the campus
WiFi network that we study.

‚Ä¢ We Ô¨Ånd that MHDs tend to have longer local RTTs
(i.e., delays within the university network) than NHDs.
In addition, the number of concurrent Ô¨Çows has nega-
tive eÔ¨Äect on performance, and the eÔ¨Äect is more sig-
niÔ¨Åcant for MHDs than NHDs. These two diÔ¨Äerences
between MHDs and NHDs might be caused by the infe-
rior computation and I/O capabilities on MHDs. The
eÔ¨Äect of local RTT on network performance seems neg-
ligible due the fact that local RTT only takes a small
portion of the RTT.

‚Ä¢ We Ô¨Ånd that earlier versions of Android OS (before
4.X) cannot take advantage of the large initial conges-
tion window adopted by many servers. While Android
OS increases the receive window adaptively in every
round trip time, it uses a very small initial receive win-
dow (much smaller than the initial congestion window
adopted by Google and Akamai servers), which limits
the performance of Andriod devices. In contrast, we
observe that iOS uses a large static receive window,
which fully exploits the beneÔ¨Åt of large initial conges-
tion window. On the other hand, most Ô¨Çows do not
fully utilize the large receive window, potentially lead-
ing to unnecessary waste of resources on iOS devices.

‚Ä¢ We Ô¨Ånd that some application-level protocols cause in-
eÔ¨Écient use of network and operating system resources
of MHDs in WiFi networks. One example is the native
YouTube application on iOS devices, which can use a
large number of TCP Ô¨Çows to serve a single video. We
suspect this is a design optimization for cellular net-
works, which is not suitable for WiFi networks.

Our Ô¨Åndings highlight the impact of TCP parameters and
application-level design choices on MHD network perfor-
mance, providing valuable insights on content distribution,
server provisioning, mobile system design, and application-
level protocol design.

The rest of the paper is organized as follows. Section 2 de-
scribes related work. Section 3 describes data collection and
classiÔ¨Åcation. Section 4 introduces the performance met-
ric. Section 5 describes our methodology. Section 6 presents

network performance of MHDs, and explores the impact of
network and application layer factors on the performance.
Section 7 discusses what Ô¨Åndings are speciÔ¨Åc to UConn WiFi
network and what can be applied to other networks. Last,
Section 8 concludes the paper and presents future research
directions.

2. RELATED WORK

Several recent studies characterize the usage and traÔ¨Écs
of MHDs in 3G cellular networks, public WiFi or residen-
tial WiFi networks. Trestian et al. analyze a trace collected
from the content billing system of a large 3G mobile service
provider to understand the relationship among people, loca-
tions and interests in 3G mobile networks [31]. Maier et al.
study packet traces collected from more than 20,000 residen-
tial DSL customers to characterize the traÔ¨Éc from MHDs in
home WiFi networks [21]. Falaki et al. employ passive snif-
fers on individual devices (Android and Windows Mobile
smartphones) to record sent and received traÔ¨Éc, and pro-
vide a detailed look at smartphone traÔ¨Éc [11]. This study is
limited to a small user population (43 users), and the traces
were not separately analyzed for the diÔ¨Äerent network inter-
faces (i.e., 3G and WiFi) being used, which have diÔ¨Äerent
properties. The study in [12] uses a larger population of
255 users, and characterizes intentional user activities and
the impact of these activities on network and energy usage.
Gember et al. compare the content and Ô¨Çow characteristics
of MHDs and NHDs in campus WiFi networks [14]. We
focus on network performance of MHDs in a campus WiFi
network and the impact of various factors on the network
performance of MHDs, which are not investigated in the
above studies.

Huang et al. anatomize the performance of smartphone
applications in 3G networks using their widely-deployed ac-
tive measurement tool, 3GTest [16]. Tso et al. study the
performance of mobile HSPA (a 3.5G cellular standard) net-
works in Hong Kong using extensive Ô¨Åeld tests [32]. Our
study diÔ¨Äers from them in that we study the network per-
formance of MHDs in WiFi networks (instead of cellular
networks). Furthermore, our study is based on large-scale
traces collected passively from a university campus network,
while the study in [16] adopts an active measurement tool
and the study in [32] uses customized applications. The
studies of [13, 26] report that iOS native YouTube player
generates multiple TCP Ô¨Çows to stream a single video. We
expand these studies by presenting the degree of ineÔ¨Écien-
cies caused by the large number of TCP Ô¨Çows and the main
reason for the design choice in the player.

Several measurement studies are on university WiFi net-
works. However, MHDs have only been widely adopted re-
cently, and none of those studies explores the network per-
formance of MHDs as in our study. Our study builds upon
the rich literature on understanding the behavior of TCPs
and content distribution in operational environments (e.g.,
[19, 23, 20, 15, 30, 10, 4, 22]), and our observations conÔ¨Årm
some of the Ô¨Åndings in those studies.

Last, several other aspects of MHDs have been studied re-
cently, including battery use and charging behaviors [6, 25],
energy consumption [29, 5], and performance enhancement
techniques (e.g., [24, 34]). Our study diÔ¨Äers in scope from
them.

3163. DATA COLLECTION & CLASSIFICATION

We collect measurements at a monitoring point inside the
University of Connecticut (UConn). The monitor is a com-
modity PC, equipped with a DAG card [2]. It is placed at a
gateway router of UConn, capturing all incoming and outgo-
ing packets through the router with accurate timestamps1.
In particular, it captures up to 900 bytes of each packet, in-
cluding application-level, TCP and IP headers. The campus
network uses separate IP pools for Ethernet and Wireless
LAN (WLAN). Since we are interested in the network per-
formance of MHDs, which use the WLAN IP address pool,
we use Ô¨Ålters to capture only WLAN traÔ¨Éc.

We have collected two data sets. One data set is for three
days, from 9am March 2 to 9am March 5, 2011. The other
data set is for one day, from 9am April 23 to 9am April
24, 2012. Unless otherwise stated, we use the Ô¨Årst data set
(reasons for focusing on this data set and Ô¨Åndings from the
second data set are deferred to Section 6.9). In the follow-
ing, we Ô¨Årst provide a high-level view of the data, and then
present methodology to separate MHD traÔ¨Éc from NHD
traÔ¨Éc (the captured data set contains a mixture of traÔ¨Éc
from both types of devices).

3.1 Data

Table 1 lists the number of packets captured during the
three days. Overall, we collected over 5.8G packets (2.9 TB
of data). Among them, 91.9% of the packets are carried
by TCP. We only report the results obtained from the data
collected on the Ô¨Årst day; the statistical characteristics of
the data on the other two days are similar2.

Table 1: The number of packets captured during the
three days.

incoming pkts
outgoing pkts

total pkts

% TCP pkts

Day 1 Day 2 Day 3 Overall
1.2G
3.5G
0.8G
2.3G
2.0G
5.8G
91.8% 92.7% 90.8% 91.9%

0.8G
0.5G
1.4G

1.4G
1.0G
2.4G

We say a TCP Ô¨Çow is valid if it Ô¨Ånishes three-way hand-
shaking and does not contain a RESET packet3. Among the
valid TCP Ô¨Çows, we identify the applications using destina-
tion port numbers (a recent study shows that this simple
approach provides accurate results [20]). Table 2 lists the
most commonly used applications, and the percentages of
their traÔ¨Éc over all TCP traÔ¨Éc in terms of Ô¨Çows, packets
and bytes, where the number of bytes in a packet is obtained
from the IP header. We observe a predominant percentage
(92.3%) of the TCP Ô¨Çows are HTTP Ô¨Çows. This is consistent
with measurement results in other campus networks [14] and
home environments [20]. In the rest of the paper, we focus on
the network performance of HTTP Ô¨Çows; the performance
of other protocols is left as future work.

1The campus network balances loads among two gateway
routers. The load balancing strategy is set up so that data
packets and ACKs in a TCP Ô¨Çow are through the same
router.
2The amount of traÔ¨Éc on Day 3 is less than that of the other
two days since Day 3 is a Friday.
311.9% of the Ô¨Çows cotain a RESET packet.

Table 2: Percentage of the traÔ¨Éc from commonly
used applications (in terms of Ô¨Çows, packets and
bytes) over all the TCP traÔ¨Éc (Day 1).

Application
HTTP (80)

HTTPS (443)
IMAPS (993)

Ô¨Çows
92.3%
4.3%
0.2%

packets

bytes
86.7%
5.4%
0.28% 0.13%

82%
8%

3.2 Data classiÔ¨Åcation

We refer to an HTTP Ô¨Çow that contains packets coming
from and/or destinating to an MHD as an MHD Ô¨Çow (sim-
ilar deÔ¨Ånition for an NHD Ô¨Çow). Since our captured data
contains a mixture of MHD and NHD Ô¨Çows, we need to sep-
arate these two types of Ô¨Çows. The Ô¨Årst approach we use to
diÔ¨Äerentiate MHD and NHD Ô¨Çows is based on the keywords
in the user-agent Ô¨Åeld in HTTP headers (MHDs and NHDs
use diÔ¨Äerent keywords) [21, 14]. Once the type of a Ô¨Çow
is identiÔ¨Åed using the user-agent Ô¨Åeld, it provides us infor-
mation on the type of the associated device (i.e., whether
it is an MHD or NHD), which can be used to further iden-
tify the types of other Ô¨Çows. SpeciÔ¨Åcally, if we know that
a Ô¨Çow, f , from a device (the device is represented using an
IP address) is an MHD Ô¨Çow, then we know all the Ô¨Çows
that are concurrent with f and have the same source IP ad-
dress as f are MHD Ô¨Çows4. If the type of an HTTP Ô¨Çow
is not identiÔ¨Åed using the above two approaches, we use the
knowledge that an IP address pool is dedicated to Apple
mobile devices at UConn5, and hence a Ô¨Çow using an IP
address from this pool is an MHD Ô¨Çow. Using the above
three approaches, we categorize 94.1% of the HTTP Ô¨Çows
to be either MHD or NHD Ô¨Çows (91.9% of them are directly
identiÔ¨Åed through the user-agent Ô¨Åeld, 6.7% are identiÔ¨Åed
through the concurrent-Ô¨Çow approach, and the rest 1.4% are
identiÔ¨Åed using the knowledge of the dedicated IP address
pool). SpeciÔ¨Åcally, we identify 0.7M MHD Ô¨Çows and 10.4M
NHD Ô¨Çows, containing 38.8M and 821.1M packets, respec-
tively. Further separation of the Ô¨Çows using other heuristics
(e.g., TTL [21]) is left as future work.

We further identify the operating systems used by the
MHDs and NHDs using the user-agent Ô¨Åeld. For MHDs,
the dominant operating system is iOS, used in 96.2% of the
MHD Ô¨Çows (iOS is used by iPhone, iPod touch, and iPad,
with the percentages of Ô¨Çows as 65%, 26%, and 9%, respec-
tively); and Android is the second most popular operating
system, used in 3.2% of the MHD Ô¨Çows. For NHDs, the two
dominant operating systems are OS X and Windows, taking
56.7% and 42.2% of the NHD Ô¨Çows, respectively; and Linux
is at the third place, used by 1.1% of the NHD Ô¨Çows.

4. PERFORMANCE METRIC

As mentioned earlier, we mainly characterize the perfor-

4We can only classify the Ô¨Çows that are concurrent with f
due to IP reassignment which can assign the same IP address
to another device at another point of time.
5UConn network administrators set aside this dedicated
pool to ease the management of IP addresses. SpeciÔ¨Åcally,
a device is assigned an IP address from this dedicated pool
if its host name indicates that it is an Apple mobile device
during the DHCP request phase.

317mance of HTTP Ô¨Çows since HTTP is the predominant traÔ¨Éc
type. The behavior of HTTP is complicated. For instance,
modern browsers often open multiple TCP connections when
browsing a web page [10]. Furthermore, an HTTP Ô¨Çow can
send and receive multiple HTTP requests/responses.
In
our passive measurements, without any knowledge of the
accessed content, it is diÔ¨Écult to correlate multiple TCP
connections. We therefore focus on per-Ô¨Çow network perfor-
mance, speciÔ¨Åcally, the performance that is the result of the
complex interactions of myriad network and application re-
lated factors. In the following, we Ô¨Årst describe our measure-
ment results on HTTP Ô¨Çows, and then deÔ¨Åne a performance
metric that we will use in the rest of the paper.

F
D
C

1.00

0.95

0.90

0.85

0.80

0.75

1

 MHD
 NHD

10

100

1000

Number of HTTP Requests

Figure 1: Distribution of the number of HTTP GET
requests in an HTTP Ô¨Çow.







t

(cid:2)

1

t1

tA



tS


	

  

	

  


   
 
  


tn

tF



(cid:2)

t

n

te

Figure 2: Illustration of a persistent HTTP Ô¨Çow.

We Ô¨Ånd that over 99% of the HTTP Ô¨Çows use HTTP 1.1,
where TCP connections are persistent, i.e., each TCP con-
nection allows multiple GET and POST commands. On the
other hand, we observe only 6.5% of the HTTP Ô¨Çows contain
POST commands. This is not surprising since most of the
users in the campus network are content consumers. In the
following, we ignore the HTTP Ô¨Çows with POST commands.
Fig. 1 plots the distribution of the number of HTTP GET
requests within an HTTP Ô¨Çow. Observe that over 20% of
the HTTP Ô¨Çows contain at least two requests. The number

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

F
D
C

 MHD,Down.
 NHD,Down.
 MHD,Total.
 NHD,Total.

 MHD

 NHD

0.01

0.1

1

10

100

1000

0.0

1E-3

0.01

0.1

1

Duration (sec)

Down. Duration / Total Duration

(a)

(b)

Figure 3: (a) Total and downloading durations of
MHD and NHD Ô¨Çows. (b) The ratio of downloading
duration over total duration.

of requests in a Ô¨Çow can be as large as 100. In addition, NHD
Ô¨Çows tend to contain a larger number of requests than MHD
Ô¨Çows. This might be because regular web pages accessed by
NHDs contain more objects than their mobile versions that
are often accessed by MHDs [33].

Fig. 2 illustrates a persistent HTTP Ô¨Çow with n requests
(the requests are sequential since we do not observe any
pipelined requests in our trace where a request is sent out
without waiting for the response of the previous request6).
The measurement point sits between the client and server,
capturing the packets with accurate timestamps. In partic-
ular, let tS denote the timestamp of the SYN packet, tA
denote the timestamp of the ACK packet in the three-way
handshaking, ti denote the timestamp of the ith HTTP re-
quest, t(cid:2)
i denote the timestamp of the last data packet corre-
sponding to the ith HTTP request, tF denote the timestamp
of the Ô¨Årst FIN packet, and te denote the timestamp indi-
cating the end of a TCP Ô¨Çow. Then the measured duration
of the HTTP Ô¨Çow at the measurement point is te ‚àí tS, while
i ‚àí ti). We refer
the duration for data downloading is
to the former as total duration and the latter as download-
ing duration. Fig. 3(a) plots the distributions of the total
and downloading durations for MHD and NHD Ô¨Çows. We
observe that total durations can be signiÔ¨Åcantly longer than
downloading durations: the median downloading durations
are 0.63s and 0.93s for MHD and NHD Ô¨Çows, respectively,
while the median total durations are 5.06s and 15.45s for
MHD and NHD Ô¨Çows, respectively. As shown in Fig. 3(b),
the downloading duration is less than 10% of the total du-
ration for respectively 60% and 42% of the MHD and NHD
Ô¨Çows.

i=1(t(cid:2)

(cid:2)n

The diÔ¨Äerence between the downloading and total dura-
tion contains various delays inside an HTTP Ô¨Çow. Parts
of the delays are for the three-way handshaking and TCP
connection termination, and the rest of the delays are ap-
plication idle/processing time. We divide the application
idle/processing time into three parts. The Ô¨Årst is the delay
from Ô¨Ånishing the three-way handshaking to the Ô¨Årst GET
request, deÔ¨Åned as TS = t1 ‚àí tA. The second is the sum
of the delays from Ô¨Ånishing one HTTP request to starting
the next HTTP request, i.e., TG =
i), and the
last is the delay from Ô¨Ånishing downloading the last object to
start to close the TCP connection, i.e., TF = tF ‚àí t(cid:2)
n. Fig. 4

i=1 (ti+1 ‚àí t(cid:2)

(cid:2)n‚àí1

6HTTP pipeline is not recommended and disabled in most
browsers [3].

3181.0

0.8

0.6

0.4

0.2

0.0

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

F
D
C

 TS
 TF
 TG

 TS
 TF
 TG

0.01

0.1

1

10

100

0.01

0.1

1

10

100

Delay (sec)

(a) MHD

Delay (sec)

(b) NHD

Figure 4: Distribution of the various delays in an
HTTP Ô¨Çow.

1.0

0.8

0.6

0.4

0.2

F
D
C

1.0

0.8

0.6

0.4

0.2

F
D
C

 TS
 TF
 TG

0.0

1E-3

0.01

0.1

1

0.0

0.01

0.1

 TS
 TF
 TG

1

Delay / Total Duration

Delay / Total Duration

(a) MHD

(b) NHD

Figure 5: Distribution of the ratios of the various
delay over the total duration in an HTTP Ô¨Çow.

plots the distributions of the various delays. We observe
that TS is small for both MHD and NHD Ô¨Çows. Although a
large fraction of the TF values is small, it can be as large as
several seconds for MHD Ô¨Çows and tens of seconds for NHD
Ô¨Çows. The values of TG are in a wide range from millisec-
onds to tens of seconds. Fig. 5 plots the CDF of the ratios
of the various delay over the total duration.

In summary, the drastic diÔ¨Äerence between total dura-
tion and downloading duration highlights the importance of
deÔ¨Åning performance metric carefully: a traditional through-
put metric, deÔ¨Åned as the total amount of data downloaded
divided by the total duration, can lead to biased results on
the performance of HTTP Ô¨Çows. To exclude the eÔ¨Äects of
application-level delays on network performance, we deÔ¨Åne
a performance metric, per-Ô¨Çow servicing rate, i.e., the total
number of data bytes that are downloaded corresponding
to HTTP requests divided by the downloading duration, to
represent network performance. This metric represents the
rate at which data are being fetched in an HTTP Ô¨Çow from a
server to a client, while excluding the eÔ¨Äect of various delays
(e.g., client processing delays and user pause times) that are
irrelevant to network performance.

5. METHODOLOGY

In the traces that we collected, MHDs are typically clients,
requesting contents from servers outside UConn campus net-
work. To understand the performance of MHD Ô¨Çows, we
Ô¨Årst group them according to their destinations. The ratio-
nale is that since the clients (i.e., MHDs) are at the same
geographic location, the destinations (corresponding to the
content servers) directly determine the network path. For

the Ô¨Çows that are served by the same group of servers, we
further divide the Ô¨Çows according to their lengths (in terms
of number of packets inside the Ô¨Çow), since Ô¨Çow length af-
fects the impact of the various TCP parameters and network
path conditions on servicing rate. SpeciÔ¨Åcally, short Ô¨Çows
may terminate before Ô¨Ånishing the slow-start phase in TCP,
while long Ô¨Çows are more likely to reach congestion avoid-
ance phase, and hence achieve more steady throughput.

In the following, we Ô¨Årst describe server and Ô¨Çow classiÔ¨Å-
cation, and then describe how we measure the various TCP
and application-level characteristics.

5.1 Server ClassiÔ¨Åcation

We use two steps to study each Ô¨Çow destination IP ad-
dress. First, we use reverse DNS lookup to Ô¨Ånd its domain
name, and then query the IP registration information to de-
termine the organization that registered the IP address. For
the IP addresses that do not have a published domain name,
we use the registration information only.

Table 3: Top Ô¨Åve domain names for MHD and NHD
Ô¨Çows (percentage is in terms of Ô¨Çows).

MHD
Akamai.com (26.1%)
Google.com (12.3%)
Facebook.com (10.0%) Facebook.com (8.3%)
Amazon.com (6.8%)
Apple.com (5.0%)

NHD
Akamai.com (20.0%)
Google.com (8.9%)

Yahoo.com (5.9%)
Amazon.com (5.3%)

Table 3 presents the top Ô¨Åve domain names of MHD Ô¨Çows
that are obtained using reverse DNS lookup. We see that the
largest percentage of MHD Ô¨Çows are served by Akamai CDN.
Indeed, many widely used applications on MHDs are served
by Akamai. For instance, when an iOS device downloads
an application from Apple App Store, the data are actually
downloaded from Akamai. In addition, some popular desti-
nations, e.g., ‚Äúfbcdn.net‚Äù, are served by Akamai (we Ô¨Ånd 12%
of MHD Ô¨Çows are destined to ‚Äúfbcdn.net‚Äù). Following Aka-
mai.com, Google.com is the second most frequently accessed
domain name. This is not surprising: Google.com is one of
the most frequently accessed web sites [1], and many MHDs
have embedded Google applications (e.g., Google Map, Google
Voice, Gmail). The third one is Facebook.com, which uses
Akamai to serve many static contents, and use its own servers
to serve many dynamic contents directly. The fourth one is
Amazon.com, whose cloud services serve many popular ap-
plications running on iOS (e.g., Foursquare).

Since Akamai.com and Google.com are the top two do-
main names, we detail the network performance of the MHD
Ô¨Çows that are served by Akamai and Google servers, as well
as those served by the rest of the servers in Section 6. We
also observe from Table 3 that 38.4% of MHD Ô¨Çows are
served by Akamai CDN and Google servers. This percentage
is signiÔ¨Åcantly larger than the corresponding value for NHD
Ô¨Çows (i.e., 28.9%). We believe the reason why MHD Ô¨Çows
use Akamai and Google servers more heavily is that the des-
tination hosts accessed by MHDs are less diverse than those
accessed by NHDs [14]. Based on the host Ô¨Åeld in the HTTP
request headers, we Ô¨Ånd around 6k distinct destination host
domains from MHDs and 51k distinct destination host do-
In addition, Table 4 lists the top 10
mains from NHDs.

319destination host domains accessed by MHD and NHD Ô¨Çows.
We see for MHDs, accesses to the top ten host domains ac-
count for 39.2% of the MHD Ô¨Çows, while the percentage
for NHDs (32%) is much lower. Last, we also observe from
Table 4 that for MHDs, except for facebook.com and pan-
dora.com, the other top eight host domains are all served by
Akamai and Google, conÔ¨Årming the heavy usage of Akamai
and Google servers by MHDs.

Table 4: Top ten destination host domains for MHD
and NHD Ô¨Çows (percentage is in terms of Ô¨Çows).

MHD
fbcdn.net (12.0%)
facebook.com (10%)
apple.com (3.9%)
googlevideo.com (3.8%)
google.com (2.5%)
admob.com (1.6%)
doubleclick.net (1.5%)
youtube.com (1.4%)
google-analytics.com (1.3%)
pandora.com (1.2%)

NHD
fbcdn.net (12.9%)
facebook.com (6.5%)
google.com (2.4%)
doubleclick.net (2.0%)
ytimg.com (1.7%)
quantservec.om (1.6%)
yieldmanager.com (1.5%)
tumblr.com (1.2%)
twitter.com (1.1%)
yahoo.com (1.1%)

5.2 Flow Length ClassiÔ¨Åcation

For the Ô¨Çows that are served by the same server category,
we divide the Ô¨Çows according to their lengths into three
groups, denoted as G1, G2, and G3. SpeciÔ¨Åcally, G1 con-
tains short Ô¨Çows, with one to ten data packets, G2 and G3
contain longer Ô¨Çows, with at least 10 and 50 data packets,
respectively. We make the above grouping since many web
pages belong to G1 [10], while long video streaming sessions
7. Table 5 lists the number of Ô¨Çows, pack-
may belong to G3
ets and bytes (calculated from packet size in IP header) of
the various groups. Observe that around 75% of the Ô¨Çows
are short G1 Ô¨Çows.

Table 5: Information of the various groups of MHD
Ô¨Çows (for the data collected on Day 1).

Ô¨Çows

packets
bytes

G3

G2
0.2M

G1
0.5M
0.03M
2.9M 25.1M 19.0M
2.3GB 22.8GB 19.4GB

5.3 TCP Ô¨Çow characteristics

We now describe how we estimate the various TCP Ô¨Çow
characteristics, including RTT, loss rate, local RTT and loss
rate, server‚Äôs initial congestion window, client‚Äôs advertised
receive window, and the maximum window size.

We use the techniques in [19] to obtain a sequence of RTT
samples for a Ô¨Çow, and use the median to represent the Ô¨Çow‚Äôs
RTT. Packet losses are detected when observing triple dupli-

cate ACKs, or a retransmission that is caused by timeout8.
Local RTT represents the delay that a TCP Ô¨Çow experiences
locally, inside a local-area network (i.e., the UConn campus
network in our context). Similarly, local loss rate represents
the loss rate that a TCP Ô¨Çow experiences inside a local-area
network. In our study, both local RTT and local loss rate
can be directly obtained from the measurements at the mon-
itoring point since it is at the edge of the local-area network
(see details in [7]).

The server‚Äôs initial congestion window size (icwnd) has a
signiÔ¨Åcant impact on the Ô¨Çow downloading rate. It deter-
mines the amount of data that the server can send in the
Ô¨Årst window. If the icwnd is too small, the sender needs to
pause and waits for ACKs before transmitting again; on the
other hand, if it is too large, the sender pushes too much
data into the network, and congestion could happen [10].
We infer the icwnd based on the packet arrival times at
the monitoring point. SpeciÔ¨Åcally, we obtain an estimate of
the icwnd as n when observing the Ô¨Årst window of n pack-
ets arriving close in time at the the monitoring point. The
Ô¨Årst window is dynamically estimated based on RTTs using
the technique in [19]. This approach will underestimate the
icwnd if the application does not have suÔ¨Écient amount of
data to send in the Ô¨Årst window. To avoid potential under-
estimation, we only apply the approach to HTTP Ô¨Çows that
contain at least one GET request and the data correspond-
ing to the Ô¨Årst GET request contain at least 20 packets. The
constraint of at least 20 data packets (corresponding to at
least 20KB when each packet is at least 1000 bytes) is based
on the literature that servers can choose a large icwnd, e.g.,
between 10KB to 20KB [10, 23]. To obtain a server‚Äôs icwnd,
we infer a series of icwnd values from all of the Ô¨Çows that
are served by this server and satisfy the requirements stated
earlier, and then obtain the average, median, and maximum
from these inferred values (our measurements indicate that
a server‚Äôs icwnd can vary among the Ô¨Çows). In Section 6.4,
we only report the results from servers that have at least 10
estimates.

Client‚Äôs advertised receive window, constantly reported
from the client to the server, states the size of the available
receive buÔ¨Äer, i.e., the maximum size of data that a sender
transmits to the receiver before the data is acknowledged by
the receiver. The receive window Ô¨Åeld in the TCP header
is 16 bits, limiting the maximum size of receive window to
64KB. However, if both TCP server and client support win-
dow scaling option, they can agree upon a window scaling
factor, which is the multiplier to the 16-bit receive window
Ô¨Åeld. For example, if window scaling factor is 4, the maxi-
mum receive window that a client can specify is 256KB.

At each time point, the server‚Äôs available window is the
minimum of the client‚Äôs advertised receive window and server
congestion window. We also measure the maximum window
of each Ô¨Çow during its lifetime. The maximum window has
impact on the network performance as well:
it determines
the maximum amount of data that the sender can transmit
in one RTT, which aÔ¨Äects the maximum downloading rate.

5.4 Application layer characteristics

We consider the following three application-level charac-
teristics. The Ô¨Årst is the design of application-level proto-

7We also consider another group, G4, which contains Ô¨Çows
with at least 100 packets. The results of G4 are similar to
those of G3, and hence are not reported in this paper.

8These are inferred losses at the TCP level. The loss rate
thus estimated is an overestimate since the inferred losses
may be due to long delays, not actual losses.

320cols.
In particular, we consider the protocol for YouTube
videos. This is motivated by the popularity of this applica-
tion and the large amount of video data in our trace (38%
of the data are video contents). Secondly, we examine how
servers respond to requests for diÔ¨Äerent types of contents
(e.g., video, image and texts). For this purpose, we deÔ¨Åne
application response time as the delay from the Ô¨Årst GET
message to the Ô¨Årst responding data packet for an HTTP
request (the delay is measured at the monitoring point).
Last, we quantify the relationship between the number of
concurrent TCP Ô¨Çows and per-Ô¨Çow servicing rate. The ra-
tionale behind this is that the number of concurrent TCP
Ô¨Çows might be an indicator of the amount of CPU and I/O
activities on an MHD device. We calculate the amount of
concurrent TCP Ô¨Çows for a Ô¨Çow as follows. Consider a Ô¨Çow
f . Since the number of its concurrent Ô¨Çows can vary over
time, we divide time into 0.3s bins, and count the number
of concurrent Ô¨Çows in each bin, and then obtain the average
number of concurrent Ô¨Çows during f ‚Äôs life-time as its num-
ber of concurrent Ô¨Çows. We only consider concurrent Ô¨Çows
for long Ô¨Çows, speciÔ¨Åcally, the Ô¨Çows in G3.

6. NETWORK PERFORMANCE OF MHDS

AND RELATED FACTORS

In this section, we Ô¨Årst present network performance of
MHDs and then investigate how network and application
layer factors aÔ¨Äect the performance. Unless otherwise stated,
we mainly report the results of iOS devices (i.e., iPhone,
iPod touch, iPad) that account for 96.2% of the MHD Ô¨Çows.
As shown in Table 4, a large percentage (38.4%) of MHD
Ô¨Çows are served by Akamai and Google servers. Using a
commercial database [17], we identify that the Akamai servers
are located in four main clusters, respectively 40km, 113km,
517km, and 966km away from UConn. Furthermore, consis-
tent with the Akamai DNS design policy that gets servers
close to end users [15], we Ô¨Ånd 90% of the MHD Akamai
Ô¨Çows are served by the Ô¨Årst two close-by server clusters. IP
registration information reveals that all Akamai servers in
the 40km range belong to the University of Hartford (only
one hop away from UConn network), while it does not reveal
any detailed information for the servers in the other distance
ranges. We therefore refer to the Ô¨Årst two server clusters
as Akamai-Hartford and Akamai-113km, respectively.
In
the rest of the paper, we classify the servers into four clus-
ters: Akamai-Hartford, Akamai-113km, Google9, and other
servers.

Figures 6(a), (b), and (c) plot per-Ô¨Çow servicing rate dis-
tributions of G1, G2 and G3 Ô¨Çows, respectively. The re-
sults for all four server clusters are plotted in the Ô¨Ågure.
For the same server cluster, we observe larger per-Ô¨Çow ser-
vicing rate for longer Ô¨Çows because longer Ô¨Çows allow the
congestion window to ramp up. Overall, Akamai-Hartford
servers provide the best performance, with median servic-
ing rates of 4.7Mbps, 5.9Mbps, and 6.5Mbps for G1, G2,
and G3 Ô¨Çows, respectively. For G2 and G3 Ô¨Çows, we ob-

9Google does not publish the location information of its data
centers. Therefore we cannot simply use the database [17] to
determine the geographic locations of Google servers. Hence
we present the aggregate results of all Google servers. It is
possible to determine the geographic location of the servers
using RTT [30]. Dividing the servers into clusters according
to their geographic locations and investigating their respec-
tive performance are left as future work.

serve a clear stochastic order: the performance achieved by
Akamai-Hartford servers is the best, followed by Akamai-
113km, Google, and the other servers. For G1 Ô¨Çows, the
performance achieved by Google servers is superior to that
of Akamai-113km servers, and is superior to that of Akamai-
Hartford servers at low percentiles, a point that we will re-
turn later.

Existing studies (e.g.,

[28, 26]) have shown that video
servers may control the sending rate of video Ô¨Çows, which
can aÔ¨Äect per-Ô¨Çow servicing rate of those Ô¨Çows. In the traces
that we collected, we conÔ¨Årm that YouTube Ô¨Çash videos to
NHDs are indeed rate controlled. On the other hand, videos
to NHDs are predominantly mp4 videos, which are not rate
controlled. Therefore, per-Ô¨Çow servicing rates of MHD Ô¨Çows
presented in Fig. 6 are not aÔ¨Äected by rate control mecha-
nisms. We next investigate the impact of various network
and application-layer factors on MHD network performance.

6.1 RTT

We observe that RTTs to Akamai-Hartford servers tend
to be the lowest, followed by Akamai-113km, Google, and
the other servers. The main reason for the lowest RTTs
to Akamai-Hartford servers is the close geographic distance
and good network provisioning (both the University of Hart-
ford where the servers are deployed and UConn belong to
Connecticut education network). Fig. 7 plots the RTT dis-
tributions of G2 MHD Ô¨Çows served by the four server clusters
(results for G1 and G3 Ô¨Çows are similar). The median RTT
of the Ô¨Çows served by Akamai-Hartford servers is 8.5ms,
while for Akamai-113km, Google, and other servers, the me-
dian RTTs are respectively 2.3, 4.3, and 10.3 times larger.
For the four server clusters, their relative order in terms of
RTT is consistent with their relative performance as shown
in Fig. 6. That is, a server cluster with lower RTTs tends
to provide better performance. This is not surprising since
RTT is a major factor that aÔ¨Äects network performance.

What is more interesting is perhaps the large percent-
age of data that is served by the Ô¨Årst three server clus-
ters. SpeciÔ¨Åcally, 58% of the MHD data bytes are served
by the Ô¨Årst three server clusters, much larger than the cor-
responding value (41%) for NHDs. Furthermore, 16% of
the MHDs Ô¨Çows (accounting for 35% of the data) are served
by Akamai-Hartford servers, the server cluster that provides
the best performance. As described earlier, we believe that
these large percentages originate from the fact that the most
popular services accessed by MHDs are provided by major
companies such as Facebook, Google, and Apple that use
CDN services heavily. The large percentage of data served
by Akamai and Google servers boost the overall network
performance of MHDs.

6.2 Local RTT

We observe that long MHD Ô¨Çows tend to experience larger
local RTTs than short MHD Ô¨Çows. As an example, Fig. 8(a)
plots local RTT distributions of G1, G2, and G3 MHD Ô¨Çows
served by Akamai-Hartford server cluster (results for the
other three server clusters are similar). We see that local
RTT of G1 Ô¨Çows is smaller than that of G2 Ô¨Çows, which is
in turn smaller than that of G3 Ô¨Çows. In addition, we ob-
serve that local RTT of MHDs tends to be larger than that
of NHDs. One example is shown in Fig. 8(b), which plots lo-
cal RTT distributions of G3 MHD and NHD Ô¨Çows served by
Akamai-Hartford servers. Using t-test [18], we conÔ¨Årm that

321 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

0.6

0.4

F
D
C

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

0.6

0.4

F
D
C

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

F
D
C

0.6

0.4

0.2

0.0

10

100

1000 10000 100000

10

100

1000 10000 100000

10

100

1000 10000 100000

Per-flow Servicing Rate (Kbps)

Per-flow Servicing Rate (Kbps)

Per-flow Servicing Rate (Kbps)

(a) G1

(b) G2

(c) G3

Figure 6: Per-Ô¨Çow servicing rate of MHD Ô¨Çows served by the four server clusters.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1E-3

0.01

0.1

1

10

RTT (sec)

Figure 7: RTT distributions of G2 Ô¨Çows served by
the four server clusters.

1.0

0.8

0.6

0.4

0.2

0.0

F
D
C

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

 G1
 G2
 G3

the above two observations indeed hold statistically. Specif-
ically, for the MHD Ô¨Çows served by the same server cluster,
the average local RTT of Gi Ô¨Çows is statistically smaller
than that of Gj Ô¨Çows, i < j; and for Gi Ô¨Çows, the average
local RTT of MHD Ô¨Çows is statistically larger than that of
NHD Ô¨Çows, i = 1, 2, 3. The reason might be limited compu-
tation and I/O capabilities on MHDs, which lead to longer
processing time for longer Ô¨Çows, as well as longer processing
time than that on NHDs (a more in-depth study using active
measurements is left as future work). On the other hand, ex-
cept for Akamai-Hartford servers, local RTT is only a small
portion of RTT (the ratio of local RTT over RTT is below
0.2 for 10% to 56% of the Ô¨Çows). Therefore, the impact of
local RTT on per-Ô¨Çow servicing rate is negligible. We also
observe a small fraction of local RTTs that are longer than
100ms, a point we will return to in Section 6.3.

6.3 Loss Rate

F
D
C

1.0

0.9

0.8

0.7

0.6

0.5

 G1

 G2

 G3

 MHD
 NHD

1E-4

1E-3

0.01

0.1

1

Loss Rate

1E-3

0.01

0.1

1

10

1E-3

0.01

0.1

1

10

Local RTT (sec)

Local RTT (sec)

(a)

(b)

Figure 8: (a) Local RTT for the MHD Ô¨Çows served
by Akamai-Hartford servers. (b) Local RTT of G3
MHD and NHD Ô¨Çows served by Akamai-Hartford
servers.

Figure 9: Loss rate distribution for the MHD Ô¨Çows
served by Akamai-Hartford servers.

We Ô¨Ånd that most Ô¨Çows have very low loss rates, indi-
cating that loss rate is not a limiting factor in the campus
WiFi network that we study. An example is shown in Fig. 9,
which plots loss rate distribution of the MHD Ô¨Çows served by
Akamai-Hartford servers (results for the other three server
clusters have similar trend). We observe that, for G1, G2
and G3 Ô¨Çows, respectively 86%, 74% and 54% of the Ô¨Çows

322have zero loss. Furthermore, 90% of G3 Ô¨Çows have loss rates
below 0.02. The loss rate of a short Ô¨Çow can be large, which
is however due to the artifact of small Ô¨Çow size (i.e., even a
small number of losses can lead to high loss rate). We also
Ô¨Ånd that all of the losses occur outside UConn (i.e., local
loss rate is zero). This, however, does not mean that there
is no loss at the MAC layer. We observe that a small per-
centage (around 3%) of local RTTs are longer than 100ms
(see Fig. 8(a)). These long local RTTs might be caused by
MAC-layer retransmissions and back-oÔ¨Ä times. Our mea-
surements captured at the monitoring point does not contain
MAC layer information; validating this conjecture through
measurements captured at MAC layer is left as future work.

6.4

Initial Congestion Window

We Ô¨Ånd that Akamai and Google servers have adopted
large initial congestion window. SpeciÔ¨Åcally, the median
initial congestion window of Akamai-Hartford servers is be-
tween 5 and 6KB, and the maximum initial congestion win-
dow is between 10 and 12KB. For Akamai-113km servers,
79% and 19% of the servers have median initial congestion
window of 8KB and 5.5KB, respectively; the maximum ini-
tial congestion window is as large as 15KB. Google‚Äôs adop-
tion of large initial congestion window is even more aggres-
sive: 98% of the Google servers use a median initial conges-
tion window of 14KB (consistent with the advocated value
of 15KB [10]), and the largest initial congestion window can
be as large as 25KB. The rest of the servers (i.e., those other
than Akamai and Google servers) have not adopted large ini-
tial congestion window as aggressively. SpeciÔ¨Åcally, 61% of
them use initial congestion window smaller than 4KB.

Large initial congestion window is particularly beneÔ¨Åcial
to short Ô¨Çows that can be completed in one RTT when ini-
tial congestion window is large. The more aggressive ini-
tial congestion window adopted by Google servers leads to
its superior performance in serving G1 Ô¨Çows: we observe
from Fig. 6(a) better performance from Google servers than
Akamai-113km servers at low percentiles despite that RTT
to Google servers tends to be larger than that to Akamai-
113km (RTT distributions for G1 Ô¨Çows are similar to those
for G2 Ô¨Çows, which is plotted in Fig. 7). Furthermore, a
smaller fraction of G1 Ô¨Çows served by Google servers has
very low servicing rate, as shown in Fig. 6(a). Even for G2
Ô¨Çows, we observe similar servicing rates between Ô¨Çows served
by Akamai-113km and Google servers (Fig. 6(b)) despite of
the more dramatic diÔ¨Äerences in RTT (Fig. 7).

On the other hand, adopting larger initial congestion win-
dow can lead to congestion in the network. The study in [10]
shows that this is only true for slow network connections.
From our results, we do not observe signiÔ¨Åcantly diÔ¨Äerent
loss rates from the server clusters that adopt diÔ¨Äerent initial
congestion windows.

6.5 Advertised Receive Window

We Ô¨Ånd that both iOS and Andriod operating systems
support window scaling. When connecting to servers that
support window scaling (e.g., we Ô¨Ånd all Akamai and Google
servers support window scaling), the receive window adver-
tised by iOS devices is 128KB10, implying that the maxi-
mum window of a Ô¨Çow is 128KB. In contrast to the static

10The windows scaling factor is 4, leading to receive window
of 256KB. However, we observe that the receive window is
reduced to 128KB at the end of three-way handshake.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 G1
 G2
 G3

10

100

1000

Maximum Window (KB)

(a) iOS

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 G1
 G2
 G3

10

100

1000

Maximum Window (KB)
(b) Android

Figure 10: Distribution of maximum window size for
iOS and Android Ô¨Çows served by Google servers.

large receive window advertised by iOS devices, Andriod
devices dynamically adjust receive window every round trip
time, starting from a fairly small size of 5KB. Furthermore,
since the receive window can grow dynamically, the maxi-
mum window size can be larger than 128KB.

The diÔ¨Äerent design choices adopted by iOS and Andriod
devices have the following implications. First, we Ô¨Ånd the
large receive window of iOS devices cannot be fully utilized
by most Ô¨Çows. To illustrate this, we plot the distribution of
maximum window size of iOS Ô¨Çows that are served by Google
servers, as shown in Fig. 10(a). The reason for choosing
Google server cluster is that it serves the highest percentage
of video Ô¨Çows among all the four server clusters (the percent-
age is 30.5%), and video Ô¨Çows tend to be longer than other
types of Ô¨Çows, and hence are more likely to reach large win-
dow size. From Fig. 10(a), we see that only 8% of G3 Ô¨Çows
reach the maximum window limit of 128KB, and the per-
centages for G1 and G2 Ô¨Çows are even lower, indicating that
the large receive window of 128KB could potentially cause
unnecessary waste of resources on iOS devices (whether it
wastes resources or not depends on the kernel memory allo-
cation mechanism: If the iOS kernel preallocates the mem-
ory for the socket, this causes waste of resources; otherwise,
it does not waste resources).

Secondly, the very small receive window adopted by An-
driod devices, while conserving resources, making Andriod
devices unable to take advantage of large initial congestion
window adopted by many servers (recall TCP congestion
window is the minimum of the sender and receiver window),
and hence can lead to inferior performance, particularly for
short Ô¨Çows. As we shall see in Section 6.9, this is indeed the
case (Section 6.9 reports Ô¨Åndings from the data set collected
in 2012, which contains more Andriod traÔ¨Éc and hence al-
lows us to draw more convincing statistical conclusions re-
garding Andriod traÔ¨Éc). The adverse eÔ¨Äect of small receive
window may be even more dramatic in cellular networks
where the round trip time can be signiÔ¨Åcantly larger than
that in WiFi networks.

Thirdly, since the receive window of Andriod devices is
adjusted dynamically, it can grow to large values. Fig. 10(b)
plots the distribution of maximum window size for Android
Ô¨Çows that are served by Google servers. Indeed, we observe
9% of G3 Ô¨Çows reach maximum window sizes larger than
128KB, while the maximum window size is bounded below
128KB for iOS Ô¨Çows. We suspect that since the receive
window of Andriod devices can grow to large values, Android
devices can achieve better performance for very long Ô¨Çows.

323Our dataset, however, does not contain suÔ¨Écient number of
very long Ô¨Çows from Android devices (even in the data set
that was collected in April 2012, which contains much more
traÔ¨Éc from Android devices) to statistically verify the above
conjecture.

Summarizing the above, we believe dynamic receive win-
dow adopted by Andriod devices is a suitable choice for re-
source limited MHDs. On the other hand, using very small
initial receive window (e.g., 5KB) is too conservative, which
can lead to inferior performance, particularly for short Ô¨Çows.
How to choose receive window for MHDs to be both resource
eÔ¨Écient and performance enhancing is beyond the scope of
this paper, but is an interesting problem that we leave as
future work.

6.6 Application-level Protocol

)

B
K

(
 
a

t

 

a
D
d
e
d
a
o
n
w
o
D

l

 20000
 18000
 16000
 14000
 12000
 10000
 8000
 6000
 4000
 2000

 0

 0  20  40  60  80 100 120 140 160 180

Time (s)

Figure 11: A series of 75 TCP Ô¨Çows is used to
download one YouTube video when using the native
YouTube application on an iOS device. The Ô¨Ågure
shows the starting and ending times of each TCP
Ô¨Çow with the requested range. For better clarity,
we use diÔ¨Äerent colors (red and blue) to represent
two adjacent TCP Ô¨Çows.

We Ô¨Ånd that some application-level protocols for MHDs
are highly optimized for cellular networks, which may cause
ineÔ¨Écient use of network and operating system resources of
MHDs in WiFi networks. The native YouTube player in iOS
is an example. When using this player, video contents are
downloaded in segments; each segment is requested in a sep-
arate TCP connection, using the HTTP Range header Ô¨Åeld
to specify the requested portion of the video [13]. While it
has been reported in literature that multiple TCP connec-
tions are used to serve a single YouTube video [13, 26], we
Ô¨Ånd, surprisingly, the number of TCP connections can be
extremely large. Fig. 11 shows an example where a series of
75 TCP Ô¨Çows (most of them very short-lived) are used to
download a single YouTube video. Considering the overhead
of creating new TCP connections and the slow-start phase
at the beginning of a TCP connection, using so many short-
lived TCP Ô¨Çows to serve a single video does not seem to be
sensible. We suspect this is a design choice that tries to cope
with TCP timeouts (e.g., caused by long delays originated
from handoÔ¨Äs, disconnections, and MAC-level retransmis-
sion) in cellular networks [8]. For example, the handoÔ¨Ä in
cellular network can take more than one second, which might

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

 img
 video
 txt

 img
 video
 txt

0.010 0.015 0.020 0.025 0.030 0.035 0.040

0.010 0.015 0.020 0.025 0.030 0.035 0.040

Application response time (s)

Application response time (s)

(a) MHD

(b) NHD

Figure 12: Application response time for MHD and
NHD Ô¨Çows that are served by Google servers.

cause TCP timeout and reduced congestion window (in the
worst case, the congestion window might be reduced to one).
Therefore, it might make sense to open a new TCP connec-
tion to take advantage of large initial congestion window to
overcome the impact of the signiÔ¨Åcantly degraded through-
put.

We again Ô¨Ånd the design choices taken by iOS and An-
droid devices are diÔ¨Äerent: Android devices use only one
TCP Ô¨Çow to download a single YouTube video. How to
optimize application-level protocols for MHDs, considering
the characteristics of both cellular and WiFi networks, is an
interesting problem, but is beyond the scope of this paper.

6.7 Application Response Time

We investigate application response time for diÔ¨Äerent types
of content to understand whether servers use diÔ¨Äerent mech-
anisms based on content type (recall application response
time represents the delay from the Ô¨Årst GET message to the
Ô¨Årst responding data packet for an HTTP request). Fig. 12(a)
plots application response time distribution for three types
of contents, video, image and text, that are served by Google
servers. Most of the requested videos are YouTube videos,
which are destined to Google servers because Google has
migrated YouTube servers to its own network infrastructure
after acquiring YouTube [30]. We observe from Fig. 12(a)
that, perhaps surprisingly, videos have the lowest response
time, followed by images and text. The reason why videos
have the lowest response time might be that users tend to
watch popular videos (note that YouTube iOS application
has ‚ÄúFeatured‚Äù and ‚ÄúMost Viewed‚Äù categories, convenient for
users to select and watch those popular videos) and servers
cache popular videos, leading to low response time. The
slowest response time for texts might be because most of
the requests for texts query search engines, which can take
a longer time to respond.

Interestingly, the results for NHDs (see Fig. 12(b)) dif-
fer from those for MHDs: for NHDs, the response times for
videos and texts are similar. We do not know the exact rea-
sons that cause the diÔ¨Äerence between MHDs and NHDs.
One observation is that the user interface for NHDs diÔ¨Äer
from that in MHDs: for NHDs, YouTube suggests related
videos based on a user‚Äôs preference (such as browsing his-
tory, local cache, etc.) while for MHDs, YouTube suggests
featured and most viewed videos. Further investigation is
left as future work.

In Fig. 12, we observe that the distribution of the response
times for image Ô¨Çows that are served by Google contains

324F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

 img
 video
 txt

 img
 video
 txt

0.015

0.020

0.025

0.030

0.035

0.040

0.015

0.020

0.025

0.030

0.035

0.040

RTT (s)

(a) MHD

RTT (s)

(b) NHD

Figure 13: RTT for MHD and NHD Ô¨Çows that are
served by Google servers.

two modes. We conjecture that this is because the images
are served by two clusters of servers, one closer to UConn
than the other. Since Google data center information is
not disclosed to the public, we try to infer the Google server
locations through the RTT distribution [30]. Fig. 13 presents
the RTT distributions for the Ô¨Çows with diÔ¨Äerent content
types. We indeed observe that the RTT distribution for
image Ô¨Çows contains two modes, implying that they may
be served by two clusters of servers at diÔ¨Äerent geographic
locations.

6.8 Number of Concurrent TCP Flows

As stated in Section 5.4, the number of concurrent TCP
Ô¨Çows might be an indicator of the amount of CPU and I/O
activities on an MHD device. We next present the rela-
tionship between the number of concurrent TCP Ô¨Çows and
per-Ô¨Çow servicing rate.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 MHD

 NHD

10

100

Number of Concurrent TCP Flows

Figure 14: Average number of concurrent TCP Ô¨Çows
for G3 Ô¨Çows served by Akamai-113km servers.

We Ô¨Ånd that many G3 Ô¨Çows have more than one concur-
rent TCP Ô¨Çow. Fig. 14 plots the distribution of the aver-
age number of concurrent TCP Ô¨Çows for G3 Ô¨Çows served by
Akamai-113km servers (for comparison, the results for both
MHDs and NHDs are plotted in the Ô¨Ågure). For MHDs,
over 90% of the G3 Ô¨Çows have more than one concurrent
TCP Ô¨Çow, and there can be tens of concurrent Ô¨Çows. The
results for the other three server clusters are similar (Ô¨Ågure
omitted). Intuitively, a larger number of concurrent Ô¨Çows on
an MHD may lead to less resources to each Ô¨Çow, and hence

lower per-Ô¨Çow servicing rate. To verify this conjecture, we
obtain the correlation coeÔ¨Écient between the average num-
ber of TCP Ô¨Çows that are concurrent with a G3 video Ô¨Çow
and the per-Ô¨Çow servicing rate of the G3 video Ô¨Çow. We Ô¨Ånd
that the correlation coeÔ¨Écients are -0.03, -0.4 and -0.29 for
G3 video Ô¨Çows served by Akamai-Hartford, Akamai-113km
and Google servers, respectively. The negative values indi-
cate that indeed more concurrent Ô¨Çows can lead to lower
per-Ô¨Çow servicing rates. The correlation is more signiÔ¨Åcant
for video Ô¨Çows served by Akamai-113km and Google. For
the Ô¨Çows served by Akamai-Harford, the less signiÔ¨Åcant cor-
relation might be due to the superior performance achieved
by this server cluster, which makes the eÔ¨Äect of other factors
less visible.

For comparison, let us look at the distribution of the av-
erage number of concurrent TCP Ô¨Çows for G3 NHD Ô¨Çows
in Fig. 14. Not surprisingly, the number of concurrent TCP
Ô¨Çows on NHDs can be much larger than that on MHDs.
On the other hand, we observe less signiÔ¨Åcant correlation
between the number of concurrent TCP Ô¨Çows and per-Ô¨Çow
servicing rates on NHDs:
in general, the negative correla-
tion coeÔ¨Écient varies between -0.12 and -0.10 for the NHD
Ô¨Çows served by the four server clusters. This less signiÔ¨Åcant
correlation may be due to superior computation and I/O
capabilities on NHDs.

6.9 Findings from the 2012 Data Set

In the previous sections, we have presented the results
from the data set collected in March 2011. We next report
the results from the data set collected in April 2012.

In this data set, we Ô¨Ånd that only 64.7% of the TCP Ô¨Çows
use HTTP, compared to 92.3% in the 2011 data set. On the
other hand, the percentage of HTTPS Ô¨Çows has increased
from 4.3% to 25.3%. This sharp increase is caused by the
fact that many popular web sites such as Google, Facebook,
and Hotmail, add the functionality to access their services
using HTTPS, which protects users‚Äô privacy especially when
they use public WiFi access points. For the rest of the TCP
Ô¨Çows (11%), we do not observe any dominant application
protocol.

Since the percentage of HTTP Ô¨Çows in the 2012 data set is
much lower than that in the 2011 data set, and our approach
to classifying MHD and NHD Ô¨Çows cannot be applied to the
non-HTTP Ô¨Çows (since it uses User-Agent Ô¨Åeld in HTTP
headers), leaving a large percentage (36.3%) of the TCP
Ô¨Çows in the 2012 data set not analyzed, we have focused on
the 2011 data set in this paper. We next describe the main
results from analyzing the HTTP Ô¨Çows in the 2012 data set
(developing new approaches to identify the device types for
the non-HTTP Ô¨Çows in the 2012 data set, and analyzing
their characteristics are left as future work). In the interests
of space, we only present the main Ô¨Åndings that diÔ¨Äer from
those from the 2011 data set.

‚Ä¢ We observe that more MHD and NHD Ô¨Çows are served
by Akamai and Google servers. SpeciÔ¨Åcally, 43.5% of
MHD Ô¨Çows and 38.4% of NHD Ô¨Çows are served by
these two sets of servers, respectively. The number of
distinct destination host domains accessed by MHDs
has increased signiÔ¨Åcantly to 9.3k (compared to 6k in
the 2011 data set), while for NHDs, the increase is
less dramatic (from 51k to 53k). The destination host
domains accessed by MHDs are still less diverse than
those accessed by NHDs. This is also evidenced by

325the observation that for MHDs, 38% of the Ô¨Çows are
destined to the top ten host domains, while for NHDs,
the percentage is 29%.

‚Ä¢ We Ô¨Ånd that Google has deployed a set of servers (con-
taining 12 IP addresses) very close to UConn, in the
Connecticut Education Network at Hartford. Simi-
lar to the Akamai-Hartford server, this set of close-by
Google servers provide high per-Ô¨Çow servicing rate due
to small RTTs. We Ô¨Ånd 2.8% of MHD Ô¨Çows are served
by these servers.
In addition, analyzing the content
type, we Ô¨Ånd that they mainly serve Youtube traÔ¨Éc
(account for 45% of the MHD Ô¨Çows from these servers).

1.0

0.8

F
D
C

0.6

0.4

0.2

0.0

10

 iOS-G1
 Android-S-G1
 Android-L-G1

100

1000

10000

Per-flow Servicing Rate (Kbps)

Figure 15: Distribution of per-Ô¨Çow servicing rate of
G1 iOS and Android Ô¨Çows that are served by Google
servers from the 2012 data set, where Android-S-G1
corresponds to Andriod Ô¨Çows using small initial re-
ceive window (5KB) and Android-L-G1 corresponds
to Andriod Ô¨Çows using large initial receive window
(14KB).

‚Ä¢ The amount of Android MHD Ô¨Çows has increased to
10% (compared to 3.2% in the 2011 data set), indi-
cating the increasing popularity of Android devices.
In addition, we observe 12% of the Android Ô¨Çows start
with a signiÔ¨Åcantly larger advertised receive window of
14KB, compared to the small initial receive window of
5KB that is observed predominantly in the 2011 data
set. This larger initial receive window was adopted by
newer versions of Android OS (starting from Android
OS 4.X), and allows Android devices to better utilize
the larger initial congestion window adopted by many
servers. Fig. 15 plots the distribution of per-Ô¨Çow ser-
vicing rate of iOS and Andriod Ô¨Çows that are served
by Google servers. We separate the Android Ô¨Çows into
two groups, one using initial receive window of 5KB,
and the other using initial receive window of 14KB. We
only plot the results for G1 Ô¨Çows; the results for G2
and G3 Ô¨Çows are consistent. Fig. 15 shows that iOS de-
vices outperform Android devices that use small initial
receive window, a point that we made in Section 6.5.
On the other hand, the performance of Android de-
vices that have adopted large initial receive window is
comparable to that of iOS devices.

7. DISCUSSION

Our study has been conducted in a speciÔ¨Åc WiFi network,
UConn campus WiFi network. A natural question is what
Ô¨Åndings from our study are speciÔ¨Åc to UConn network and
what are applicable to other WiFi networks.

‚Ä¢ Our Ô¨Åndings related to hardware and software of MHDs

and application-level protocols are not speciÔ¨Åc to UConn
network, and hence we believe that they are equally
applicable to other WiFi networks.

‚Ä¢ The content delivery infrastructures in other networks
may diÔ¨Äer signiÔ¨Åcantly from that perceived by UConn
network. On the other hand, we believe MHDs in other
networks also use Akamai and Google servers heavily
because the most popular services accessed by MHDs
are provided by major companies such as Facebook,
Google, and Apple that use Akamai and Google servers
heavily. The extent of usage and whether the usage by
MHDs is heavier than that by NHDs in other networks,
however, depend on the popularity of the applications
in other networks.

‚Ä¢ The use of Akamai and Google servers can also boost
the network performance of MHDs in other networks
in the US due to small RTTs provided by these servers.
SpeciÔ¨Åcally, the study of [15] reports that Akamai tends
to deploy CDN sites close to the end users, and among
all of the Akamai services, the 10th percentile and me-
dian of delays to Akamai are around 10ms and 20ms,
respectively (these delays are comparable to the me-
dian delays of 8.5ms and 20ms from UConn campus
to the two closest Akamai sites); Google is reported to
provide similar RTT performance to most of the users
in the US [27].

‚Ä¢ The loss rates and RTTs in other networks may be sig-
niÔ¨Åcantly larger than those in UConn network. This
implies that local losses and RTTs may play a signif-
icant role in network performance of MHDs in other
networks. In addition, large initial congestion window
adopted by many servers may not be strictly beneÔ¨Åcial
in other networks.

8. CONCLUSION AND FUTURE WORK

In this paper, we have studied the network performance of
MHDs inside UConn campus network. We Ô¨Ånd that, com-
pared to NHDs, MHDs use well provisioned Akamai and
Google servers more heavily, which boosts the overall net-
work performance of MHDs. Furthermore, MHD Ô¨Çows, par-
ticularly short Ô¨Çows, beneÔ¨Åt from the large initial conges-
tion window that has been adopted by Akamai and Google
servers. Secondly, MHDs tend to have longer local delays
inside the WiFi network and are more adversely aÔ¨Äected by
the number of concurrent Ô¨Çows. Thirdly, Android OS can-
not take advantage of the large initial congestion window
adopted by many servers, while the large receive window
adopted by iOS is not fully utilized by most Ô¨Çows, leading
to waste of resources. Last, some application-level proto-
cols cause ineÔ¨Écient use of network and operating system
resources of MHDs in WiFi networks. Our observations pro-
vide valuable insights on content distribution, server provi-
sioning, MHD system design, and application-level protocol
design.

326As future work, we plan to use active controlled experi-
ments to understand why local RTTs of MHDs tend to be
larger than those on NHDs, and to understand the bot-
tleneck(s) of MHD Ô¨Çows. We also plan to study network
performance of MHDs in other public WiFi networks. For
instance, we believe that WiFi hotspots (e.g., hotspots in
Starbucks) and other campus WiFi networks might have
diÔ¨Äerent network characteristics (e.g., higher loss rates and
higher RTTs) as well as diÔ¨Äerent content popularity among
users. Furthermore, the content delivery infrastructure may
diÔ¨Äer signiÔ¨Åcantly from that perceived by UConn network.
Quantifying the network performance of MHDs and iden-
tifying the performance limiting factors in a wide range of
settings will provide us better insights on designing network
services for MHDs. Last, we plan to study how the perfor-
mance of 3G cellular network diÔ¨Äers from that of WiFi in
our campus network and when/why users on campus switch
from one network interface to another network interface.

Acknowledgement
The work was supported in part by NSF CAREER Award
0746841. We thank J. Farese, R. Kocsondy, J. Pufahl, and S.
Maresca (UConn) for their help with the monitoring equip-
ment and their assistance in gathering the data. We thank
Ilhwan Kim and Jaewan Jang (NHN) and D. Xuan (Ohio
State University) for helpful discussions. We also thank the
anonymous reviewers for their insightful comments, and our
shepherd A. Balasubramanian for many helpful suggestions.

9. REFERENCES
[1] Alexa. http://www.alexa.com.
[2] DAG card. http://www.endace.com.
[3] HTTP Pipelining.

http://en.wikipedia.org/wiki/HTTP pipelining.

[4] M. Allman, S. Floyd, and C. Partridge. Increasing

TCP‚Äôs initial window, 2002. RFC 3390.

[5] N. Balasubramanian, A. Balasubramanian, and

A. Venkataramani. Energy consumption in mobile
phones: a measurement study and implications for
network applications. In Proc. of ACM IMC, 2009.
[6] N. Banerjee, A. Rahmati, M. D. Corner, S. Rollins,
and L. Zhong. Users and batteries: Interactions and
adaptive energy management in mobile systems. In
Proc. of ACM Ubicomp, 2007.

[7] X. Chen, B. Wang, K. Suh, and W. Wei. Passive

online wireless LAN health monitoring from a single
measurement point. ACM Mobile Computer Comm.
Review, 14, November 2010.

[8] C. M. Choon and R. Ram. Improving TCP/IP

performance over third-generation wireless networks.
IEEE Transactions on Mobile Computing, 2008.

[9] P. Deshpande, X. Hou, and S. R. Das. Performance

comparison of 3G and metro-scale WiFi for vehicular
network access. In Proc. of ACM IMC, 2010.

[10] N. Dukkipati, T. ReÔ¨Åce, Y. Cheng, J. Chu,

T. Herbert, A. Agarwal, A. Jain, and N. Sutin. An
argument for increasing TCP‚Äôs initial congestion
window. ACM SIGCOMM CCR, 40:26‚Äì33, June 2010.

[11] H. Falaki, D. Lymberopoulos, R. Mahajan,

S. Kandula, and D. Estrin. A Ô¨Årst look at traÔ¨Éc on
smartphones. In Proc. of ACM IMC, 2010.

[12] H. Falaki, R. Mahajan, S. Kandula,

D. Lymberopoulos, R. Govindan, and D. Estrin.
Diversity in smartphone usage. In Proc. of ACM
MobiSys, 2010.

[13] A. Finamore, M. Mellia, M. Munafo, and S. G. Rao.

YouTube everywhere: Impact of device and
infrastructure synergies on user experience. In Proc. of
ACM IMC, 2011.

[14] A. Gember, A. Anand, and A. Akella. A comparative
study of handheld and non-handheld traÔ¨Éc in campus
WiFi networks. In Proc. of PAM, 2011.

[15] C. Huang, A. Wang, J. Li, and K. W. Ross. Measuring

and evaluating large-scale CDNs. Technical Report
MSR-TR-2008-106, Microsoft Research, 2008.

[16] J. Huang, Q. Xu, B. Tiwana, Z. M. Mao, M. Zhang,

and P. Bahl. Anatomizing application performance
diÔ¨Äerences on smartphones. In Proc. of ACM Mobisys,
2010.

[17] IP2Location. http://www.ip2location.com.
[18] R. Jain. The art of computer systems performance

analysis - techniques for experimental design,
measurement, simulation, and modeling. Wiley
professional computing. Wiley, 1991.

[19] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, and

D. Towsley. Inferring TCP connection characteristics
through passive measurements. In Proc. of IEEE
INFOCOM, 2004.

[20] G. Maier, A. Feldmann, V. Paxson, and M. Allman.
On dominant characteristics of residential broadband
Internet traÔ¨Éc. In Proc. of ACM IMC, 2009.

[21] G. Maier, F. Schneider, and A. Feldmann. A Ô¨Årst look

at mobile hand-held device traÔ¨Éc. In Proc. of PAM,
2010.

[22] A.-F. Mohammad, E. Khaled, R. Benjamin, and

G. Igor. Overclocking the Yahoo!: CDN for faster web
page loads. In Proc. of ACM IMC, 2011.

[23] F. Qian, A. Gerber, Z. M. Mao, S. Sen, O. Spatscheck,
and W. Willinger. TCP revisited: a fresh look at TCP
in the wild. In Proc. of ACM IMC, 2009.

[24] M.-R. Ra, J. Paek, A. B. Sharma, R. Govindan, M. H.

Krieger, and M. J. Neely. Energy-delay tradeoÔ¨Äs in
smartphone applications. In Proc. of ACM MobiSys,
2010.

[25] A. Rahmati and L. Zhong. Human battery interaction

on mobile phones. Elsevier Pervasive and Mobile
Computing Journal, 5(5), 2009.

[26] A. Rao, Y.-S. Lim, C. Barakat, A. Legout, D. Towsley,

and W. Dabbous. Network characteristics of video
streaming traÔ¨Éc. In Proc. of ACM CoNext, 2011.

[27] K. Rupa, M. H. V., S. Sridhar, J. Sushant, K. Arvind,

A. Thomas, and G. Jie. Moving beyond end-to-end
path information to optimize CDN performance. In
Proc. of ACM IMC, 2009.

[28] A. Shane and N. Richard. Application Ô¨Çow control in

YouTube video streams. SIGCOMM Computer
Comm. Review, 41, 2011.

[29] A. Shye, B. Sholbrock, and G. Memik. Into the wild:

Studying real user activity patterns to guide power
optimization for mobile architectures. In Proc. of
IEEE/ACM MICRO, 2009.

[30] R. Torres, A. Finamore, J. Kim, M. Mellia,

327M. Munafo, and S. Rao. Dissecting video server
selection strategies in the YouTube CDN. In Proc.
IEEE ICDCS, 2011.

[33] D. Zhang. Web content adaptation for mobile

handheld devices. Communications of the ACM, 50(2),
February 2007.

[31] I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci.

[34] Z. Zhuang, K.-H. Kim, and J. P. Singh. Improving

Measuring serendipity: connecting people, locations
and interests in a mobile 3G network. In Proc. of
ACM IMC, 2009.

[32] F. P. Tso, J. Teng, W. Jia, and D. Xuan. Mobility: a
double-edged sword for HSPA networks: a large-scale
test on Hong Kong mobile HSPA networks. In Proc. of
ACM MobiHoc, 2010.

energy eÔ¨Éciency of location sensing on smartphones.
In Proc. of ACM MobiSys, 2010.

328