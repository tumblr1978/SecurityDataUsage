2013 IEEE Symposium on Security and Privacy

Pinocchio: Nearly Practical Veriﬁable Computation

Bryan Parno
Jon Howell

Microsoft Research

Abstract
To instill greater conﬁdence in computations outsourced to
the cloud, clients should be able to verify the correctness
of the results returned. To this end, we introduce Pinoc-
chio, a built system for efﬁciently verifying general computa-
tions while relying only on cryptographic assumptions. With
Pinocchio, the client creates a public evaluation key to de-
scribe her computation; this setup is proportional to evalu-
ating the computation once. The worker then evaluates the
computation on a particular input and uses the evaluation key
to produce a proof of correctness. The proof is only 288
bytes, regardless of the computation performed or the size of
the inputs and outputs. Anyone can use a public veriﬁcation
key to check the proof.

Introduction

Crucially, our evaluation on seven applications demon-
strates that Pinocchio is efﬁcient in practice too. Pinocchio’s
veriﬁcation time is typically 10ms: 5-7 orders of magni-
tude less than previous work; indeed Pinocchio is the ﬁrst
general-purpose system to demonstrate veriﬁcation cheaper
than native execution (for some apps). Pinocchio also reduces
the worker’s proof effort by an additional 19-60×. As an
additional feature, Pinocchio generalizes to zero-knowledge
proofs at a negligible cost over the base protocol. Finally, to
aid development, Pinocchio provides an end-to-end toolchain
that compiles a subset of C into programs that implement the
veriﬁable computation protocol.
1
Since computational power is often asymmetric (particularly
for mobile devices), a relatively weak client may wish to out-
source computation to one or more powerful workers. Com-
mon examples include cloud or grid computing, as well as
volunteer distributed computing [1]. In all of these settings,
the client should be able to verify the results returned, to guard
against malicious or malfunctioning workers. Even from a
legitimate worker’s perspective, veriﬁable results are beneﬁ-
cial, since they are likely to command a higher price. They
also allow the worker to shed liability: any undesired outputs
are provably the result of data the client supplied.
Considerable systems and theory research has looked at the
problem of verifying computation (§6). However, most of
this work has either been function speciﬁc, relied on assump-
tions we prefer to avoid, or simply failed to pass basic prac-
ticality requirements. Function speciﬁc solutions [2–6] are
often efﬁcient, but only for a narrow class of computations.
More general solutions often rely on assumptions that may
not apply. For example, systems based on replication [1, 7, 8]
assume uncorrelated failures, while those based on Trusted

1081-6011/13 $26.00 © 2013 IEEE
DOI 10.1109/SP.2013.47

238

Craig Gentry

Mariana Raykova

IBM Research

Computing [9–11] or other secure hardware [12–15] assume
that physical protections cannot be defeated. Finally, the the-
ory community has produced a number of beautiful, general-
purpose protocols [16–23] that offer compelling asymptotics.
In practice however, because they rely on complex Probabilis-
tically Checkable Proofs (PCPs) [17] or fully-homomorphic
encryption (FHE) [24], the performance is unacceptable –
verifying small instances would take hundreds to trillions of
years (§5.2). Very recent work [25–28] has improved these
protocols considerably, but efﬁciency is still problematic, and
the protocols lack features like public veriﬁcation.

In contrast, we describe Pinocchio, a concrete system for
efﬁciently verifying general computations while making only
cryptographic assumptions. In particular, Pinocchio supports
public veriﬁable computation [22, 29], which allows an un-
trusted worker to produce signatures of computation.
Ini-
tially, the client chooses a function and generates a public
evaluation key and a (small) public veriﬁcation key. Given
the evaluation key, a worker can choose an input (or veriﬁ-
ably use one provided by the client), compute the function,
and produce a proof (or signature) to accompany the result.
Anyone (not just the client) can then use the veriﬁcation key
to check the correctness of the worker’s result for the spe-
ciﬁc input used. As an additional feature, Pinocchio supports
zero-knowledge veriﬁable computation, in which the worker
convinces the client that it knows an input with a particular
property, without revealing any information about the input.
Pinocchio’s asymptotics are excellent: key setup and proof
generation require cryptographic effort linear in the size of
the original computation, and veriﬁcation requires time linear
in the size of the inputs and outputs. Even more surprising,
Pinocchio’s proof is constant sized, regardless of the compu-
tation performed. Crucially, our evaluation (§5) demonstrates
that these asymptotics come with small constants, making
Pinocchio close to practical for a variety of applications.

Compared with previous work, Pinocchio improves veri-
ﬁcation time by 5-7 orders of magnitude and requires less
than 10ms in most conﬁgurations, enabling it to beat native
C execution for some apps. We also improve the worker’s
proof efforts by 19-60× relative to prior work. The resulting
proof is tiny, 288 bytes (only slightly more than an RSA-2048
signature), regardless of the computation. Making a compu-
tation zero-knowledge is also cheap, adding negligible over-
head (213µs to key generation and 0.1% to proof generation).
While these improvements are promising, additional
progress is likely needed before the overhead reaches true
practicality. However, even now, this overhead may be ac-
ceptable in scenarios that require high assurance, or that need
the zero-knowledge properties Pinocchio supports.

Figure 1: Overview of Pinocchio’s Toolchain. Pinocchio takes a
high-level C program all the way through to a distributed set of exe-
cutables that run the program in a veriﬁed fashion. It supports both
arithmetic circuits, via Quadratic Arithmetic Programs (§2.2.1), and
Boolean circuits via Quadratic Span Programs (§2.2.2).

To achieve efﬁcient veriﬁable computation, Pinocchio
combines quadratic programs, a computational model intro-
duced by Gennaro et al. [30], with a series of theoretical re-
ﬁnements and systems engineering to produce an end-to-end
toolchain for verifying computations. Speciﬁcally, via an im-
proved protocol and proof technique, we slash the cost of key
generation by 61%, and the cost of producing a proof by 64%.
From a developer’s perspective, Pinocchio provides a com-
piler that transforms C code into a circuit representation (we
support both Boolean and arithmetic), converts the circuit into
a quadratic program, and then generates programs to execute
the cryptographic protocol (Fig. 1).

Pinocchio’s end-to-end toolchain, plus its support for both
Boolean and arithmetic circuits, allows us to implement real
applications that beneﬁt from veriﬁcation. In particular, we
implement two forms of matrix multiplication, multivari-
ate polynomial evaluation, image matching, all-pairs shortest
paths, a lattice-gas scientiﬁc simulator, and SHA-1. We ﬁnd
(§5) that the ﬁrst three apps translate efﬁciently into arith-
metic circuits, and hence Pinocchio can verify their results
faster than native execution of the same program. The lat-
ter four apps translate less efﬁciently, due to their reliance on
inequality comparisons and bitwise operations, and yet they
may still be useful for zero-knowledge applications.
Contributions. In summary, this paper contributes:

1. An end-to-end system for efﬁciently verifying computa-
tion performed by one or more untrusted workers. This
includes a compiler that converts C code into a format
suitable for veriﬁcation, as well as a suite of tools for
running the actual protocol.

2. Theoretical and systems-level improvements that bring
performance down by 5-7 orders of magnitude, and
hence into the realm of plausibility.

3. An evaluation on seven real C applications, showing ver-
iﬁcation faster than 32-bit native integer execution for
some apps.

2 Background
2.1 Veriﬁable Computation (VC)
A public veriﬁable computation (VC) scheme allows a com-
putationally limited client to outsource to a worker the eval-
uation of a function F on input u. The client can then verify

the correctness of the returned result F(u) while performing
less work than required for the function evaluation.

More formally, we deﬁne public VC as follows, generaliz-

ing previous deﬁnitions [22, 29, 30].
Deﬁnition 1 (Public Veriﬁable Computation) A
public
veriﬁable computation scheme V C consists of a set of three
polynomial-time
(KeyGen, Compute, Verify)
deﬁned as follows.

algorithms

• (EKF ,V KF ) ← KeyGen(F,1λ): The randomized key
generation algorithm takes the function F to be out-
sourced and security parameter λ; it outputs a public
evaluation key EKF, and a public veriﬁcation key V KF.
• (y,πy) ← Compute(EKF ,u): The deterministic worker
algorithm uses the public evaluation key EKF and input
u. It outputs y ← F(u) and a proof πy of y’s correctness.
• {0,1} ← Verify(V KF ,u,y,πy): Given the veriﬁcation
key V KF, the deterministic veriﬁcation algorithm out-
puts 1 if F(u) = y, and 0 otherwise.

rity, and efﬁciency [30], so we merely summarize:

Prior work gives formal deﬁnitions for correctness, secu-
• Correctness For any function F, and any input u
if we run (EKF ,V KF ) ← KeyGen(F,1λ) and
to F,
(y,πy) ← Compute(EKF ,u), then we always get 1 =
Verify(V KF ,u,y,πy).

• Security

For

any

function

probabilistic
polynomial-time
Pr[( ˆu, ˆy, ˆπy) ← A(EKF ,V KF )
1 = Verify(V KF , ˆu, ˆy, ˆπy)] ≤ negl(λ).

any
A,
and
• Efﬁciency KeyGen is assumed to be a one-time opera-
tion whose cost is amortized over many calculations, but
we require that Verify is cheaper than evaluating F.

F
and
adversary
(cid:54)= ˆy

: F( ˆu)

Several previous VC schemes [22, 23] were not public, but
rather designated veriﬁer, meaning that the veriﬁcation key
V KF must be kept secret. Indeed, in these schemes, even re-
vealing the output of the veriﬁcation function (i.e., whether
or not the worker had been caught cheating) could lead to at-
tacks on the system. A public VC scheme avoids such issues.
Zero-Knowledge Veriﬁable Computation. We also con-
sider an extended setting where the outsourced computation
is a function, F(u,w), of two inputs: the client’s input u and
an auxiliary input w from the worker. A VC scheme is zero-
knowledge if the client learns nothing about the worker’s in-
put beyond the output of the computation.1

Zero knowledge is relevant to practical scenarios where the
worker’s input is private. For example, to anonymously au-
thenticate, the worker’s input w might be a signature from a
third party; the client’s input u is the third party’s public key,
and the function F(u,w) validates the signature. The client
learns that the worker holds a valid credential, but learns noth-
ing about the credential itself. Another potential application
is for privately aggregating sensitive data, for example, in the

1Such a scheme may also be referred to as a non-interactive zero knowl-

edge (NIZK) proof [31].

239

CILC exprsARITHGATES+, *, splitQAPpolynomialsECCveriﬁcationBOOLGATESQSPpolynomialshigh-levellanguagelow-levellogicsatisﬁabilityencodingveriﬁcationprotocolv1(ri)
v2(ri)
v3(ri)
v4(ri)
v5(ri)
v6(ri)

(r5,r6)
(0,1) w1(ri)
(0,1) w2(ri)
(1,0) w3(ri)
(0,0) w4(ri)
(0,0) w5(ri)
(0,0) w6(ri)

(r5,r6)
(0,0)
(0,0)
(0,0)
(1,0)
(0,1)
(0,0)

(r5,r6)
y1(ri)
(0,0)
y2(ri)
(0,0)
y3(ri)
(0,0)
y4(ri)
(0,0)
y5(ri)
(1,0)
y6(ri) (0,1)

t(x) = (x− r5)(x− r6)

Figure 2: Arithmetic Circuit and Equivalent QAP. Each wire
value comes from, and all operations are performed over, a ﬁeld F.
The polynomials in the QAP are deﬁned in terms of their evaluations
at the two roots, r5 and r6. See text for details.

context of smart-meter billing [32], where individual meter
readings should be private to the client, but the utility needs
to authenticate the aggregate amount owed.
2.2 Quadratic Programs
Gennaro, Gentry, Parno, and Raykova (GGPR) recently
showed how to compactly encode computations as quadratic
programs [30], so as to obtain efﬁcient VC and zero-
knowledge VC schemes. Speciﬁcally, they show how to con-
vert any arithmetic circuit into a comparably sized Quadratic
Arithmetic Program (QAP), and any Boolean circuit into a
comparably sized Quadratic Span Program (QSP). We sum-
marize these transformations.

Standard results show that polynomially-sized circuits are
equivalent (up to a logarithmic factor) to Turing machines that
run in polynomial time [33], though of course the actual ef-
ﬁciency of computing via circuits versus on native hardware
depends heavily on the application (e.g., an arithmetic cir-
cuit for matrix multiplication adds essentially no overhead,
whereas a Boolean circuit for integer multiplication is less
efﬁcient than executing a single 32-bit assembly instruction).
2.2.1 Arithmetic Circuits and QAPs
An arithmetic circuit consists of wires that carry values from
a ﬁeld F and connect to addition and multiplication gates –
see Figure 2 for an example. We deﬁne a QAP, an encoding
of such a circuit, as follows.
Deﬁnition 2 (Quadratic Arithmetic Program (QAP) [30])
A QAP Q over ﬁeld F contains three sets of m+1 polynomials
V = {vk(x)}, W = {wk(x)},Y = {yk(x)}, for k ∈ {0 . . .m},
and a target polynomial t(x). Suppose F is a function that
takes as input n elements of F and outputs n(cid:48) elements, for
a total of N = n + n(cid:48) I/O elements. Then we say that Q
computes F if: (c1, . . . ,cN) ∈ FN is a valid assignment of
F’s inputs and outputs, if and only if there exist coefﬁcients
(cN+1, . . . ,cm) such that t(x) divides p(x), where:

(cid:32)

p(x) =

−

(cid:32)

(cid:33)

(cid:32)
(cid:33)

·

v0(x) +

ck · vk(x)

m

∑

k=1

y0(x) +

ck · yk(x)

m

∑

k=1

(cid:33)

w0(x) +

ck · wk(x)

m

∑

k=1

.

240

In other words, there must exist some polynomial h(x) such
that h(x)· t(x) = p(x). The size of Q is m, and the degree is
the degree of t(x).

Building a QAP Q for an arithmetic circuit C is fairly
straightforward. We pick an arbitrary root rg ∈ F for each
multiplication gate g in C and deﬁne the target polynomial
to be t(x) = ∏g(x − rg). We associate an index k ∈ [m] =
{1 . . .m} to each input of the circuit and to each output from
a multiplication gate (the addition gates will be compressed
into their contributions to the multiplication gates). Finally,
we deﬁne the polynomials in V , W , and Y by letting the
polynomials in V encode the left input into each gate, the W
encode the right input into each gate, and the Y encode the
outputs. For example, vk(rg) = 1 if the k-th wire is a left input
to gate g, and vk(rg) = 0 otherwise. Similarly, yk(rg) = 1 if
the k-th wire is the output of gate g, and yk(rg) = 0 otherwise.
Thus, if we consider a particular gate g and its root rg, Equa-
k=1 ck · wk(rg)) =
tion 1 simpliﬁes to: (∑m
∑k∈Ile f t ck
∑k∈Iright ck
= cgyk(rg) = cg, which just says
that the output value of the gate is equal to the product of its
inputs, the very deﬁnition of a multiplication gate.

(cid:17)
k=1 ck · vk(rg)) · (∑m

(cid:17)·(cid:16)

(cid:16)

In short, the divisibility check that t(x) divides p(x) de-
composes into deg(t(x)) separate checks, one for each gate g
and root rg of t(x), that p(rg) = 0.

Taking the circuit in Figure 2 as a concrete example, we
build the equivalent QAP as follows. First, we select two
roots, r5,r6 ∈ F to represent the two multiplication gates.
Hence the QAP’s degree is 2. We deﬁne six polynomials for
each set V , W , and Y , four for the input wires, and two for
the outputs from the multiplication gates. Thus, the QAP’s
size is 6. We deﬁne these polynomials based on each wire’s
contributions to the multiplication gates. Speciﬁcally all of
the vk(r5) = 0, except v3(r5) = 1, since the third input wire
contributes to the left input of c5’s multiplication gate. Simi-
larly, vk(r6) = 0, except for v1(r6) = v2(r6) = 1, since the ﬁrst
two inputs both contribute to the left input of c6’s gate. For
W , we look at right inputs. Finally, Y represents outputs;
none of the input wires is an output, so yk(r5) = yk(r6) = 0
for k ∈ {1, ...,4}, and y5(r5) = y6(r6) = 1.

Note the extreme sparsity of the polynomials in the exam-
ple (in terms of evaluations of the polynomials). The VC pro-
tocol (§2.3) exploits this sparseness to achieve efﬁciency.

The actual construction [30] is a bit more complex, as it
handles addition and multiplication by constants. Nonethe-
less, GGPR show that for any arithmetic circuit with d mul-
tiplication gates and N I/O elements, one can construct an
equivalent QAP with degree (the number of roots rg) d and
size (number of polynomials in each set) d +N. Note that ad-
dition gates and multiplication-by-constant gates do not con-
tribute to the size or degree of the QAP. Thus, these gates are
essentially “free” in QAP-based VC schemes.
Strong QAPs. In their QAP-based VC scheme, described be-
low, GGPR unfortunately require a strong property from the
QAP. Note that Deﬁnition 2 only considers the case where the

+xxc(cid:31)c(cid:30)c(cid:29)c(cid:28)c(cid:27)c(cid:26)InputsOutputk=1 ck · wk(x)).

k=1 bk · wk(x))− (∑m

same set of coefﬁcients ci are applied to all three sets of poly-
nomials. GGPR additionally require the if-and-only-if condi-
tion in Deﬁnition 2 to hold even when different coefﬁcients
k=1 ck · vk(x)) ·
ai, bi, ci are applied – i.e., when p(x) = (∑m
k=1 ak · yk(x)). They show how to con-
(∑m
vert any QAP into a strong QAP that satisﬁes this stronger
condition. Unfortunately, this strengthening step increases
the QAP’s degree to 3d + 2N, more than tripling it. This in
turn, more than triples the cost of key generation, the size of
the evaluation key, and the worker’s effort to produce a proof.
2.2.2 Boolean Circuits and QSPs
Boolean circuits operate over bits, with bitwise gates for
AND, OR, XOR, etc. GGPR propose Quadratic Span
Programs (QSPs) as a custom encoding for Boolean cir-
cuits [30]. QSPs are superﬁcially similar to QAPs, but
because they only support Boolean wire values, they use
only two sets of polynomials V and W . The divisibility
k=1 ck · vk(x))·
check is updated to consider p(x) = (v0(x) + ∑m
(w0(x) + ∑m
Instead of the arithmetic circuit-
based polynomial construction above, QSPs build a small set
of polynomials for each Boolean gate. Speciﬁcally, each gate
adds 9 roots and 12 polynomials to the overall QSP. Like
QAPs, the QSPs require a strengthening step.
2.3 Building VC from Quadratic Programs
To construct a VC protocol from a quadratic program, the
main idea is that each polynomial – e.g., vk(x) ∈ F – of the
quadratic program is mapped to an element gvk(s) in a bilin-
ear group, where s is a secret value selected by the client,
g is a generator of the group, and F is the ﬁeld of discrete
logarithms of g. These group elements are given to the
worker. For a given input, the worker evaluates the circuit
directly to obtain the output and the values of the internal
circuit wires. These values correspond to the coefﬁcients ci
of the quadratic program. Thus, the VC worker can eval-
uate v(s) = ∑k∈[m] ck · vk(s) “in the exponent” to get gv(s);
it computes w(s) and y(s), in the exponent, similarly. Fi-
i=0 hi · xi,
nally, the worker computes h(x) = p(x)/t(x) = ∑d
and then uses the hi, along with gsi terms in the evaluation
key, to compute gh(s). To oversimplify, the proof consists of
(gv(s),gw(s),gy(s),gh(s)). The veriﬁer uses the bilinear map to
check that p(s) = h(s)t(s). The actual protocol (Protocol 1) is
a bit more complex, because additional machinery is needed
to ensure that the worker incorporates the client’s input u cor-
rectly, and that the worker indeed generates (say) v(s) in the
exponent as some linear function of the vk(s) values.
Protocol 1 (Veriﬁable Computation from strong QAPs)

• (EKF ,V KF ) ← KeyGen(F,1λ): Let F be a function
with N input/output values from F. Convert F into
an arithmetic circuit C; then build the corresponding
QAP Q = (t(x),V ,W ,Y ) of size m and degree d. Let
Imid = {N + 1, ...,m}, i.e., the non-IO-related indices.
Let e be a non-trivial bilinear map [34] e : G×G → GT ,
and let g be a generator of G.

Choose s,α,βv,βw,βy,γ R← F.
Construct the public evaluation key EKF as:
{gyk(s)}k∈[m],
( {gvk(s)}k∈Imid ,
{gαvk(s)}k∈Imid ,
{gαyk(s)}k∈[m],
{gβvvk(s)}k∈Imid , {gβwwk(s)}k∈[m], {gβyyk(s)}k∈[m]
{gsi}i∈[d],

{gwk(s)}k∈[m],
{gαwk(s)}k∈[m],
{gαsi}i∈[d]

).
The public veriﬁcation key is: V KF = (g1, gα, gγ, gβvγ,
gβwγ, gβyγ, gt(s),{gvk(s)}k∈[N], gv0(s), gw0(s), gy0(s)).
• (y,πy) ← Compute(EKF ,u): On input u, the worker
evaluates the circuit for F to obtain y ← F(u). As a
result of the evaluation, he knows the values {ci}i∈[m] of
the circuit’s wires.
He
such that
p(x) = h(x) · t(x)), and computes the proof πy as:

(the polynomial

for h(x)

solves

( gvmid (s),

gw(s),

gh(s),
gαvmid (s), gαw(s), gαy(s), gαh(s),
gβvv(s)+βww(s)+βyy(s)

gy(s),

),

where vmid(x) = ∑k∈Imid ck·vk(x), v(x) = ∑k∈[m] ck·vk(x),
w(x) = ∑k∈[m] ck · wk(x), and y(x) = ∑k∈[m] ck · yk(x).
Since these are linear equations, he can compute them
“in the exponent” using the material in the evaluation
key, e.g., gv(s) = gv0(s) · ∏k∈[m]

(cid:16)
gvk(s)(cid:17)ck.

• {0,1} ← Verify(V KF ,u,y,πy): To verify a proof, anyone
with access to the veriﬁcation key V KF can use the pair-
ing function e to check that the α and β proof terms are
correct (e.g., check that e(gvmid (s),gα) = e(gαvmid (s),g)).
This requires 8 pairings for the α terms, and 3 for the β
term.
Finally, the veriﬁer can compute a term representing
the I/O, u and y, by representing them as coefﬁcients
c1, . . . ,cN ∈ F and computing, using elements from V KF,
gvio(s) = ∏k∈[N]
A ﬁnal check (with 3 pairings) veriﬁes the divisibil-
that e(gv0(s) · gvio · gv(s),gw0(s) ·
ity requirement,
gw(s))/e(gy0(s) · gy(s),g) = e(gh(s),gt(s)).
In a designated veriﬁer setting (where the veriﬁer knows
s, α, etc.), pairings are only needed for this last check,
and the I/O term can be computed directly over F, rather
than “in the exponent”.

(cid:16)
gvk(s)(cid:17)ck.

i.e.,

Regarding efﬁciency, GGPR [30] show that the one-time
setup of KeyGen runs in time linear in the original circuit size,
O(|C|). The worker performs O(|C|) cryptographic work, but
he must also perform O(|C|log2|C|) non-cryptographic work
to calculate h(x). To achieve this performance, the worker
exploits the fact that the evaluation vectors (vk(r1), . . . ,vk(rd))
are all very sparse (also for the w and y polynomials). The
proof itself is constant size, with only 7 group elements for
QSPs and 9 for QAPs, though the veriﬁer’s work is still linear,
O(N), in the size of the inputs and outputs of the function.

In terms of security, GGPR [30] show this VC scheme is
sound under the d-PKE and q-PDH assumptions (see Ap-
pendix A), which are weak versions of assumptions in prior
work [21, 35, 36]. The q-PDH assumption belongs to a class

241

of cryptographic assumptions that do not lend themselves to
efﬁcient falsiﬁcation [37], though some members have indeed
been proven false [38]. Gentry and Wichs recently showed
that assumptions from this class are likely to be inherent for
efﬁcient, non-interactive arguments for NP relations [39].
Zero Knowledge. Making the VC scheme zero-knowledge
is remarkably simple. One simply includes the target poly-
nomial t(x) itself in the polynomial sets V , W , and Y . This
allows the worker to “randomize” its proof by adding δvt(s)
in the exponent to vmid(s), δwt(s) to w(s), and δyt(s) to y(s)
for random δv,δw,δy, and modifying the other elements of
the proof accordingly. The modiﬁed value of p(x) remains
divisible by t(x), but the randomization makes the scheme
statistically zero-knowledge [30].
3 Theoretical Reﬁnements
In this section, we improve Protocol 1 to signiﬁcantly reduce
key generation time, evaluation key size, and worker effort.
We provide a cost model for the new protocol elsewhere [40],
and we analyze our improvements empirically in §5.4.

Our main optimization is that we construct a VC scheme
that uses a regular QAP (as in Deﬁnition 2), rather than a
strong QAP. Recall that GGPR show how to transform a reg-
ular QAP into a strong QAP, but the transformation more than
triples the degree of the QAP. Consequently, when they plug
their strong QAP into their VC construction, the strengthen-
ing step more than triples the key generation time, evaluation
key size, and worker computation. We take a different ap-
proach that uses a regular QAP, and hence we do not need a
strengthening step at all. Instead, we embed additional struc-
ture into our new VC proof that ensures that the worker uses
the same linear combination to construct the v, w, and y terms
of its proof.2 Surprisingly, this additional structure comes at
no cost, and our VC scheme is actually less complicated than
GGPR’s! For example, we manage to shave the proof down
from nine group elements to eight. Experiments (§5.4) show
that these improvements indeed give substantial savings.

We also remove the need for the worker to compute gαh(s),
and hence the gαsi
i∈[d] terms from EK. Finally, we expand the
expressivity and efﬁciency of the functions QAPs can com-
pute by designing a number of custom circuit gates for spe-
cialized functions.
3.1 Our New VC Protocol
Next we describe our more efﬁcient VC scheme, with some
remarks afterwards on some its properties.
Protocol 2 (Veriﬁable Computation from regular QAPs)
• (EKF ,V KF ) ← KeyGen(F,1λ): Let F be a function
with N input/output values from F. Convert F into
an arithmetic circuit C; then build the corresponding

2Our proof contains a term that enforces this linear constraint without
increasing the degree. GGPR’s generic strengthening step checked the con-
sistency of the linear combinations via additional multiplication gates, which
increased the degree of the QAP.

QAP Q = (t(x),V ,W ,Y ) of size m and degree d. Let
Imid = {N + 1, ...,m}, i.e., the non-IO-related indices.
Let e be a non-trivial bilinear map [34] e : G×G → GT ,
and let g be a generator of G.
Choose rv,rw,s,αv,αw,αy,β,γ R← F and set ry = rv · rw,
gv = grv, gw = grw and gy = gry.
Construct the public evaluation key EKF as:
( {gvk(s)
{gyk(s)
}k∈[m],
}k∈[m], {gαyyk(s)
{gαvvk(s)
{gsi}i∈[d],
}k∈[m]
gβwk(s)
w

}k∈Imid ,
{gwk(s)
}k∈Imid , {gαwwk(s)
{gβvk(s)

}k∈[m],
}k∈[m],

y
gβyk(s))
y

w

w

v

y

v

v

),

v

y

, {gvk(s)

}k∈[N], gv0(s)

and the public veriﬁcation key as: V KF = (g1, gαv, gαw ,
gαy, gγ, gβγ, gt(s)
• (y,πy) ← Compute(EKF ,u): On input u, the worker
evaluates the circuit for F to obtain y ← F(u); he also
learns the values {ci}i∈[m] of the circuit’s wires.
He
such that
p(x) = h(x) · t(x)), and computes the proof πy as:

(the polynomial

for h(x)

, gw0(s)

, gy0(s)

solves

).

w

v

y

( gvmid (s)

,
v
gαvvmid (s)
v
gβw(s)
gβv(s)
w
v

gw(s)
w ,
, gαww(s)
w
gβy(s)
y

gy(s)
,
y
, gαyy(s)
y

gh(s),

),

where vmid(x) = ∑k∈Imid ck·vk(x), v(x) = ∑k∈[m] ck·vk(x)
w(x) = ∑k∈[m] ck · wk(x), and y(x) = ∑k∈[m] ck · yk(x).
• {0,1} ← Verify(V KF ,u,y,πy): The veriﬁcation of an al-
leged proof with elements gVmid , gW , gY , gH, gV(cid:48)
mid , gW(cid:48)
,
gY(cid:48)
, and gZ uses the public veriﬁcation key V KF and the
pairing function e to perform the following checks.
• Divisibility check for the QAP: using elements from
V KF compute gvio(s)
e(gv0(s)
gY
y ,g).
• Check that the linear combinations computed over V ,
W and Y are in their appropriate spans:
e(gV(cid:48)
midv

(cid:17)ck and check:

w ) = e(gt(s)
gW
y

,gH )e(gy0(s)

w ,g) = e(gW

w ,gαw ),

= ∏k∈[N]

e(gW(cid:48)

,gw0(s)

gvio(s)
v

,gαv),

gvk(s)
v

gVmid
v

(cid:16)

w

v

v

y

v

,g) = e(gVmid
e(gY(cid:48)

y ,g) = e(gY

y ,gαy).

• Check that the same coefﬁcients were used in each of
the linear combinations over V , W and Y :
y ,gβγ).

e(gZ,gγ) = e(gvio(s)

gW
w gY

gVmid
v

v

The correctness of the VC scheme follows from the prop-
erties of the QAP. Regarding security, we have the following:

Theorem 1 Let d be an upper bound on the degree of the
QAP used in the VC scheme, and let q = 4d + 4. The VC
scheme is sound under the d-PKE, q-PDH and 2q-SDH as-
sumptions (see Appendix A).
The proof of Theorem 1 is in Appendix B.

242

1 ), (g2,gα

Security Intuition. As intuition for why the VC scheme is
sound, note that it seems hard for an adversary who does
not know α to construct any pair of group elements h,hα ex-
cept in the obvious way: by taking pairs (g1,gα
2 ), . . .
that he is given, and applying the same linear combination
(in the exponent) to the left and right elements of the pairs.
This hardness is formalized in the d-PKE assumption, a sort
of “knowledge-of-exponent” assumption [41], that says that
the adversary must “know” such a linear combination, in the
sense that this linear combination can be extracted from him.
Roughly, this means that, in the security proof, we can extract
polynomials Vmid(x), W (x), Y (x) such that Vmid (from the
proof) equals Vmid(s), W =W (s) and Y =Y (s), and that more-
over these polynomials are in the linear spans of the vk(x)’s,
wk(x)’s, and yk(x)’s respectively. If the adversary manages to
provide a proof of a false statement that veriﬁes, then these
polynomials must not actually correspond to a QAP solution.
So, either p(x) is not actually divisible by t(x) (in this case we
break 2q-SDH) or V (x) = vio(x) +Vmid(x), W (x) and Y (x) do
not use the same linear combination (in this case we break
q-PDH because in the proof we choose β in a clever way).
Zero Knowledge. We can apply GGPR’s rerandomization
technique [30] (§2.3) to provide zero-knowledge for our new
veriﬁable computation construction. The worker chooses
R← F and in his proof, instead of the polynomials
δv,δw,δy
vmid(x), v(x), w(x) and y(x), he uses the following random-
ized versions vmid(x) + δvt(x), v(x) + δvt(x), w(x) + δwt(x)
and y(x) + δyt(x). In order to facilitate the randomization of
the proof we add the following terms to the evaluation key:
gαvt(s)
v
Performance. Our main improvement is that our VC scheme
only requires a regular QAP, rather than a strong QAP, which
improves performance by more than a factor of 3. Moreover,
the scheme itself is simpler, leading to fewer group elements
in the keys and proof, fewer bilinear maps for Verify, etc.

, gαwt(s)

, gαyt(s)

, gβt(s)

, gβt(s)

w

The scheme above assumes a symmetric bilinear map. In
practice, for performance reasons, we use an asymmetric bi-
linear map e : G1 × G2 → GT where G1 is an elliptic curve
group called the “base” curve, and G2 is the “twist” curve.
Operations over the base curve are about 3 times faster than
over the twist curve (§5.1). Due to our optimizations, while
the worker must compute the gw(s)
term over the twist curve,
all of the other proof terms can be over the base curve.
3.2 Expressive Circuit Constructions
The QAP that we use in our VC scheme is deﬁned over Fp,
where p is a large prime. We can, as explained above, de-
rive a QAP over Fp that efﬁciently computes any function F
that can be expressed in terms of addition and multiplication
modulo p. This provides no obvious way to express some op-
erations, such as a ≥ b using mod-p arithmetic. On the other
hand, given a and b as bits, comparison is easy. Hence, one
might infer that Boolean circuits are more general and thus
QSPs superior to QAPs.

, gβt(s)

v

y

.

w

y

w

However, we design an arithmetic split gate to translate an
arithmetic wire a ∈ Fp, known to be in [0,2k − 1], into k bi-
nary output wires. Given such binary values, we can compute
Boolean functions using arithmetic gates: NAND(a,b) =
1−ab, AND(a,b) = ab, OR(a,b) = 1− (1−a)(1−b). Each
embedded Boolean gate costs only one multiply.

Furthermore, the expression ∑k

Surprisingly, even though QSPs are “designed for”
Boolean circuits, the arithmetic embedding gives a more efﬁ-
cient VC scheme. With a QSP, each gate increases the degree
of t(x) by 9 and the QSP size by 12. Embedding introduces
an expensive initial gate that constrains each input to {0,1},
but henceforth, each embedded gate preserves the {0,1} in-
variant, adding only 1 to the degree and size of the QAP. 3
i=1 2i−1ai combines a bit-
wise representation of a back into a single wire. Because the
sum consists of additions and multiplications by constants,
recombination is free; it doesn’t increase the size of the QAP.
Below, we deﬁne a split gate as a standalone QAP which
can be composed [30, Thm.11] with other gates. In our full
paper [40], we design a gate that enforces wire equality and a
gate that checks whether a wire value is equal to zero.
Split Gate. Given input a ∈ Fp known to be in [0,2k − 1], the
split gate outputs k wires holding the binary digits a1, . . . ,ak
i=1 2i−1ai = a, and that
of a. Thus, the QAP ensures that ∑k
each ai is either 0 or 1. For convenience, we number the
output wires 1, . . . ,k and the input wire k + 1.
In our mini-QAP, let t(x) = (x − r)∏k
i=1(x − ri) where
for 1 ≤ i ≤ k, vk+1(r) = 0,
for 1 ≤ i ≤ k, wk+1(r) = 0,
for 1 ≤ i ≤ k, yk+1(r) = 1;

v0(r) = 0,
•
w0(r) = 1, wi(r) = 0
y0(r) = 0,
yi(r) = 0
• For 1 ≤ j ≤ k: v j(r j) = 1, vi(r j) = 0 for all i (cid:54)= j,
w0(r j) = 1, w j(r j) = −1, wi(r j) = 0 for all i (cid:54)= 0, j,
and yi(r j) = 0 for all i.

r,r1, . . . ,rk are distinct roots. We set:

k=1 ak · vk(x)) · (w0(x) + ∑m

k=1 ak · wk(x)) −
If (v0(x) + ∑m
k=1 ak · yk(x)) is divisible by t(x), it must evaluate
(y0(x) + ∑m
to 0 at r, and therefore the ﬁrst set of equations guarantee that
i=1 2i−1ai − a = 0. This guarantees that if all a1, . . . ,ak are
∑k
binary, then they are the binary digits of a. The second set of
equations guarantees that each ai is either 0 or 1. In particu-
lar, for each 1 ≤ j ≤ k, the above polynomial evaluates to 0 at
r j if and only if a j · (1− a j) = 0.
4 Implementation
We implemented a compiler that takes a subset of C to an
equivalent arithmetic circuit (§4.1). Our veriﬁable compu-
tation suite then compiles the circuit representation to the
equivalent QAP, and generates code to run the VC protocol,
including key generation, proof computation, and proof ver-
iﬁcation (§4.2). The toolchain compiles a large collection of
applications and runs them with veriﬁcation (§4.3). Source
code for the toolchain is available [40].

vi(r) = 2i−1

3QSPs still have smaller proofs, since they require only two sets of poly-

nomials (V ,W ) vs. three (V ,W ,Y ).

243

int mat[SIZE*SIZE] = { 0x12, ... };
struct In { int vector[SIZE]; };
struct Out { int result[SIZE]; };

void compute(struct In *input, struct Out *output){

int i, j, k, t;
for (i=0; i<SIZE; i+=1) {

int t=0;
for (k=0; k<SIZE; k+=1) {

t = t + mat->[i*SIZE+k] * input->vector[k];

}
output->result[i] = t;

}

}

Figure 3: Fixed-Matrix Multiplication. The qcc compiler unrolls
the loops and decodes the struct and array references to generate an
arithmetic expression for Out in terms of In.

4.1 Compiler Toolchain
The applications described below (§4.3) and evaluated in §5
are each compiled using qcc, our C-to-arithmetic-expression
compiler, a 3,525-line Python program [42]. They are also
compiled with gcc to produce the Native timings in Figures 7
and 8. A unit test validates that the gcc and 32-bit qcc exe-
cutables produce matching output on varying inputs.

The compiler understands a substantial subset of C, in-
cluding global, function and block-scoped variables; arrays,
structs, and pointers; function calls, conditionals, loops; and
static initializers (Fig. 3). It also understands arithmetic and
bitwise Boolean operators and preprocessor syntax. The pro-
gram’s entry point is a function
void compute(struct In *in, struct Out *out)
whose parameters identify the set of input and output values.
Since the “target machine” (arithmetic circuits) supports
only expressions, not mutable state and iteration, we re-
strict the C program’s semantics accordingly. For example,
pointers and array dereferences must be compile-time con-
stants; otherwise, each dynamic reference would produce
conditional expressions of size proportional to the address-
able memory. Function calls are inlined, while preserving C
variable scope and pointer semantics.

Imperative conditionals compile to conditional expressions
that encode the imperative side effects. Static conditions are
collapsed at compile time. Similarly, loops with statically-
evaluatable termination conditions are automatically unrolled
completely. A loop with dynamic termination—depending
on an input value—requires a pragma unroll to inform
the compiler how far it should unroll.

The only scalar type presently supported is int; a compiler
ﬂag selects the integer size. The compiler inserts masking
expressions to ensure that a k-bit int behaves exactly as the
corresponding C type, including overﬂow. As described be-
low, our arithmetic circuits operate over a 254-bit ﬁeld; if the
program’s computation is known not to overﬂow 254 bits, the
programmer can disable masking with a compiler ﬂag. We
plan to extend our compiler to support ﬂoating point values
via standard techniques [28, 43].

These features (and limitations) are similar to a parallel ef-
fort [44] to compile C for the purposes of secure multiparty
computation, though they compile only to Boolean circuits.
Details. The compiler front-end tracks scopes and variable
values (as expressions), and unrolls imperative execution into
a ﬁnal program state that provides expressions for each output
value. The intermediate language is a set of expressions of C-
like operators, such as +, *, <=, ?:, &, and ˆ.

The compiler back-end expands each expression into the
arithmetic gate language of mul, add, const-mul, wire-split,
etc., eliminating common subexpressions. It carefully bounds
the bit-width of each wire value:

bit width 4);

• inputs have the compiler-speciﬁed int width;
• each constant has a known width (e.g. 13 = 11012 has
• a bitwise op produces the max of its arguments’ widths;
• add can produce max+1 bits (for a carry); and
• mul can produce 2· max bits.

When the width nears the available bits in the crypto ﬁeld
(254), the compiler generates a split operation to truncate the
value back to the speciﬁed int width. Tracking bit width min-
imizes the cost of split gates.

Signed numbers are handled just as they are in C: a 32-
bit int is a twos-complement number with a sign bit at 1 <<
31. Each C expression value is treated either as an arithmetic
scalar wire or a Boolean expansion, e.g. 32 wires in {0,1}
(§3.2). The format is translated only when values pass from
one operator type to the other; for example, in (aˆbˆc) +
z, the bitwise xor (ˆ) operators manipulate bitwise Booleans,
which are joined into a scalar for the addition +.
4.2 Quadratic Programs and Cryptographic Protocol
The next pipeline stage accepts a Boolean or arithmetic cir-
cuit and builds a QSP or QAP (§2). Then, per §3.1, it com-
piles the quadratic program into a set of cryptographic rou-
tines for the client (key generation and veriﬁcation) and the
worker (computation and proof generation). For comparison,
we also implement the original GGPR [30]; §5.4 shows that
Pinocchio’s enhancements reduce overhead by 18-64%.

The key-generation routine runs at the client, with se-
lectable public veriﬁcation and zero-knowledge features
(§5.3). The code transmits the evaluation key over the net-
work to the worker; to save bandwidth, the program transmits
as C and the worker compiles it locally.

The computation routine runs at the server, collecting input
from the client, using the evaluation key to produce the proof,
and transmitting the proof back to the client (or, if desired, a
different veriﬁer). The veriﬁcation routine uses the veriﬁca-
tion key and proof to determine whether the worker attempted
to cheat.

Our cryptographic code is single-threaded, but each stage
is embarrassingly parallel. Prior work [28] shows that stan-
dard techniques can parallelize work across cores, machines,
or GPUs.
For the cryptographic code, we use a high-
speed elliptic curve library [45] with a 256-bit BN-curve [46]

244

that provides 128 bits of security. The quadratic-program-
construction and protocol-execution code is 10,832 lines of C
and C++ [42].
4.2.1 Optimizing Operations
We summarize some of the key optimizations we imple-
mented, as well as lessons learned.
Faster Exponentiation. Generating the evaluation key EK
requires exponentiating the same base g to many different
powers. We optimize this operation by adapting Pippenger’s
multi-exponentiation [47] algorithm for use with a single
base. Essentially this means that we build a table of inter-
mediate powers of g, allowing us to compute any particular
exponent with only a few multiplications.

1g1

2,g1

1g0

1g0

2,g1

2,g0

In a similar vein, the worker’s largest source of overhead is
applying the coefﬁcients from the circuit “in the exponent” to
compute gY (s) etc. Here Pippinger’s algorithm is less directly
useful, since the worker does a handful of such operations
for a given work instance, but each operation involves hun-
dreds of thousands of different bases, i.e., given g1, . . . ,gm,
e1, . . . ,em, for very large m, the worker needs to compute
∏i gei
i . To optimize this operation, we use a sliding-window
technique to build a small table of powers for each pair of
bases. For example, for the ﬁrst two bases, with a window of
size 1, we compute {g0
2}. In this case, the
1g1
table only requires one multiply to build. We can then con-
sider the high order bit of both e1 and e2; together these bits
select one of four values in our table; we multiply that value
into our accumulator and proceed to the next pair of bases.
After all bases have been considered, we square the accumu-
lator (to “move” the portion of the exponent we’ve computed
into the next higher “slot”), and then repeat. In practice, these
tables can save 3-4x, even counting the time to build the ta-
bles in the ﬁrst place.
Polynomial Asymptotics. To generate a proof, the worker
must compute the polynomial h(x) such that t(x)·h(x) = P(x)
(§2). Since we store P(x) in terms of its evaluations at the
roots of the quadratic program (recall Figure 2), the worker
must ﬁrst interpolate to ﬁnd P(x) and then perform a polyno-
mial division to arrive at h(x).

Note that all of these computations take place in a normal
ﬁeld, whereas all of the worker’s other steps involve crypto-
graphic operations, which §5.1 shows are about three orders
of magnitude more expensive.

Thus, one might na¨ıvely conclude, as we did, that simple
polynomial algorithms, such as Lagrangian interpolation and
“high-school” polynomial multiplication, sufﬁce. However,
we quickly discovered that the O(n2) behavior of these algo-
rithms, at the scale required for veriﬁable computing, dwarfed
the linear number of cryptographic operations (§5.1). Hence
we implemented an FFT-based O(nlogn) polynomial mul-
tiplication library and used a polynomial interpolation algo-
rithm [48] that builds a binary tree of polynomials, giving
total time O(nlog2 n). Even so optimized, solving for h(x) is
the second largest source of worker overhead.

245

Preparing for the Future; Learning from the Past. In our
implementation and evaluation, we assume a worst case sce-
nario in which the client decides, without any warning, to
outsource a new function, and similarly that the worker only
ever computes a single instance for a given client. In prac-
tice, neither scenario is plausible. When the client ﬁrst installs
Pinocchio, the program, could, in theory, build the single base
exponent table discussed above. Further, it can choose a ran-
dom s and begins computing powers of s in the background,
since these are entirely independent of the computation.

Similarly, if the worker performs more than a single com-
putation for the client, he can hold onto the exponentiation
tables he built for the ﬁrst computation and save substantial
time on subsequent computations. He can also save the poly-
nomial tree used to accelerate the computation of h(x). None
of these values have any secret information, so workers could
potentially even share this information amongst themselves.
Working With Elliptic Curves. Our BN curve is deﬁned by
the equation y2 = x3 + b, in that each group element gi is a
point (x,y) on the curve. To speed operations, while comput-
ing on elliptic curve points, we represent them in projective
form, as three coordinates (x,y,z), which corresponds to the
afﬁne point (x/z2,y/z3). This is analogous to representing
a rational number as an integral numerator and denominator.
Projective coordinates reduce EC operation costs by ∼60%.
We save space in the cryptographic keys and proof by con-
verting points back to afﬁne form before storing or transmit-
ting them. Furthermore, rather than store (x,y), we store x
and a bit indicating which square root of x3 + b to use for y,
reducing key and proof size by another 50%.
4.3 Applications
Our system runs several applications, each of which can be
instantiated with some static parameters, and then each in-
stance can be executed with dynamic inputs.
Fixed Matrix multiplies an n × n matrix parameter M by
an n-length input vector A, and outputs the resulting n-length
vector M · A. We choose ﬁve parameter settings that range
from |M| = 200× 200 to |M| = 1000× 1000.
Two Matrices has parameter n, takes as input two n × n
matrices M1 and M2, and outputs the n× n matrix M1 · M2.
Matrix operations are widely used, e.g., in collaborative ﬁl-
tering [49]. (|M| = 30× 30 to |M| = 110× 110)

MultiVar Poly evaluates a k-variable, m-degree multivariate
polynomial. The (m + 1)k coefﬁcients are parameters, the k
variables x1, . . . ,xk are the inputs, and the polynomial’s scalar
value is the output. (k = 5, m = 6, 16,807 coeff. to k = 5,
m = 10; 644,170 coeff.)
Image Matching is parameterized by an iw × ih rectangular
image and parameters kw,kh. It takes as input a kw × kh im-
age kernel, and outputs the minimum difference and the point
(x,y) in the image where it occurs. (iw × ih = 25, kw × kh = 9
to iw × ih = 2025, kw × kh = 9)

Shortest Paths implements the Floyd-Warshall O(n3) graph
algorithm, useful for network routing and matrix inversion.

Op
Fixed Base Exp (naive)
Fixed Base Exp (opt)
Multi Exp, 254-bit exp (naive)
Multi Exp, 254-bit exp (opt)
Multi Exp, 32-bit exp (opt)
Multi Exp,
1-bit exp (opt)
Compress
Decompress
Pairing
Field Add
Field Mul

Base Curve
318.5µs
38.2µs
318.5µs
104.5µs
14.9µs
9.5µs
30.2µs
27.0µs

Twist Curve
1221.4µs
118.3µs
1221.5µs
401.0µs
56.8µs
36.4µs
2160.9µs
2168.3µs

0.9ms
50.2ns
361.1ns

Figure 4: Microbenchmarks. Breakdown of the main sources of
performance overhead in the larger protocol. (N = 100,σ ≤ 1%).

Poly Interp (naive)
Poly Interp (opt)
Poly Mul (naive)
Poly Mul (opt)

10
0.5ms
1.1ms
0.1ms
0.1ms

Degree
100
238.3ms
21.1ms
8.6ms
0.4ms

1000
202013.1ms
331.1ms
799.7ms
4.1ms

Figure 5: Cost of Polynomial Operations. Illustrates the impor-
tance of optimizing polynomial algorithms. (N = 500,σ ≤ 5%).

Its parameter n speciﬁes the number of vertices, its input is
an n× n edge matrix, and its output is an n× n matrix of all-
pairs shortest paths. (n = 8, e = 64 to n = 24, e = 576)

LGCA is a Lattice-Gas Cellular Automata implementation
that converges to Navier-Stokes [50]. It has parameter n, the
ﬂuid lattice size, and k, the iteration count. It inputs one n-cell
lattice and outputs another reﬂecting k steps. (n = 294, k = 5
to n = 294, k = 40)

SHA-1 has no parameters. Its input is a 13-word (416-bit)
input string, and it outputs the 5-word (160-bit) SHA-1 hash
of the input.
5 Evaluation
We experiment on a Lenovo X201 ThinkPad. We run on a
single core of a 2.67 GHz Intel Core i7 with 8 GB of RAM.
Pinocchio’s results use QAPs, since theory (§3.2) and practice
(§5.5) show they offer superior performance.
5.1 Microbenchmarks
We performed a series of microbenchmarks to quantify the
basic cost units of our protocol (Fig. 4). Field operations are
about three orders of magnitude cheaper than cryptographic
exponentiations or multiplications. As §3.1 explained, we use
an asymmetric pairing function, meaning that some group el-
ements live on a (relatively) cheap base curve, while others
live on the “twist” curve. Operations on the latter are 3-4×
as expensive, reinforcing the importance of our optimizations
to the VC protocol to move as many operations as possible
to the base curve. Ultimately, Pinocchio’s protocol requires
only the W polynomials to operate on the twist curve; all
other operations take place on the base curve.

(a) Per-Instance Veriﬁcation Latency

(b) Worker Latency

Figure 6: Performance Relative to Prior Schemes. Pinocchio
reduces costs by orders of magnitude (note the log scale on the y-
axis). We graph the time necessary to (a) verify and (b) produce a
proof result for multiplying two NxN matrices.

Figures 4 and 5 also show the impact of the exponentiation
and polynomial optimizations described in §4.2.1, which re-
duce costs by two to three orders of magnitude for polynomial
operations, and factors of 3-10 for exponentiations.

5.2 Comparison With Previous Work

Figure 6 plots Pinocchio’s performance against that of pre-
vious systems. We use the multiplication of two matrices as
our test application since it has appeared in several prior pa-
pers [25, 27], though simpler, non-cryptographic veriﬁcation
procedures exist [51, §7.1]. Since all of these prior schemes
are designated veriﬁer, we measure against Pinocchio’s des-
ignated veriﬁer mode.

We compare against 1) a na¨ıve version of a PCP-based
scheme [52]; 2) GGP [22], an early scheme that deﬁned veri-
ﬁable computation, but which relies on fully-homomorphic-
encryption (FHE); 3) Pepper [27], an optimized reﬁnement
of (1); and 4) Ginger [28], a further reﬁnement of Pepper.
We omit results from a separate PCP-based effort [25, 26],
since Ginger’s performance dominates it [28]. See Section 6
for more details on these schemes and the tradeoffs between
them. Since most of these schemes are ridiculously imprac-
tical, we model, rather than measure, their performance. For
GGP, we built a model of its performance based on the latest
performance results for FHE [53], while for the others, we
used previously published models [27, 28]. For Pinocchio,
however, we use real numbers from our implementation.

Figure 6 shows that Pinocchio continues the recent trend of
reducing costs by orders of magnitude. Early PCP and FHE-
based schemes are laughably impractical, taking hundreds to
trillions of years to produce or verify a single proof. Pepper
and Ginger have made huge improvements over prior work,
but, as we discuss in more detail in §6, they do not offer public
veriﬁcation or zero knowledge.

In addition to offering new properties, Pinocchio signif-
icantly improves performance and security. The systems
shown in Figure 6 amortize setup work across many work

246

255075100Matrix Dimension (NxN)10-310-1101103105107109101110131015101710191021Time (s)PCPsGGPPepperGingerPinocchio255075100Matrix Dimension (NxN)100102104106108101010121014101610181020Time (s)a parameterized app, and each point represents a particular
parameter setting. Our key ﬁnding is that, for sufﬁciently
large parameters, three apps cross the line where outsourc-
ing makes sense; i.e., verifying the results of an outsourced
computation is cheaper than local native execution.

On the downside, the other three apps, while trending in
the right direction, fail to cross the outsourcing threshold.
The difference is that these three apps perform large numbers
of inequality comparisons and/or bitwise operations. This
makes our circuit-based representation less efﬁcient relative
to native, and hence on our current experimental platform, we
cannot push the application parameter settings to the point
where they would beat local execution. Nonetheless, these
applications may still be useful in settings that require Pinoc-
chio’s zero-knowledge proofs.

Fortunately, additional experiments show that enabling
zero-knowledge proofs adds a negligible, ﬁxed cost to key
generation (213µs), and re-randomizing a proof to make it
zero-knowledge requires little effort (e.g., 300ms or 0.1% for
the multivariate polynomial app).

Figure 8 provides more details of Pinocchio’s performance.
For KeyGen, our experiments conservatively assume that the
client does no precomputation in anticipation of outsourcing
a function, and for Compute, we assume that the worker only
does a single work instance before throwing away all of its
state. As discussed in §4.2.1, in practice, we would take
advantage of both precomputation and caching of previous
work, which on average saves at least 43% of the effort for
KeyGen and 16% of the effort for Compute.

In Figure 8, we see again that three apps (starred) beat
native execution, including one in the public veriﬁer setting
(which requires more expensive operations per IO). The data
also reinforces the point that using a circuit representation
imposes a signiﬁcant cost on image matching, shortest paths,
and the lattice gas sim relative to native, suggesting a tar-
get for optimization. Relative to the circuit representation,
Pinocchio’s veriﬁcation is cheap: both the public and the des-
ignated veriﬁer “win” most of the time when compared to the
circuit execution. Speciﬁcally, the designated veriﬁer wins
in 12 of 13 (92%) application settings. Public veriﬁcation is
more expensive, particularly for large IO, but still wins in 11
of 13 (85%) settings.

Since Pinocchio offers public veriﬁcation, some clients
will beneﬁt from the KeyGen work of others, and hence only
care about the veriﬁcation costs. For example, a cellphone
carrier might perform the one-time KeyGen so that its cus-
tomers can verify computations done by arbitrary workers.

However, in other settings, e.g., a company outsourcing
work to the cloud, the key generator and veriﬁer may be the
same entity, and will wish to amortize the cost of key gen-
eration via the savings from veriﬁcation. Figure 8 shows
that most apps have a low “break even” point vs. circuit ex-
ecution:
the median for the designated veriﬁer is 550 in-
stances and for public veriﬁer is 525 instances,5 both with
5The public veriﬁer’s median is lower, since it wins on fewer app settings.

Figure 7: Cost of Veriﬁcation Vs. Local. Veriﬁcation must be
cheaper than native execution for outsourcing to make sense, though
for applications that want zero-knowledge, more expensive veriﬁca-
tion may be acceptable. All apps trend in the right direction, and
three apps cross the plane where veriﬁcation is cheaper than na-
tive. Error bars, often too small to see, represent 95% conﬁdence
intervals (N = 10,σ ≤ 5%).
instances,4 but the characteristics of the amortization differ.
To reach a break-even point, where the client does less work
verifying than performing the work locally, Pepper and Gin-
ger must batch work instances, whereas GGP and Pinocchio
must perform enough instances to amortize key setup costs.
These approaches have very different effects on latency. A
client cannot beneﬁt from Pepper or Ginger until it has accu-
mulated an entire batch of instances. In Pinocchio, key setup
can be precomputed, and henceforth every instance (includ-
ing the ﬁrst one) enjoys a better-than-break-even latency. Fig-
ure 6 shows the minimum latency achievable by each system.
Compared with Ginger, Pinocchio’s veriﬁer is ∼125,000× -
22,000,000× faster, and the worker is 19-60× faster. To im-
prove performance, Ginger’s parameters are chosen such that
the probability that the adversary can successfully cheat can
be as high as 1
220 [28, Figure 2], while in Pinocchio, the prob-
ability is roughly 1
5.3 End-to-End Application Performance
We measure Pinocchio’s performance for the applications and
parameter settings described in Section 4.3. All applications
are written in C and compile to both QAPs and to native exe-
cutables. We measure performance using 32-bit input values,
so we can compare against the native C version. This ob-
viously makes things more challenging for Pinocchio, since
Pinocchio operates over a 254-bit ﬁeld using multi-precision
integers, whereas the local execution uses the CPU’s native
32-bit operations.

2128 .

Figure 7 plots Pinocchio’s veriﬁcation time against the
time to execute the same app natively; each line represents

4 In contrast, Pinocchio’s public veriﬁer (not shown) enables a client to

beneﬁt from a third party’s key setup work.

247

0246810121416Native Execution (ms)0246810121416QAP DV Verify (ms)Cheaper to verifythan execute locallyShortest PathsImage MatchingLattice Gas SimTwo MatricesMultiVar PolyFixed Matrix0.00.10.20.30.40.59.09.510.010.511.011.5(MB)

Fixed Matrix, Medium
Fixed Matrix, Large
Two Matrices, Medium
Two Matrices, Large
MultiVar Poly, Medium
MultiVar Poly, Large
Image Matching, Medium
Image Matching, Large
Shortest Paths, Medium
Shortest Paths, Large
Lattice Gas Sim, Medium
Lattice Gas Sim, Large
SHA-1

|IO|
600
1,201
1,000
2,001
14,701
347,900
36,301 1,343,100
203,428
571,046
86,345
277,745
366,089
1,153 1,400,493
144,063
283,023
23,785

7
7
13
13
513

21
21
22

Mult KeyGen Compute Verify (ms) Circuit Native EvalKey VerKey Proof
(B)
Gates Pub (s)
288
0.7
288
1.5
288
80.1
299.7
288
288
41.9
288
127.3
288
26.2
67.0
288
288
85.1
288
346.0
288
38.4
288
75.3
11.9
288

Priv
(s) Pub
8.2
11.2
0.4
*8.3
13.4
0.9
9.7
267.1 141.9
1128.7 368.8 *10.9
10.6
245.5
713.0 *12.7 *10.9
9.7
40.6
10.5
145.0
10.3
197.2
10.9
852.2
9.6
76.7
9.7
164.1
9.4
14.8

(KB)
37.9
0.3
62.9
0.5
97.9
459.8
374.8 1134.8
0.6
55.9
0.6
156.8
0.8
23.6
75.8
0.8
16.4
99.6
36.4
381.4
1.1
39.6
1.1
77.7
6.5
1.1

(ms)
124.7
345.6
125.5
519.8
92.3
265.7
5.5
18.0
19.2
70.5
92.2
177.1
17.4

(ms)
4.3
12.4
4.0
15.5
4.5
12.9
0.1
0.4
0.1
0.3
0.2
0.4
0.0

12.5

11.2
12.0
13.2
16.1
10.8
10.8
8.1

Figure 8: Application Performance. Pinocchio’s performance for a sampling of the parameter settings (§4.3). All programs are compiled
directly from C. Private KeyGen is always within 0.4% of public KeyGen, and so is omitted. Veriﬁcation values in bold indicate veriﬁcation
is cheaper than computing the circuit locally; those with stars (*) indicate veriﬁcation is cheaper than native execution. (N = 10,σ ≤ 5%).

a low of 4 instances for ﬁxed matrix. Every instance after-
wards is a net “win”, even for the key generator. The median
break-even points are higher (64K and 321K) when compared
against native execution, since the outsourcing margin is rel-
atively small for the current parameter settings. Larger pa-
rameter settings, improved veriﬁcation techniques, or com-
paring against a big integer library instead of native execu-
tion [27, 28] would all bring these values down.

Figure 8 holds more good news for Pinocchio:

the keys
it generates are reasonably sized, with the evaluation key
(which describes the entire computation) typically requiring
10s or 100s of MB. The weak veriﬁer’s key (which grows lin-
early with the I/O) is typically only a few KB, and even at its
largest, for two-matrix multiplication, it requires only slightly
more than 1 MB. This suggests that the keys are quite portable
and will not require excessive bandwidth to transmit.

Finally, from the client’s perspective, if the worker’s efforts
are free, then the worker’s additional overhead of generating a
proof is irrelevant, as long as it doesn’t hurt response latency.
Our results, combined with prior work on parallelization [28],
suggest that latency can be brought down to reasonable levels,
given enough hardware. And indeed in high-assurance sce-
narios, scenarios where the client is incapable of performing
the calculation itself (e.g., a power-limited device), or scenar-
ios where the worker’s resources are otherwise idle, the client
may very well view the worker as “free”.

However, in other scenarios, such as cloud computing, the
worker’s efforts are not free. Even here, however, Chen and
Sion estimate that the cost of cloud computing is about 60×
cheaper than local computing for a small enterprise [54]. This
provides an approximate upper-bound for the amount of extra
work we should be willing to add to the worker’s overhead.
While we do not yet achieve this bound, we make substantial
progress on reducing the worker’s overhead, and the progress
shown in Figure 6(b) gives us hope.

KeyGen

Build table
Encode powers of s
Eval polys at s
Encode polys

Compute

Solve for h(x)
Apply coefﬁcients

Verify

Process I/O
Crypto checks

Evaluation Key Size
Veriﬁcation Key Size
Proof Size

GGPR [30]
108.7s
7.8s
28.4s
5.0s
67.2s
691.4s
252.3s
391.1s
15.2ms
456.5µs
14.8ms
105.5MB
640B
352B

This Paper
41.9s
7.9s
4.7s
1.7s
27.4s
245.5s
76.1s
154.5s
12.5ms
319.1µs
12.2ms
55.9MB
640B
288B

Reduction
61%
-2%
83%
65%
59%
64%
70%
60%
18%
30%
18%
47%
0%
18%

Figure 9: Improving GGPR [30]. Performance for the multi-
variate polynomial application. Pinocchio’s high-level operations
are 2.6x, 2.8x, and 1.2x faster than the original. (N = 10,σ ≤ 3%).

Impact of Our Optimizations

5.4
In Figure 9, we break down Pinocchio’s protocol overhead for
the large multivariate polynomial example application, to bet-
ter identify the major bottlenecks. For comparison, we also
measure the performance of our implementation of GGPR’s
scheme [30], using the same underlying cryptographic and
polynomial libraries.

The results indicate that our protocol improvements had a
signiﬁcant impact. KeyGen and Compute are more than twice
as fast, and even veriﬁcation is 18% faster. Of Pinocchio’s re-
maining KeyGen overhead, the majority comes from encod-
ing the evaluations of the QAP’s polynomials in the gener-
ator’s exponent. For Compute, the multi-exponentiation re-
quired to compute the QAP’s polynomials in the exponent
still dominate, but the overhead of solving for h(x) is non-
trivial as well.

248

Pinocchio also drastically reduces the size of the evaluation
key and even manages to reduce the size of GGPR’s already
svelte 9 element proof to 8 elements.
5.5 QSPs versus QAPs
Finally,
to conﬁrm our theoretical prediction that QAPs
would outperform QSPs (§3.2), we compared the two on our
SHA-1 application, which performs numerous bitwise oper-
ations, and hence should favor QSPs. The resulting QSP’s
size was 38.6× that of the QAP, and the degree was 55.5×
as large. Not surprisingly, the QSP’s KeyGen took 35.2× and
Compute took 55.4× as long as those of the QAP; the veriﬁ-
cation times were comparable.
6 Related Work
Much of the prior work in this area focuses on verifying spe-
ciﬁc functions via auditing or special properties of the func-
tions [2–6]. Other systems rely on replication, and hence as-
sume failures are uncorrelated [1, 7, 8, 55]. A large body of
work veriﬁes computation by assuming the worker employs
secure hardware [9–15].

While the theory and cryptography community has long
studied the problem of general-purpose proof systems [16–
23], until recently, this work was largely regarded as highly
impractical, to the point where no one bothered to implement
it. Much of this work [16–20, 56] relied on Probabilisti-
cally Checkable Proofs (PCPs), which offer impressive the-
oretical performance, but which can take trillions of years to
verify in practice [27]. Other work [22, 23] relies on fully-
homomorphic encryption (FHE) [24], which, despite contin-
uing advances [53], remains highly impractical.

Recently, security and systems researchers have started to
develop techniques to make theoretical cryptographic proto-
cols practical. Secure multiparty computation, for example,
has seen tremendous progress [57–59]. However, since the
primary focus is on secrecy, not outsourcing, both parties typ-
ically perform work equal to evaluating the function.

With regard to implementing veriﬁed computation, in the
last year, two parallel efforts have emerged, both using opti-
mized PCP-based arguments. One effort [25, 26] builds on
the PCP-based arguments of Goldwasser et al. [20] (GKR).
They target a streaming setting where the client cannot store
all of the data it wishes to compute over; the system currently
requires the function computed to be highly parallelizable.
On the plus side, it does not require cryptography, and it is
secure against computationally unbounded adversaries.

Setty et al. produced a second line of PCP-based systems
called Pepper [27] and Ginger [28]. They build on a particular
type of PCP called a linear PCP [52], in which the proof can
be represented as a linear function. This allows the worker
to use a linearly-homomorphic encryption scheme to create a
commitment to its proof while relying only on standard cryp-
tographic assumptions. Through a combination of theoretical
and systems-level improvements, this work made tremendous
progress in making PCP-based systems practical. Indeed, for

applications that can tolerate large batch sizes, the amortized
costs of veriﬁcation can be quite low.

A few downsides remain, however. Because the work
builds on the Hadamard PCP [56], the setup time, network
overhead, and the prover’s work are quadratic in the size of
the original computation, unless the protocol is hand-tailored.
To achieve efﬁciency, the veriﬁer must outsource computa-
tions in batches, which means it cannot verify the results un-
til the full batch returns. The scheme is designated veriﬁer,
meaning that third parties cannot verify the results of out-
sourced computations without sharing the client’s secret key,
and hence opening the possibility for fraud. The scheme also
does not support zero-knowledge proofs.

Concurrent work [60] also builds on the quadratic pro-
grams of Gennaro et al [30]. They observe that QAPs can be
viewed as linear PCPs and hence can ﬁt into Ginger’s crypto-
graphic framework [28]. Their work shows worker computa-
tion improvements similar to those of Pinocchio. Additional
concurrent work [61] adapts GKR’s protocol to the batching
model and develops a compiler that chooses amongst three
PCP-based backends. Both systems retain PCPs and Ginger’s
cryptographic protocol, so they rely on simpler cryptographic
assumptions than Pinocchio, but they must still batch compu-
tations to obtain an efﬁcient veriﬁer. They also remain desig-
nated veriﬁer and do not support zero-knowledge proofs.

Previous systems either did not offer a compiler [25–27], or
compiled from a subset of an academic language, SFDL [28,
60]. In contrast, we compile from a subset of C, which should
ease the development burden for verifying computation.

Several systems provide compilers for zero-knowledge
(ZK) proofs [62–64]. Both the systems of Almeida et al. [62]
and Meiklejohn et al. [63] adopt an approach based on Σ-
protocols [65]. The former provides functionality for prov-
ing knowledge in arbitrary groups, AND and OR composi-
tions, and linear relations. The latter focuses on functionali-
ties for cryptographic protocols, e.g., e-cash, blind signatures,
or veriﬁable encryption. The compiler of Backes et al. [64]
uses Groth-Sahai ZK proofs [66] and handles logical formu-
las. Rial and Danezis [32] propose a system for privacy-
preserving smart metering in which clients use a ZK protocol
to prove correctness of the billing computation they perform
on meter readings. In general, these systems are likely to ex-
hibit better performance than Pinocchio for their particular
subset of functionality, but they do not possess the same level
of efﬁcient generality.
7 Conclusion and Future Work
We have presented Pinocchio, a system for public veriﬁ-
able computing. Pinocchio uses quadratic programs, a new
method for encoding computation, combined with a highly
efﬁcient cryptographic protocol to achieve both asymptotic
and concrete efﬁciency. Pinocchio produces 288-byte proofs,
regardless of the size of the computation, and the proofs can
be veriﬁed rapidly, typically in tens of milliseconds, beating
native execution in several cases. This represents ﬁve to seven

249

orders of magnitude performance improvement over prior
work. The worker also produces the proof 19-60× faster.
Pinocchio even slashes the cost of its underlying protocol,
cutting the cost of both key and proof generation by more
than 60%. The end result is a natural cryptographic protocol
for efﬁciently signing computations. Combined with a com-
piler for real C programs, Pinocchio brings veriﬁable compu-
tation much closer to practicality.

Nonetheless gaps still remain. We hope that additional the-
oretic improvements, combined with efforts to expand our
toolchain, e.g., to support ﬂoating point or parallel execu-
tion (via standard techniques [25, 28, 43]), will continue to
advance us towards truly practical veriﬁable computing.
Acknowledgements
The authors gratefully thank: Peter Montgomery, Michael
Naehrig, and Patrick Pierola for assisting us with the crypto-
graphic library used by Pinocchio; Chris Hawblitzel for his
sage guidance on compiler development; Rosario Gennaro
for valuable discussions; and the anonymous reviewers for
their helpful comments. Mariana Raykova was supported by
NSF Grant No. 1017660.
References
[1] D. P. Anderson, J. Cobb, E. Korpela, M. Lebofsky, and D. Werthimer,
“SETI@Home: An experiment in public-resource computing,” Com-
munications of the ACM, vol. 45, no. 11, 2002.

[2] F. Monrose, P. Wyckoff, and A. Rubin, “Distributed execution with

remote audit,” in Proc. of ISOC NDSS, 1999.

[3] P. Golle and S. G. Stubblebine, “Secure distributed computing in a
commercial environment,” in Proc. of Financial Cryptography, 2002.
[4] W. Du and M. T. Goodrich, “Searching for high-value rare events with

uncheatable grid computing,” in ACNS, 2005.

[5] P. Golle and I. Mironov, “Uncheatable distributed computations,” in

Proc. of CT-RSA, 2001.

[6] R. Sion, “Query execution assurance for outsourced databases,” in The

Very Large Databases Conference (VLDB), 2005.

[7] M. Castro and B. Liskov, “Practical Byzantine fault tolerance and

proactive recovery,” ACM Trans. on Comp. Sys., vol. 20, no. 4, 2002.

[8] B. Carbunar and R. Sion, “Uncheatable reputation for distributed com-

putation markets,” in Financial Cryptography, 2006.

[9] R. Sailer, X. Zhang, T. Jaeger, and L. van Doorn, “Design and im-
plementation of a TCG-based integrity measurement architecture,” in
Proc. of the USENIX Security, 2004.

[10] L. Chen, R. Landfermann, H. L¨ohr, M. Rohe, A.-R. Sadeghi, and
C. St¨uble, “A protocol for property-based attestation,” in Proc. of the
ACM Workshop on Scalable Trusted Computing (STC), 2006.

[11] B. Parno, J. M. McCune, and A. Perrig, Bootstrapping Trust in Modern

Computers. Springer, 2011.

[12] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. VanDoorn, and P. Khosla,
“Pioneer: Verifying integrity and guaranteeing execution of code on
legacy platforms,” in Proc. of the ACM SOSP, 2005.

[13] R. B. Lee, P. Kwan, J. P. McGregor, J. Dwoskin, and Z. Wang, “Archi-
tecture for protecting critical secrets in microprocessors,” in Proc. of
the International Symposium on Computer Architecture (ISCA), 2005.
[14] D. Lie, C. A. Thekkath, M. Mitchell, P. Lincoln, D. Boneh, J. C.
Mitchell, and M. Horowitz, “Architectural support for copy and tamper
resistant software,” in Proc. of the ACM ASPLOS, 2000.

[15] A.-R. Sadeghi, T. Schneider, and M. Winandy, “Token-based cloud
computing: secure outsourcing of data and arbitrary computations with
lower latency,” in TRUST, 2010.

[16] S. Goldwasser, S. Micali, and C. Rackoff, “The knowledge complexity

of interactive proof systems,” SIAM J. Comput., vol. 18, no. 1, 1989.

250

[17] S. Arora and S. Safra, “Probabilistic checking of proofs: A new char-

acterization of NP,” J. ACM, vol. 45, no. 1, pp. 70–122, 1998.

[18] J. Kilian, “A note on efﬁcient zero-knowledge proofs and arguments

(extended abstract),” in STOC, 1992.

[19] S. Micali, “Computationally sound proofs,” SIAM J. Comput., vol. 30,

no. 4, pp. 1253–1298, 2000. Extended abstract in FOCS ’94.

[20] S. Goldwasser, Y. T. Kalai, and G. N. Rothblum, “Delegating compu-

tation: Interactive proofs for muggles,” in STOC, 2008.

[21] J. Groth, “Short pairing-based non-interactive zero-knowledge argu-

ments,” in ASIACRYPT, 2010.

[22] R. Gennaro, C. Gentry, and B. Parno, “Non-interactive veriﬁable com-

puting: Outsourcing computation to untrusted workers,” 2010.

[23] K.-M. Chung, Y. T. Kalai, and S. P. Vadhan, “Improved delegation of
computation using fully homomorphic encryption,” in CRYPTO, 2010.
[24] C. Gentry, A fully homomorphic encryption scheme. PhD thesis, Stan-

ford University, 2009. crypto.stanford.edu/craig.

[25] J. Thaler, M. Roberts, M. Mitzenmacher, and H. Pﬁster, “Veriﬁable
computation with massively parallel interactive proofs,” in USENIX
HotCloud Workshop, 2012.

[26] G. Cormode, M. Mitzenmacher, and J. Thaler, “Practical veriﬁed com-

putation with streaming interactive proofs,” in ITCS, 2012.

[27] S. Setty, R. McPherson, A. J. Blumberg, and M. Walﬁsh, “Making ar-
gument systems for outsourced computation practical (sometimes),” in
Proceedings of the ISOC NDSS, 2012.

[28] S. Setty, V. Vu, N. Panpalia, B. Braun, A. J. Blumberg, and M. Wal-
ﬁsh, “Taking proof-based veriﬁed computation a few steps closer to
practicality,” in Proc. of USENIX Security, 2012.

[29] B. Parno, M. Raykova, and V. Vaikuntanathan, “How to delegate and
verify in public: Veriﬁable computation from attribute-based encryp-
tion,” in IACR Theory of Cryptography Conference (TCC), 2012.

[30] R. Gennaro, C. Gentry, B. Parno, and M. Raykova, “Quadratic span
programs and succinct NIZKs without PCPs,” in EUROCRYPT, 2013.
Originally published as Cryptology ePrint Archive, Report 2012/215.
[31] M. Blum, A. D. Santis, S. Micali, and G. Persiano, “Noninteractive

zero-knowledge,” SIAM J. on Computing, vol. 20, no. 6, 1991.

[32] A. Rial and G. Danezis, “Privacy-preserving smart metering,” in Proc.

of the ACM WPES, 2011.

[33] N. Pippenger and M. J. Fischer, “Relations among complexity mea-

sures,” J. ACM, vol. 26, no. 2, 1979.

[34] D. Boneh and M. Franklin, “Identity-based encryption from the Weil

pairing,” Proceedings of IACR CRYPTO, 2001.

[35] D. Boneh, X. Boyen, and E.-J. Goh, “Hierarchical identity based en-

cryption with constant size ciphertext,” in EUROCRYPT, 2005.

[36] D. Boneh, C. Gentry, and B. Waters, “Collusion resistant broadcast
encryption with short ciphertexts and private keys,” in CRYPTO, 2005.
[37] M. Naor, “On cryptographic assumptions and challenges,” in Proceed-

ings of IACR CRYPTO, 2003.

[38] M. Bellare and A. Palacio, “The knowledge-of-exponent assumptions

and 3-round zero-knowledge protocols,” in CRYPTO, 2004.

[39] C. Gentry and D. Wichs, “Separating succinct non-interactive argu-

ments from all falsiﬁable assumptions,” in STOC, 2011.

[40] “Veriﬁable

computation:

Pinocchio.” http://research.

microsoft.com/verifcomp/, Mar. 2013.

[41] I. Damg˚ard, “Towards practical public key systems secure against cho-

sen ciphertext attacks,” in IACR CRYPTO, 1991.

[42] D. A. Wheeler, “SLOCCount.” http://www.dwheeler.com/

sloccount/.

[43] M. Aliasgari, M. Blanton, Y. Zhang, and A. Steele, “Secure computa-

tion on ﬂoating point numbers,” in Proc. of ISOC NDSS, 2013.

[44] A. Holzer, M. Franz, S. Katzenbeisser, and H. Veith, “Secure two-party

computations in ANSI C,” in Proc. of ACM CCS, 2012.

[45] M. Naehrig, R. Niederhagen, and P. Schwabe, “New software speed

records for cryptographic pairings,” in Proc. of LATINCRYPT, 2010.

[46] P. S. L. M. Barreto and M. Naehrig, “Pairing-friendly elliptic curves of

prime order,” in Selected Areas in Cryptography (SAC), 2006.

[47] N. Pippenger, “On the evaluation of powers and related problems (pre-

liminary version),” in Proc. of FOCS, 1976.

[48] A. Aho, J. Hopcroft, and J. Ulman, The Design and Analysis of Com-

puter Algorithms. Addison-Wesley, 1974.

[49] G. Adomavicius and A. Tuzhilin, “Toward the next generation of rec-
ommender systems: A survey of the state-of-the-art and possible exten-
sions,” Trans. Knowledge and Data Engineering, vol. 17, no. 6, 2005.
[50] D. A. Wolf-Gladrow, Lattice-Gas Cellular Automata and Lattice Boltz-

mann Models: An Introduction. Springer, 2005.

[51] R. Motwani and P. Raghavan, Randomized Algorithms. Cambridge

University Press, 1995.

[52] Y. Ishai, E. Kushilevitz, and R. Ostrovsky, “Efﬁcient arguments without
short PCPs,” in IEEE Conference on Computational Complexity, 2007.
[53] C. Gentry, S. Halevi, and N. Smart, “Homomorphic evaluation of the

AES circuit,” in Proceedings of CRYPTO, 2012.

[54] Y. Chen and R. Sion, “To cloud or not to cloud? Musings on costs and
viability,” in Proc. of the ACM Symposium on Cloud Computing, 2011.
[55] G. O. Karame, M. Strasser, and S. Capkun, “Secure remote execution
of sequential computations,” in Intl. Conf. on Information and Commu-
nications Security, 2009.

[56] S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy, “Proof ver-
iﬁcation and the hardness of approximation problems,” J. ACM, vol. 45,
no. 3, 1998.

[57] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella, “Fairplay—a secure two-

party computation system,” in Proc. of USENIX Security, 2004.

[58] Y. Huang, D. Evans, J. Katz, and L. Malka, “Faster secure two-party

computation using garbled circuits,” in USENIX Security, 2011.

[59] B. Kreuter, abhi shelat, and C.-H. Shen, “Billion-gate secure computa-

tion with malicious adversaries,” in Proc. of USENIX Security, 2012.

[60] S. Setty, B. Braun, V. Vu, A. J. Blumberg, B. Parno, and M. Walﬁsh,
“Resolving the conﬂict between generality and plausibility in veriﬁed
computation,” in Proc. of the ACM European Conference on Computer
Systems (EuroSys), Apr. 2013.

[61] V. Vu, S. Setty, A. J. Blumberg, and M. Walﬁsh, “A hybrid architecture
for interactive veriﬁable computation,” in IEEE Symposium on Security
and Privacy, May 2013.

[62] J. B. Almeida, E. Bangerter, M. Barbosa, S. Krenn, A.-R. Sadeghi,
and T. Schneider, “A certifying compiler for zero-knowledge proofs of
knowledge based on σ-protocols,” in Proc. of ESORICS, 2010.

[63] S. Meiklejohn, C. C. Erway, A. K¨upc¸ ¨u, T. Hinkle, and A. Lysyanskaya,
“ZKPDL: A language-based system for efﬁcient zero-knowledge
proofs and electronic cash,” in Proc. of USENIX, 2010.

[64] M. Backes, M. Maffe, and K. Pecina, “Automated synthesis of privacy-

preserving distributed applications,” in Proc. of ISOC NDSS, 2012.

[65] R. Cramer, I. Damg˚ard, and B. Schoenmakers, “Proofs of partial
knowledge and simpliﬁed design of witness hiding protocols,” in Proc.
of CRYPTO, 1994.

[66] J. Groth and A. Sahai, “Efﬁcient non-interactive proof systems for bi-

linear groups,” in Proc. of EUROCRYPT, 2008.

[67] D. Boneh and X. Boyen, “Short signatures without random oracles,” in

EUROCRYPT, 2004.

[68] R. Gennaro, “Multi-trapdoor commitments and their applications to
proofs of knowledge secure under concurrent man-in-the-middle at-
tacks,” in CRYPTO, 2004.

A Cryptographic Assumptions
We deﬁne the hardness assumptions that we use in the secu-
rity proof of our optimized VC construction from Section 3.1.
Suppose (p,G,GT ,e) ← G(1κ) outputs a description of a
cyclic bilinear group [34] of order p, a κ-bit prime, where
e : G× G → GT is the usual pairing (bilinear map) function.
Below, let κ be a security parameter, q = poly(κ), and let A
be a non-uniform probabilistic polynomial time adversary.
Assumption 1 (q-PDH [21]) The q-power Difﬁe-Hellman
(q-PDH) assumption holds for G if for all A we have
(p,G,GT ,e) ← G(1κ) ; g ← G\{1} ; s ← Z∗
p ;
σ ← (p,G,GT ,e,g,gs, . . . ,gsq
, . . . ,gs2q
) ;
y ← A(σ) : y = gsq+1

,gsq+2
] = negl(κ).

Pr[

Assumption 2 (q-PKE [21]) The q-power knowledge of ex-
ponent assumption holds for G if for all A there exists a non-
uniform probabilistic polynomial time extractor χA such that
Pr[

(p,G,GT ,e) ← G(1κ) ; g ← G\{1} ; α,s ← Z∗
p ;
σ ← (p,G,GT ,e,g,gs, . . . ,gsq
,gα,gαs, . . . ,gαsq
) ;

(c, ˆc ; a0, . . . ,aq) ← (A (cid:107) χA)(σ,z) :
ˆc = cα ∧ c (cid:54)= ∏q
] = negl(κ)

i=0 gaisi

for any auxiliary information z ∈ {0,1}poly(κ) that is gener-
ated independently of α. Note that (y;z) ← (A (cid:107) χA)(x) sig-
niﬁes that on input x, A outputs y, and that χA, given the same
input x and A’s random tape, produces z.
Assumption 3 (q-SDH [67, 68]) The
Difﬁe-
Hellman (q-SDH) assumption holds for G if for all A:

q-strong

Pr[

(p,G,GT ,e) ← G(1κ) ; g ← G\{1} ; s ← Z∗
p ;

σ ← (p,G,GT ,e,g,gs, . . . ,gsq

) ;

y ← A(σ) : y = e(g,g)

1

s+c ,c ∈ Z∗

p] = negl(κ).

B Security Proof for Our VC Scheme
To prove Theorem 1, we assume there exists an adversary Avc
who returns a cheating proof, and we show how to construct
an adversary B that uses Avc and a d-PKE assumption knowl-
edge extractor [21] to break either the q-PDH assumption [21]
or the 2q-SDH assumption [67, 68], where q = 4d + 4. These
assumptions are deﬁned in Appendix A.

,gsq+2

Let B be given a challenge g,gs, . . . ,gsq

, . . . ,gs2q.
(This challenge can be interpreted as either a q-PDH in-
stance, or a subset (missing gsq+1) of a 2q-SDH instance.)
Avc generates a function f with N input/output wires that has
a QAP Q f = (t(x),V ,W ,Y ) of size m and degree d. (So
that this proof covers the zero-knowledge variant of the VC
scheme, we include t(x) in each of the sets V ,W ,Y .) Let
Imid = {N + 1, ...,m}, i.e., the non-IO-related indices.

v

y

v

w

B provides evaluation and veriﬁcation keys to Avc, using
}k∈[m],
}k∈[m],

the same structure as in Protocol 2:
}k∈Imid ,
EK = ( {gvk(s)
{gwk(s)
}k∈Imid , {gαwwk(s)
{gαvvk(s)
{gβvk(s)
{gsi}i∈[d],
V KF = (g,gαv ,gαw ,gαy ,gγ,gβγ,{gvk(s)
v
where r(cid:48)
v, r(cid:48)
w, gv = gr(cid:48)
r(cid:48)
vr(cid:48)
y = r(cid:48)
and the values β and γ are set as described next.
Regarding β, write the ﬁnal proof term with g as the base:

,gy0(s)
w, αv, αw, and αy are chosen uniformly at random,
ys3(d+1),

{gyk(s)
}k∈[m],
}k∈[m], {gαyyk(s)
}k∈[m]
gβwk(s)
w
}k∈{0}∪Iin ,gw0(s)
ws2(d+1), and gy = gr(cid:48)

vsd+1, gw = gr(cid:48)

y
gβyk(s))
y

w

w

v

y

),
,gt(s)

y

),

gβv(s)
v

gβw(s)
w

y = gβ(r(cid:48)
gβy(s)

vsd+1v(s)+r(cid:48)

ws2(d+1)w(s)+r(cid:48)

ys3(d+1)y(s)).

(1)

That is, in the exponent, β is multiplied with a certain poly-
nomial that is evaluated at s. B also generates β as a polyno-
mial evaluated at s. In particular, it sets β = sq−(4d+3)βpoly(s),
where βpoly(x) is a polynomial of degree at most 3d + 3

251

sampled uniformly at random such that βpoly(x)· (r(cid:48)
vvk(x) +
r(cid:48)
wxd+1wk(x) + r(cid:48)
yx2(d+1)yk(x)) has a zero coefﬁcient in front
of x3d+3 for all k. We know that such a polynomial βpoly(x)
exists by Lemma 10 of GGPR [30], which says (roughly)
that, given a set of polynomials {uk(x) ∈ F[x]} of degree at
most D, then there exists a nonzero polynomial a(x) of de-
gree D + 1 such that all of the polynomials a(x)uk(x) have a
nonzero coefﬁcient for xD+1, and moreover for any polyno-
mial u(x) not in the linear span of {uk(x)}, the coefﬁcient of
xD+1 in a(x)u(x) is uniformly random in F (when sampling
a(x) uniformly subject to the above restriction). By inspec-
tion, when we now write out β in terms of s, we see that the
exponent in Equation 1 has a zero in front of sq+1, and also
the powers of s only go up to degree q + 3d + 3 ≤ 2q. There-
fore, B can efﬁciently generate the terms in the evaluation
key that contain β using the elements given in his challenge.
Regarding γ, B generates γ(cid:48) uniformly at random from
F and sets γ = γ(cid:48)sq+2.
B can generate gγ efﬁciently
from its challenge, since gsq+2
is given. Also, βγ =
sq−(4d+3)βpoly(s)γ(cid:48)sq+2 does not have a term sq+1 and has de-
gree at most q− (4d + 3) + (3d + 3) + (q + 2) ≤ 2q, and so B
can generate gβγ from the elements in its challenge.

Similarly none of the other elements in the CRS contains a
term sq+1 in the exponent, since all of the polynomials vk(x),
wk(x) and yk(x) are of degree d and q ≥ 4d +4. Hence, B can
generate them using the terms from its challenge.

Thus, the evaluation and veriﬁcations keys generated by B
have a distribution statistically identical to the real scheme.

Given EKF and V KF, Avc can run the Compute and Verify
w , gY(cid:48)
, gW(cid:48)
algorithms on its own. Let gVmidv
y ,
and gZ be a cheating proof that Avc provides for the result of
the computation of f with input and output {ck}k∈[N].

y , gH, gV(cid:48)
midv

w , gY

, gW

)αv = gV(cid:48)

v }i∈[0,d],{gαvsi

Since the veriﬁcation holds, we have that (gVmidv

mid .
We claim that B can run the d-PKE extractor χA to re-
cover a polynomial Vmid(x) of degree at most d such that
Vmid = Vmid(s). Though it may not be immediately recog-
nizable as such, the parameters received by Avc are a valid
input (σ,z) for the d-PKE assumption. In particular, a valid
input consists of σ = ({gsi
}i∈[0,d]) and z, where
the auxiliary information z is independent of αv, and other
terms in the CRS can be generated efﬁciently from (σ,z).
The terms {gvk(s)
} can indeed be efﬁciently generated from
{gsi
v }, and the auxiliary information z that Avc receives is in-
deed independent of αv. Therefore, B can invoke the d-PKE
extractor χA to recover Vmid(x), which must be a polyno-
mial of degree at most d. Similarly, B recovers W (x), Y (x)
such that W = W (s) and Y = Y (s). Then, it sets H(x) =
((v0(x) +V (x))(w0(x) +W (x))− (y0(x) +Y (x)))/t(x), where
V (x) = ∑k∈[N] ckvk(x) +Vmid(x).

v

v

Since the proof veriﬁes, but the statement is false (and
therefore the extracted polynomials cannot actually be a QAP
solution), there are two possible cases: (1) H(x) has a non-
trivial denominator, or (2) The polynomial r(cid:48)
vxd+1V (x) +

vxd+1vk(x) + r(cid:48)

vxd+1V †(x) + r(cid:48)

yx3(d+1)Y (x) is not in the space gener-
wx2(d+1)wk(x) +

r(cid:48)
wx2(d+1)W (x) + r(cid:48)
ated by the polynomials {r(cid:48)
yx3(d+1)yk(x)}k∈[m].
r(cid:48)
First, we show that these are the only two cases, if the
proof veriﬁes but the statement is false. Let S be the linear
subspace generated by linear combinations of {r(cid:48)
vxd+1vk(x) +
yx3(d+1)yk(x)}k∈[m]. For any polynomial in
r(cid:48)
wx2(d+1)wk(x) + r(cid:48)
this subspace, there exists a linear combination {ck}k∈[m] such
that the polynomial equals r(cid:48)
wx2(d+1)W †(x) +
r(cid:48)
yx3(d+1)Y †(x), where V †(x) = ∑k∈[m] ckvk(x), W †(x) =
∑k∈[m] ckwk(x), Y †(x) = ∑k∈[m] ckyk(x). Each of the polyno-
mials V †(x),W †(x),Y †(x) has degree at most d because they
are in the spans of {vk(x)k∈[m]}, {wk(x)k∈[m]} and {yk(x)k∈[m]}
Since V (x), W (x) and Y (x) also have de-
respectively.
gree d, and since the linear subspaces {xd+1+i|i ∈ [0,d]},
{x2(d+1)+i|i ∈ [0,d]}, and {x3(d+1)+i|i ∈ [0,d]} are disjoint
(except at the origin), we conclude that if r(cid:48)
vxd+1V (x) +
r(cid:48)
wx2(d+1)W (x) + r(cid:48)
yx3(d+1)Y (x) is in S, then it must be true
that V (x) = V †(x), W (x) = W †(x) , Y (x) = Y †(x). Therefore,
V (x), W (x), and Y (x) indeed use the same linear combination
{ck}k∈[m] of their polynomial sets, as required in a QAP. If,
moreover, t(x) evenly divides V (x)W (x)−Y (x) (so that H(x)
has no nontrivial denominator), then V (x), W (x), and Y (x)
constitute a QAP solution: a true proof (contradiction).
In Case 1,
t(x) does not divide Z(x) := (v0(x) +V (x))(w0(x) +W (x))−
(y0(x) + Y (x)). Let (x − r) be a polynomial that divides
t(x) but not Z(x), and let T (x) = t(x)/(x − r). Let d(x) =
gcd(t(x),Z(x)). Since t(x) and Z(x) have degrees at most d
and 2d respectively, B can use the extended Euclidean algo-
rithm for polynomials to ﬁnd polynomials a(x) and b(x) of
degrees 2d − 1 and d − 1 respectively such that a(x)t(x) +
b(x)Z(x) = d(x). Set A(x) = a(x)· (T (x)/d(x)) and B(x) =
b(x) · (T (x)/d(x)); these polynomials have no denominator
since d(x) divides T (x). Then A(x)t(x) + B(x)Z(x) = T (x).
Dividing by t(x), we have A(x)+B(x)H(x) = 1/(x−r). Since
A(x) and B(x) have degree at most 2d − 1 ≤ q, B can use
the terms in its challenge to compute e(gA(s),g)e(gB(s),gH ) =
e(g,g)1/(s−r), and thus solve 2q-SDH.
there does not exist {ck}k∈[m] such that
V (x) = ∑k∈[m] ckvk(x), W (x) = ∑k∈[m] ckwk(x) and Y (x) =
∑k∈[m] ckyk(x).
By Lemma 10 [30], we have that
with high probability xq−(4d+3)βpoly(x) · (r(cid:48)
vxd+1vk(x) +
r(cid:48)
wx2(d+1)wk(x) + r(cid:48)
yx3(d+1)yk(x)) has a non-zero coefﬁ-
Now B can use gZ =
cient
gsq−(4d+3)βpoly(s)(sd+1V (s)+s2(d+1)W (s)+s3(d+1)Y (s)) to subtract off
all elements of the form gs j where j (cid:54)= q + 1 and to obtain
gsq+1. This breaks the q-PDH assumption.

Next we address the two cases from above.

the term xq+1.

In Case 2,

for

252

